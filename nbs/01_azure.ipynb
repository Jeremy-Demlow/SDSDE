{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:08.732340Z",
     "start_time": "2022-02-02T01:09:08.730033Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp azure.filehanling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:08.907048Z",
     "start_time": "2022-02-02T01:09:08.904728Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``FileHandling``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is Azure Blob? \n",
    "\n",
    "Azure Blob storage is Microsoft's object storage solution for the cloud. Blob storage is optimized for storing massive amounts of unstructured data, such as text or binary data.\n",
    "\n",
    "Blob storage is ideal for:\n",
    "\n",
    "- Serving images or documents directly to a browser\n",
    "\n",
    "- Storing files for distributed access\n",
    "\n",
    "- Streaming video and audio\n",
    "\n",
    "- Storing data for backup and restore, disaster recovery, and archiving\n",
    "\n",
    "- Storing data for analysis by an on-premises or Azure-hosted service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:09.660308Z",
     "start_time": "2022-02-02T01:09:09.457639Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:09.720855Z",
     "start_time": "2022-02-02T01:09:09.695436Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class FileHandling:\n",
    "    def __init__(self, connection_string, logger=None):\n",
    "        \"\"\"\n",
    "        file handling for all sdsde Azure blob storage. Both upload and download\n",
    "        have there own clients with in this class so that they can call different\n",
    "        containers with in the storage account within on init. This is designed,\n",
    "        but can be removed if we still this to be a waste.\n",
    "        Args:\n",
    "        * connection_string (str): azure connection string to blob storage\n",
    "        * self._logger: logging choices can be overwritten if you want the defaults\n",
    "        \"\"\"\n",
    "        self.connection_string = connection_string\n",
    "        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        self._logger = logger if logger is not None else logging.getLogger(__name__)\n",
    "\n",
    "    def upload(self,\n",
    "               container_name: str,\n",
    "               file_path: str,\n",
    "               dest: str = None,\n",
    "               overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        upload a folder a file to azure blob storage\n",
    "        Args:\n",
    "        * container_name (str): azure blob container name\n",
    "        * file_path (str): file or directory to upload to azure\n",
    "        * dest (str, optional): destination in azure/ file name in azure. Defaults to ''.\n",
    "        * overwrite (bool, optional): write over same file names. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.container_name = container_name\n",
    "        self.container_client = self.blob_service_client.get_container_client(container_name)\n",
    "        self.create_blob_container(container_name=container_name)\n",
    "        if (os.path.isdir(file_path)) is not False:\n",
    "            self.load_dir = True\n",
    "            self.upload_dir(file_path, dest, overwrite)\n",
    "        else:\n",
    "            if dest is not None:\n",
    "                pass\n",
    "            else:\n",
    "                dest = file_path.split('/')[-1]\n",
    "                self._logger.warning(f'destination to blob is now {dest}')\n",
    "                self._logger.warning('to delcare new path or file name add dest to upload')\n",
    "            self.load_dir = False\n",
    "            self.upload_file(file_path, dest, overwrite)\n",
    "\n",
    "    def upload_file(self,\n",
    "                    file_path: str,\n",
    "                    blob_name: str,\n",
    "                    overwrite=False):\n",
    "        \"\"\"\n",
    "        simply uploads a file to blob storage\n",
    "        Args:\n",
    "        * file_path (str): file to blob\n",
    "        * blob_name (str): destination in blob\n",
    "        * overwrite (bool, optional): write over same file names. Defaults to False.\n",
    "        \"\"\"\n",
    "        self._logger.info(f'Uploading {file_path}, to to Azure Storage {blob_name}')\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            try:\n",
    "                self.container_client.upload_blob(data=file, name=blob_name, overwrite=overwrite)\n",
    "            except Exception as e:\n",
    "                self._logger.error(f'\"Error Message: {e.error_code.value}\"')\n",
    "        self._logger.info('Azure Upload Complete')\n",
    "\n",
    "    def upload_dir(self,\n",
    "                   directory: str,\n",
    "                   dest: str = None,\n",
    "                   overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        simply uploads a directory to azure blob note with python\n",
    "        we can have arguement be used from upload will figure\n",
    "        that out hopefully\n",
    "        Args:\n",
    "        * directory (str): directory that is being moved to blob\n",
    "        * overwrite (bool, optional): write over same file names. Defaults to False.\n",
    "        \"\"\"\n",
    "        prefix = os.path.basename(directory) + '/'\n",
    "        # Walk Through Directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for name in files:\n",
    "                if dest is None:\n",
    "                    dir_part = os.path.relpath(root, directory)\n",
    "                    dir_part = '' if dir_part == '.' else dir_part + '/'\n",
    "                else:\n",
    "                    dir_part = dest\n",
    "                file_path = os.path.join(root, name)\n",
    "                blob_path = prefix + dir_part + name\n",
    "                self.upload_file(file_path, blob_path, overwrite)\n",
    "\n",
    "    def create_blob_container(self,\n",
    "                              container_name: str = str(uuid.uuid4()),\n",
    "                              unique: bool = False):\n",
    "        \"\"\"\n",
    "        creates/check for container when ``upload`` is called, but\n",
    "        this function can be used seperately to create\n",
    "        a new container in isolation within the specific storage account\n",
    "        Args:\n",
    "        * container_name (str, optional): creates azure blob container name. Defaults to str(uuid.uuid4()).\n",
    "        * unique (bool, optional): add a unique tail to container name. Defaults to False.\n",
    "        \"\"\"\n",
    "        regex = re.compile(r'[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "        # Check if container_name is valid\n",
    "        if (regex.search(container_name) is None):\n",
    "            self._logger.info(f'{container_name} is a valid')\n",
    "        else:\n",
    "            container_name = re.sub(r'[\\W_]+', '', container_name)\n",
    "            self._logger.info(f'container_name changed to {container_name}')\n",
    "        if unique:\n",
    "            container_name = container_name + str(uuid.uuid4())\n",
    "        else:\n",
    "            container_name = container_name\n",
    "        # Current Python SDK doesn't support exist\n",
    "        try:\n",
    "            # Create the container\n",
    "            _ = self.blob_service_client.create_container(container_name)\n",
    "        except Exception as e:\n",
    "            self._logger.info(f'{e.error_code.value}')\n",
    "\n",
    "    def download(self,\n",
    "                 blob_location: str,\n",
    "                 dest: str,\n",
    "                 container_name: str,\n",
    "                 blob_path: str = '',\n",
    "                 recursive: bool = True,\n",
    "                 overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        * blob_location (str): location in container\n",
    "        * dest (str): local destination of file\n",
    "        * container_name (str): azure blob container name.\n",
    "        * blob_path (str, optional): path in blob helps search. Defaults to ''.\n",
    "        * recursive (bool, optional): helps with search for file can be false if blob_path is known. Defaults to True.\n",
    "        * overwrite (bool, optional): write over same file names. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not dest.endswith('/'):\n",
    "            dest += '/'\n",
    "        if blob_location.endswith('/'):\n",
    "            if blob_path == '':\n",
    "                blobs = self.ls_blob(container_name=container_name, path=blob_path, recursive=recursive)\n",
    "                blobs = [blobs for blobs in blobs if os.path.basename(os.path.normpath(blob_location)) in blobs]\n",
    "            else:\n",
    "                blobs = self.ls_blob(container_name=container_name, path=blob_path, recursive=recursive)\n",
    "                blobs = [blob_location + blob for blob in blobs]\n",
    "            for blob in blobs:\n",
    "                self._logger.info(f'Downloading {blob}')\n",
    "                self.download_file(container_name, blob, dest, overwrite)\n",
    "        else:\n",
    "            self.download_file(container_name, os.path.basename(os.path.normpath(blob_location)), dest, overwrite)\n",
    "        self._logger.info('Download complete')\n",
    "\n",
    "    def download_file(self,\n",
    "                      container_name: str,\n",
    "                      file: str,\n",
    "                      file_path: str,\n",
    "                      overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        * container_name (str): azure blob container name.\n",
    "        * file (str): file to download from blob\n",
    "        * file_path (str): location to put `file`\n",
    "        * overwrite (bool, optional): write over same file names. Defaults to False.\n",
    "        \"\"\"\n",
    "        if file_path.endswith('.'):\n",
    "            file_path += '/'\n",
    "        blob_dest = file_path + os.path.basename(file) if file_path.endswith('/') else file_path\n",
    "        self._logger.info(f'{file} to {blob_dest}')\n",
    "        if not overwrite:\n",
    "            if os.path.exists(blob_dest):\n",
    "                self._logger.warning('file path already exist change overwrite to ``True`` if you want to overwrite file')\n",
    "                return\n",
    "        os.makedirs(os.path.dirname(blob_dest), exist_ok=True)\n",
    "        download_client = self.blob_service_client.get_container_client(container_name)\n",
    "        downloader = download_client.get_blob_client(blob=file)\n",
    "        with open(blob_dest, 'wb') as file:\n",
    "            data = downloader.download_blob()\n",
    "            file.write(data.readall())\n",
    "\n",
    "    def ls_blob(self,\n",
    "                container_name: str,\n",
    "                path: str,\n",
    "                recursive: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        * container_name (str): azure blob container name.\n",
    "        * path (str): blob path to look at\n",
    "        * recursive (bool, optional): recurisve look. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        * list: file list\n",
    "        \"\"\"\n",
    "        if not path == '' and not path.endswith('/'):\n",
    "            path += '/'\n",
    "        download_client = self.blob_service_client.get_container_client(container_name)\n",
    "        blob_looker = download_client.list_blobs(name_starts_with=path)\n",
    "        files = []\n",
    "        for blob in blob_looker:\n",
    "            relative_path = os.path.relpath(blob.name, path)\n",
    "            if recursive or not '/' in relative_path:  # NOQA:\n",
    "                files.append(relative_path)\n",
    "        return files\n",
    "\n",
    "    def rm_files(self,\n",
    "                 container_name: str,\n",
    "                 delete_path: str = '',\n",
    "                 recursive: bool = False):\n",
    "        \"\"\"\n",
    "        removes files from storage account\n",
    "        Args:\n",
    "        * container_name (str): azure blob container name.\n",
    "        * delete_path (str, optional): what to delete file or directory. Defaults to ''.\n",
    "        * recursive (bool, optional): recursive delete. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not delete_path == '' and not delete_path.endswith('/'):\n",
    "            delete_path, delete_file = delete_path.rsplit('/', 1)\n",
    "            delete_path += '/'\n",
    "            blobs = self.ls_blob(container_name, delete_path, recursive)\n",
    "            if delete_file in blobs:\n",
    "                blobs = [delete_file]\n",
    "        else:\n",
    "            blobs = self.ls_blob(container_name, delete_path, recursive)\n",
    "        if not blobs:\n",
    "            self._logger.warning('location in blob is empty')\n",
    "            return\n",
    "        blobs = [delete_path + blob for blob in blobs]\n",
    "        self._logger.info(f'files to be removed {blobs}')\n",
    "        delete_client = self.blob_service_client.get_container_client(container_name)\n",
    "        if len(blobs) > 1:\n",
    "            for x in blobs:\n",
    "                delete_client.delete_blobs(*x)\n",
    "        else:\n",
    "            delete_client.delete_blob(*blobs)\n",
    "\n",
    "    def rm_folder(self, container_name: str, folder_path: str):\n",
    "        \"\"\"Doesn't work need to build\"\"\"\n",
    "        container_client = ContainerClient.from_connection_string(conn_str=self.connection_string,\n",
    "                                                                  container_name=container_name)\n",
    "        container_client.delete_blob(blob=folder_path)\n",
    "\n",
    "    def rm_container(self,\n",
    "                     container_name: str):\n",
    "        \"remove container from storage account\"\n",
    "        self.blob_service_client.delete_container(container_name)\n",
    "\n",
    "    def ls_containers(self,\n",
    "                      name_starts_with: str = None):\n",
    "        \"show containers in storage account\"\n",
    "        container_names = self.blob_service_client.list_containers(name_starts_with=name_starts_with)\n",
    "        for names in container_names:\n",
    "            print(names['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:09.872405Z",
     "start_time": "2022-02-02T01:09:09.869416Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def unlink_files(files: list, file_path: str = './'):\n",
    "    \"\"\"\n",
    "    clean up tool for files that shouldn't be there\n",
    "\n",
    "    Args:\n",
    "    * files (list): can be a list of just one file to remove\n",
    "    * file_path (str, optional): location of the file. Defaults to './'.\n",
    "    \"\"\"\n",
    "    file_list = files\n",
    "    for x in file_list:\n",
    "        os.unlink(os.path.join(file_path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:09.917345Z",
     "start_time": "2022-02-02T01:09:09.912966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"FileHandling\" class=\"doc_header\"><code>class</code> <code>FileHandling</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>FileHandling</code>(**`connection_string`**, **`logger`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Use ``FileHandling``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:10.314890Z",
     "start_time": "2022-02-02T01:09:10.312512Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "connect_str = os.environ['connection_str']\n",
    "fh = FileHandling(connect_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Section will show how a user can send files up to azure for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:10.844683Z",
     "start_time": "2022-02-02T01:09:10.828484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.upload\" class=\"doc_header\"><code>FileHandling.upload</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.upload</code>(**`container_name`**:`str`, **`file_path`**:`str`, **`dest`**:`str`=*`None`*, **`overwrite`**:`bool`=*`False`*)\n",
       "\n",
       "upload a folder a file to azure blob storage\n",
       "Args:\n",
       "* container_name (str): azure blob container name\n",
       "* file_path (str): file or directory to upload to azure\n",
       "* dest (str, optional): destination in azure/ file name in azure. Defaults to ''.\n",
       "* overwrite (bool, optional): write over same file names. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a Directory To Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the ``file_path`` is specified as a folder the entire folder will be sent to azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:21.281476Z",
     "start_time": "2022-02-02T01:09:11.344013Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sdsdetestazurenotebook is a valid\n",
      "INFO:__main__:Uploading testing/clean_multi.sql, to to Azure Storage /testing/clean_multi.sql\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/multi.sql, to to Azure Storage /testing/multi.sql\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/sdsde_test.yaml, to to Azure Storage /testing/sdsde_test.yaml\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/sftp.yaml, to to Azure Storage /testing/sftp.yaml\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/sftp_only.yaml, to to Azure Storage /testing/sftp_only.yaml\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/sftp_query.sql, to to Azure Storage /testing/sftp_query.sql\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/snowflake.yaml, to to Azure Storage /testing/snowflake.yaml\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/test.txt, to to Azure Storage /testing/test.txt\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/test_df.csv, to to Azure Storage /testing/test_df.csv\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/test_yaml.sql, to to Azure Storage /testing/test_yaml.sql\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Uploading testing/view_test.sql, to to Azure Storage /testing/view_test.sql\n",
      "INFO:__main__:Azure Upload Complete\n"
     ]
    }
   ],
   "source": [
    "# Upload dir \n",
    "fh.upload(container_name='sdsdetestazurenotebook',\n",
    "          file_path='testing/',\n",
    "          dest='testing/',\n",
    "          overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:21.410968Z",
     "start_time": "2022-02-02T01:09:21.283548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if files arrived\n",
    "assert fh.ls_blob(container_name='sdsdetestazurenotebook', path='testing/', recursive=True) == ['clean_multi.sql',\n",
    " 'multi.sql',\n",
    " 'sdsde_test.yaml',\n",
    " 'sftp.yaml',\n",
    " 'sftp_only.yaml',\n",
    " 'sftp_query.sql',\n",
    " 'snowflake.yaml',\n",
    " 'test.txt',\n",
    " 'test_df.csv',\n",
    " 'test_yaml.sql',\n",
    " 'view_test.sql'], 'Test folder should be in azure'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send A File To Azure Blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having just a file be sent up to azure is as simple as giving it a file path and it will take the file name and add it to the root directory of the ``container_name``.\n",
    "\n",
    "If you want to change the name or sent this file to a certain location with in the container_name ``dest`` option allows you to specify the name of the file as well as the location that it will be dropped off at in azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:21.702399Z",
     "start_time": "2022-02-02T01:09:21.412511Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sdsdetestazurenotebook is a valid\n",
      "INFO:__main__:ContainerAlreadyExists\n",
      "WARNING:__main__:destination to blob is now test.txt\n",
      "WARNING:__main__:to delcare new path or file name add dest to upload\n",
      "INFO:__main__:Uploading testing/test.txt, to to Azure Storage test.txt\n",
      "INFO:__main__:Azure Upload Complete\n"
     ]
    }
   ],
   "source": [
    "# Upload file using actual file name in root \n",
    "fh.upload(container_name='sdsdetestazurenotebook',\n",
    "          file_path='testing/test.txt',\n",
    "          overwrite=True)\n",
    "\n",
    "assert 'test.txt' in fh.ls_blob(container_name='sdsdetestazurenotebook', path='', recursive=False), 'test.txt should be in the root'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a File to Azure Blob and Rename File on the Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:22.045887Z",
     "start_time": "2022-02-02T01:09:21.705485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sdsdetestazurenotebook is a valid\n",
      "INFO:__main__:ContainerAlreadyExists\n",
      "INFO:__main__:Uploading testing/test.txt, to to Azure Storage trainwreck.txt\n",
      "INFO:__main__:Azure Upload Complete\n"
     ]
    }
   ],
   "source": [
    "# Upload file with changed name\n",
    "fh.upload(container_name='sdsdetestazurenotebook',\n",
    "          file_path='testing/test.txt',\n",
    "          dest='trainwreck.txt',\n",
    "          overwrite=True)\n",
    "\n",
    "assert 'trainwreck.txt' in fh.ls_blob(container_name='sdsdetestazurenotebook', path='', recursive=False), 'test.txt & trainweck should be there'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section show how you can pull files down from azure blob as long as ``FileHandling`` is instantiated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:22.056935Z",
     "start_time": "2022-02-02T01:09:22.047225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.download\" class=\"doc_header\"><code>FileHandling.download</code><a href=\"__main__.py#L121\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.download</code>(**`blob_location`**:`str`, **`dest`**:`str`, **`container_name`**:`str`, **`blob_path`**:`str`=*`''`*, **`recursive`**:`bool`=*`True`*, **`overwrite`**:`bool`=*`False`*)\n",
       "\n",
       "Args:\n",
       "* blob_location (str): location in container\n",
       "* dest (str): local destination of file\n",
       "* container_name (str): azure blob container name.\n",
       "* blob_path (str, optional): path in blob helps search. Defaults to ''.\n",
       "* recursive (bool, optional): helps with search for file can be false if blob_path is known. Defaults to True.\n",
       "* overwrite (bool, optional): write over same file names. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Directory From Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.079466Z",
     "start_time": "2022-02-02T01:09:22.058451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloading testing\n",
      "INFO:__main__:testing to testing2/testing\n",
      "INFO:__main__:Downloading testing/clean_multi.sql\n",
      "INFO:__main__:testing/clean_multi.sql to testing2/clean_multi.sql\n",
      "INFO:__main__:Downloading testing/multi.sql\n",
      "INFO:__main__:testing/multi.sql to testing2/multi.sql\n",
      "INFO:__main__:Downloading testing/sdsde_test.yaml\n",
      "INFO:__main__:testing/sdsde_test.yaml to testing2/sdsde_test.yaml\n",
      "INFO:__main__:Downloading testing/sftp.yaml\n",
      "INFO:__main__:testing/sftp.yaml to testing2/sftp.yaml\n",
      "INFO:__main__:Downloading testing/sftp_only.yaml\n",
      "INFO:__main__:testing/sftp_only.yaml to testing2/sftp_only.yaml\n",
      "INFO:__main__:Downloading testing/sftp_query.sql\n",
      "INFO:__main__:testing/sftp_query.sql to testing2/sftp_query.sql\n",
      "INFO:__main__:Downloading testing/snowflake.yaml\n",
      "INFO:__main__:testing/snowflake.yaml to testing2/snowflake.yaml\n",
      "INFO:__main__:Downloading testing/test.txt\n",
      "INFO:__main__:testing/test.txt to testing2/test.txt\n",
      "INFO:__main__:Downloading testing/test_df.csv\n",
      "INFO:__main__:testing/test_df.csv to testing2/test_df.csv\n",
      "INFO:__main__:Downloading testing/test_yaml.sql\n",
      "INFO:__main__:testing/test_yaml.sql to testing2/test_yaml.sql\n",
      "INFO:__main__:Downloading testing/view_test.sql\n",
      "INFO:__main__:testing/view_test.sql to testing2/view_test.sql\n",
      "INFO:__main__:Download complete\n"
     ]
    }
   ],
   "source": [
    "# download calls download_file when downloading directory of files\n",
    "fh.download(blob_location = 'testing/',\n",
    "            dest = 'testing2/',\n",
    "            container_name='sdsdetestazurenotebook',\n",
    "            blob_path = '',\n",
    "            recursive = True,\n",
    "            overwrite = True)\n",
    "assert os.path.exists('testing/multi.sql') == True, 'File should be located here'\n",
    "unlink_files(['clean_multi.sql', 'multi.sql', 'test.txt', 'view_test.sql', 'testing'], './testing2/')\n",
    "assert os.path.exists('testing2/multi.sql') == False, 'File should not be there'\n",
    "! rm -rf testing2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Specific File From Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.092940Z",
     "start_time": "2022-02-02T01:09:29.081983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.download_file\" class=\"doc_header\"><code>FileHandling.download_file</code><a href=\"__main__.py#L153\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.download_file</code>(**`container_name`**:`str`, **`file`**:`str`, **`file_path`**:`str`, **`overwrite`**:`bool`=*`False`*)\n",
       "\n",
       "Args:\n",
       "* container_name (str): azure blob container name.\n",
       "* file (str): file to download from blob\n",
       "* file_path (str): location to put `file`\n",
       "* overwrite (bool, optional): write over same file names. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.download_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.158357Z",
     "start_time": "2022-02-02T01:09:29.095016Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:testing/multi.sql to testing/multi.sql\n"
     ]
    }
   ],
   "source": [
    "fh.download_file(file='testing/multi.sql',\n",
    "                 file_path = 'testing/',\n",
    "                 container_name='sdsdetestazurenotebook',\n",
    "                 overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.167979Z",
     "start_time": "2022-02-02T01:09:29.160323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.rm_files\" class=\"doc_header\"><code>FileHandling.rm_files</code><a href=\"__main__.py#L204\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.rm_files</code>(**`container_name`**:`str`, **`delete_path`**:`str`=*`''`*, **`recursive`**:`bool`=*`False`*)\n",
       "\n",
       "removes files from storage account\n",
       "Args:\n",
       "* container_name (str): azure blob container name.\n",
       "* delete_path (str, optional): what to delete file or directory. Defaults to ''.\n",
       "* recursive (bool, optional): recursive delete. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.rm_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.478666Z",
     "start_time": "2022-02-02T01:09:29.170639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:files to be removed ['testing/view_test.sql']\n"
     ]
    }
   ],
   "source": [
    "fh.rm_files(container_name='sdsdetestazurenotebook', delete_path='testing/view_test.sql')\n",
    "assert 'view_test.txt' not in fh.ls_blob(container_name='sdsdetestazurenotebook', path='testing/', recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a Specific File inside Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.776944Z",
     "start_time": "2022-02-02T01:09:29.480586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:files to be removed ['testing/test.txt']\n"
     ]
    }
   ],
   "source": [
    "# remove files from blob\n",
    "# Note: remember when there is only one file in the folder the folder will be deleted\n",
    "fh.rm_files(container_name='sdsdetestazurenotebook', \n",
    "            delete_path='testing/test.txt')\n",
    "assert 'test.txt' not in fh.ls_blob(container_name='sdsdetestazurenotebook', path='testing', recursive=True), 'only test.txt should be removed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.784101Z",
     "start_time": "2022-02-02T01:09:29.778332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.rm_container\" class=\"doc_header\"><code>FileHandling.rm_container</code><a href=\"__main__.py#L241\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.rm_container</code>(**`container_name`**:`str`)\n",
       "\n",
       "remove container from storage account"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.rm_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Container Fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:29.992442Z",
     "start_time": "2022-02-02T01:09:29.785506Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove container\n",
    "fh.rm_container(container_name='sdsdetestazurenotebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.000197Z",
     "start_time": "2022-02-02T01:09:29.993845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.ls_blob\" class=\"doc_header\"><code>FileHandling.ls_blob</code><a href=\"__main__.py#L180\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.ls_blob</code>(**`container_name`**:`str`, **`path`**:`str`, **`recursive`**:`bool`=*`False`*)\n",
       "\n",
       "Args:\n",
       "* container_name (str): azure blob container name.\n",
       "* path (str): blob path to look at\n",
       "* recursive (bool, optional): recurisve look. Defaults to False.\n",
       "\n",
       "Returns:\n",
       "* list: file list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.ls_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.006372Z",
     "start_time": "2022-02-02T01:09:30.001774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.ls_containers\" class=\"doc_header\"><code>FileHandling.ls_containers</code><a href=\"__main__.py#L246\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.ls_containers</code>(**`name_starts_with`**:`str`=*`None`*)\n",
       "\n",
       "show containers in storage account"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.ls_containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.072765Z",
     "start_time": "2022-02-02T01:09:30.008199Z"
    }
   },
   "outputs": [],
   "source": [
    "fh.ls_containers(name_starts_with='sdsde_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.081839Z",
     "start_time": "2022-02-02T01:09:30.074059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FileHandling.create_blob_container\" class=\"doc_header\"><code>FileHandling.create_blob_container</code><a href=\"__main__.py#L92\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FileHandling.create_blob_container</code>(**`container_name`**:`str`=*`'0910a90c-50ea-4bd0-ba18-71126b803cd4'`*, **`unique`**:`bool`=*`False`*)\n",
       "\n",
       "creates/check for container when ``upload`` is called, but\n",
       "this function can be used seperately to create\n",
       "a new container in isolation within the specific storage account\n",
       "Args:\n",
       "* container_name (str, optional): creates azure blob container name. Defaults to str(uuid.uuid4()).\n",
       "* unique (bool, optional): add a unique tail to container name. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FileHandling.create_blob_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.269719Z",
     "start_time": "2022-02-02T01:09:30.083593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sdsdetest is a valid\n"
     ]
    }
   ],
   "source": [
    "fh.create_blob_container(container_name='sdsdetest')\n",
    "assert fh.ls_blob(container_name='sdsdetest', path='', recursive=True) == [], \"empty container should be created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.356644Z",
     "start_time": "2022-02-02T01:09:30.271313Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove container\n",
    "fh.rm_container(container_name='sdsdetest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rename_adls_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.364677Z",
     "start_time": "2022-02-02T01:09:30.358856Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def rename_adls_directory(old_dir, new_dir, storage_account_name, storage_account_key, container):\n",
    "    \"\"\"renames a directory in azure data lake\n",
    "\n",
    "    Args:\n",
    "    * old_dir (str): name of old directory\n",
    "    * new_dir (str): new directory name\n",
    "    * storage_account_name (str): adls storage account name\n",
    "    * storage_account_key (str): adls storage account secret key (not connection string)\n",
    "    * container (str): adls container\n",
    "    \"\"\"\n",
    "\n",
    "    # first connect\n",
    "    try:\n",
    "        service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\n",
    "            \"https\", storage_account_name), credential=storage_account_key)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    logger.info('Connected to data lake')\n",
    "\n",
    "    # now rename\n",
    "    try:\n",
    "        file_system_client = service_client.get_file_system_client(file_system=container)\n",
    "        directory_client = file_system_client.get_directory_client(old_dir)\n",
    "        logger.info('new_dir - ' + directory_client.file_system_name + '/' + new_dir)\n",
    "        logger.info('old_dir - ' + directory_client.file_system_name + '/' + old_dir)\n",
    "        directory_client.rename_directory(new_name=directory_client.file_system_name + '/' + new_dir)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:30.373798Z",
     "start_time": "2022-02-02T01:09:30.367089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"rename_adls_directory\" class=\"doc_header\"><code>rename_adls_directory</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>rename_adls_directory</code>(**`old_dir`**, **`new_dir`**, **`storage_account_name`**, **`storage_account_key`**, **`container`**)\n",
       "\n",
       "renames a directory in azure data lake\n",
       "\n",
       "Args:\n",
       "* old_dir (str): name of old directory\n",
       "* new_dir (str): new directory name\n",
       "* storage_account_name (str): adls storage account name\n",
       "* storage_account_key (str): adls storage account secret key (not connection string)\n",
       "* container (str): adls container"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(rename_adls_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:09:41.508760Z",
     "start_time": "2022-02-02T01:09:30.375551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sdsdedirectorytest is a valid\n",
      "INFO:__main__:ContainerAlreadyExists\n",
      "INFO:__main__:sdsdedirectorytest is a valid\n",
      "INFO:__main__:ContainerAlreadyExists\n",
      "INFO:__main__:Uploading testing/test.txt, to to Azure Storage old-dir/test.txt\n",
      "INFO:__main__:Azure Upload Complete\n",
      "INFO:__main__:Connected to data lake\n",
      "INFO:__main__:new_dir - sdsdedirectorytest/new-dir/\n",
      "INFO:__main__:old_dir - sdsdedirectorytest/old-dir/\n"
     ]
    }
   ],
   "source": [
    "old_path = 'old-dir/'\n",
    "new_path = 'new-dir/'\n",
    "\n",
    "fh = FileHandling(os.environ['DATALAKE_CONN_STR_SECRET'])\n",
    "fh.create_blob_container(container_name='sdsdedirectorytest')\n",
    "fh.upload(container_name='sdsdedirectorytest',\n",
    "          file_path='testing/test.txt',\n",
    "          dest=old_path+'test.txt',\n",
    "          overwrite=True)\n",
    "\n",
    "rename_adls_directory(old_dir=old_path,\n",
    "                      new_dir=new_path,\n",
    "                      storage_account_name=os.environ['azure_account'], \n",
    "                      storage_account_key=os.environ['DATALAKE_SECRET'], \n",
    "                      container='sdsdedirectorytest')\n",
    "\n",
    "fh.rm_container(container_name='sdsdedirectorytest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:05:47.876670Z",
     "start_time": "2022-02-02T01:05:47.174633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
