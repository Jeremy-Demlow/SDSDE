{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26fe87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:31.463621Z",
     "start_time": "2022-02-04T00:17:31.458849Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp snowflake.copyinto2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd77cc23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:32.128137Z",
     "start_time": "2022-02-04T00:17:31.467376Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d168573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:32.146745Z",
     "start_time": "2022-02-04T00:17:32.134817Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1affecf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:32.438446Z",
     "start_time": "2022-02-04T00:17:32.148496Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from sdsde import files\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7411a1",
   "metadata": {},
   "source": [
    "# ``SF_Copy_Into``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14721ad",
   "metadata": {},
   "source": [
    "TODO: Think about making a class to wrap these functions together for a simple API call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c2942",
   "metadata": {},
   "source": [
    "## ``make_data_lake_stage``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85bf53e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:32.895091Z",
     "start_time": "2022-02-04T00:17:32.875541Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def make_data_lake_stage(stage_name: str,\n",
    "                         account: str,\n",
    "                         container: str,\n",
    "                         data_lake_path: str,\n",
    "                         sas_token: str,\n",
    "                         file_type: str,\n",
    "                         compression: str = None,\n",
    "                         field_delimiter: str = None,\n",
    "                         field_optionally_enclosed_by: str = None,\n",
    "                         encoding: str = None):\n",
    "    \"\"\"\n",
    "    creates a data lake staging environment from snowflake this calls ``stage_query_generator``\n",
    "    which does the manipulation to the sdsde file that has the options currently available be\n",
    "    sure to rip this whole file out if there is something that you need to add before it can\n",
    "    be a request add to sdsde.\n",
    "\n",
    "    how to use:\n",
    "\n",
    "    ```python\n",
    "    stage_query = make_data_lake_stage(sf_connection=sf,\n",
    "                                       stage_name='sdsdestage_test',\n",
    "                                       account=os.environ['azure_account'],\n",
    "                                       container='sdsdetesting',\n",
    "                                       data_lake_path='testing_stage/',\n",
    "                                       field_delimiter=r\",\",\n",
    "                                       compression='None',\n",
    "                                       encoding='UTF-8',\n",
    "                                       sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                       file_type='csv'\n",
    "                                       )\n",
    "    sf.run_str_query(stage_query)\n",
    "\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "    * stage_name (str): name of stage in snowflake\n",
    "    * account (str): blob storage account\n",
    "    * container (str): blob container\n",
    "    * data_lake_path (str): path in the container to stage in\n",
    "    * sas_token (str): shared access token for blob\n",
    "    * file_type (str): for most use cases csv has been used but parquet and others can be used\n",
    "    * compression (str): the file type compression None if you want the raw file type like csv\n",
    "      AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n",
    "    * encoding (str): file encoding method used to parse files on snowflakes side\n",
    "    * field_delimiter (str): file type deliminter like ; or /t\n",
    "    \"\"\"\n",
    "    stage_url = f'azure://{account}.blob.core.windows.net/{container}/{data_lake_path}'\n",
    "    logger.info(f'Datalake Stage path that copy into will use {data_lake_path}')\n",
    "    stage_query = stage_query_generator(stage_name, stage_url, sas_token, field_delimiter,\n",
    "                                        encoding, compression, file_type=file_type,\n",
    "                                        field_optionally_enclosed_by=field_optionally_enclosed_by)\n",
    "    logger.info(f\"stage_query: \\n {stage_query.replace(sas_token, '**MASKED**')}\")\n",
    "    return stage_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1df545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:33.383114Z",
     "start_time": "2022-02-04T00:17:33.353809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"make_data_lake_stage\" class=\"doc_header\"><code>make_data_lake_stage</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>make_data_lake_stage</code>(**`stage_name`**:`str`, **`account`**:`str`, **`container`**:`str`, **`data_lake_path`**:`str`, **`sas_token`**:`str`, **`file_type`**:`str`, **`compression`**:`str`=*`None`*, **`field_delimiter`**:`str`=*`None`*, **`field_optionally_enclosed_by`**:`str`=*`None`*, **`encoding`**:`str`=*`None`*)\n",
       "\n",
       "creates a data lake staging environment from snowflake this calls ``stage_query_generator``\n",
       "which does the manipulation to the sdsde file that has the options currently available be\n",
       "sure to rip this whole file out if there is something that you need to add before it can\n",
       "be a request add to sdsde.\n",
       "\n",
       "how to use:\n",
       "\n",
       "```python\n",
       "stage_query = make_data_lake_stage(sf_connection=sf,\n",
       "                                   stage_name='sdsdestage_test',\n",
       "                                   account=os.environ['azure_account'],\n",
       "                                   container='sdsdetesting',\n",
       "                                   data_lake_path='testing_stage/',\n",
       "                                   field_delimiter=r\",\",\n",
       "                                   compression='None',\n",
       "                                   encoding='UTF-8',\n",
       "                                   sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
       "                                   file_type='csv'\n",
       "                                   )\n",
       "sf.run_str_query(stage_query)\n",
       "\n",
       "```\n",
       "\n",
       "Args:\n",
       "* stage_name (str): name of stage in snowflake\n",
       "* account (str): blob storage account\n",
       "* container (str): blob container\n",
       "* data_lake_path (str): path in the container to stage in\n",
       "* sas_token (str): shared access token for blob\n",
       "* file_type (str): for most use cases csv has been used but parquet and others can be used\n",
       "* compression (str): the file type compression None if you want the raw file type like csv\n",
       "  AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n",
       "* encoding (str): file encoding method used to parse files on snowflakes side\n",
       "* field_delimiter (str): file type deliminter like ; or /t"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(make_data_lake_stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e657c",
   "metadata": {},
   "source": [
    "### ``stage_query_generator``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a85c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:33.805448Z",
     "start_time": "2022-02-04T00:17:33.786254Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def stage_query_generator(stage_name: str,\n",
    "                          url: str,\n",
    "                          sas_token: str,\n",
    "                          field_delimiter: str,\n",
    "                          encoding: str,\n",
    "                          compression: str,\n",
    "                          file_type: str,\n",
    "                          field_optionally_enclosed_by: str):\n",
    "    \"\"\"\n",
    "    generates the snowflake query needed to create an external stage in\n",
    "    azure blob this is inside of ``make_data_lake_stage`` that is the only\n",
    "    due to the vars() that makes this a little simpler and more robost for this\n",
    "    use case.\n",
    "\n",
    "    TODO: figure out string manipulation inside of a list comp, but is not supported\n",
    "    in python 3.8 and figure out a better way to have the chained replace calls\n",
    "\n",
    "    Args:\n",
    "    * stage_name (str): name of the stage in snowflake\n",
    "    * url (str): azure formated string for account, container, and path\n",
    "    * sas_token (str): blob sas token for shared access\n",
    "    * field_delimiter (str): file type deliminter like ; or /t\n",
    "    * encoding (str): file encoding method used to parse files on snowflakes side\n",
    "    * compression (str): the file type compression None if you want the raw file type like csv\n",
    "    AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n",
    "    * file_type (str, optional): type of files expected in stage. Defaults to 'parquet'. Can use 'csv' as well.\n",
    "\n",
    "    Returns:\n",
    "    * str: snowflake query to create stage\n",
    "    \"\"\"\n",
    "    values = vars()\n",
    "    with open(os.path.join(files.__path__[0], 'stage_template.sql'), 'r') as f:\n",
    "        lines = f.read()\n",
    "        f.close()\n",
    "    for k, v in values.items():\n",
    "        if v is not None:\n",
    "            lines = lines.replace(f'<{k.upper()}>', v)\n",
    "        else:\n",
    "            lines = lines.replace(f\"'<{k.upper()}>'\", '').replace(f\"{k} =\", '').replace(f\"<{k.upper()}>\", '')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa41c005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:34.030568Z",
     "start_time": "2022-02-04T00:17:34.005208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"stage_query_generator\" class=\"doc_header\"><code>stage_query_generator</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>stage_query_generator</code>(**`stage_name`**:`str`, **`url`**:`str`, **`sas_token`**:`str`, **`field_delimiter`**:`str`, **`encoding`**:`str`, **`compression`**:`str`, **`file_type`**:`str`, **`field_optionally_enclosed_by`**:`str`)\n",
       "\n",
       "generates the snowflake query needed to create an external stage in\n",
       "azure blob this is inside of ``make_data_lake_stage`` that is the only\n",
       "due to the vars() that makes this a little simpler and more robost for this\n",
       "use case.\n",
       "\n",
       "TODO: figure out string manipulation inside of a list comp, but is not supported\n",
       "in python 3.8 and figure out a better way to have the chained replace calls\n",
       "\n",
       "Args:\n",
       "* stage_name (str): name of the stage in snowflake\n",
       "* url (str): azure formated string for account, container, and path\n",
       "* sas_token (str): blob sas token for shared access\n",
       "* field_delimiter (str): file type deliminter like ; or /t\n",
       "* encoding (str): file encoding method used to parse files on snowflakes side\n",
       "* compression (str): the file type compression None if you want the raw file type like csv\n",
       "AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE\n",
       "* file_type (str, optional): type of files expected in stage. Defaults to 'parquet'. Can use 'csv' as well.\n",
       "\n",
       "Returns:\n",
       "* str: snowflake query to create stage"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(stage_query_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e6a6a",
   "metadata": {},
   "source": [
    "## ``copy_into_adls_query_generator``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3a7959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:35.020722Z",
     "start_time": "2022-02-04T00:17:34.998392Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def copy_into_adls_query_generator(stage_name: str = None,\n",
    "                                   azure_sas_token: str = None,\n",
    "                                   data_lake_path: str = None,\n",
    "                                   azure_path: str = None,\n",
    "                                   sf_query: str = None,\n",
    "                                   table_name: str = None,\n",
    "                                   field_delimiter: str = None,\n",
    "                                   partition_by: str = None,\n",
    "                                   max_file_size: str = None,\n",
    "                                   header: str = None,\n",
    "                                   encoding: str = None,\n",
    "                                   file_type: str = None,\n",
    "                                   field_optionally_enclosed_by: str = None,\n",
    "                                   skip_header: str = None,\n",
    "                                   compression: str = None,\n",
    "                                   over_write: str = None):\n",
    "    \"\"\"\n",
    "    Generate query to dump snowflake data to an adls stage that has already been created.\n",
    "    There are a lot of optional arguements to allow the user to have a pleasurable experience\n",
    "    unlocking more than a user typically needs in our current sdsde stage of technology.\n",
    "\n",
    "    How To Use:\n",
    "\n",
    "    Note: that the sf_query could also be sf.execute_file(custom_query) to allow for more complex\n",
    "    queries to dump to azure for the use case at hand.\n",
    "\n",
    "    ```python\n",
    "    sg_query = copy_into_adls_query_generator(stage_name='sdsdestage_test',\n",
    "                                              azure_sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                              sf_query=r'''SELECT * FROM BIDE_EDWDB_ARA_PROD.dbo.FactScan\n",
    "                                                       WHERE ECID = 84412913 LIMIT 100''',\n",
    "                                              data_lake_path='testing_stage/',\n",
    "                                              max_file_size = '32000',\n",
    "                                              header='True',\n",
    "                                              over_write='True')\n",
    "    sf.run_str_query(sg_query)\n",
    "\n",
    "    ```\n",
    "    Args:\n",
    "    * stage_name (str): name of the stage in snowflake\n",
    "    * azure_sas_token (str): blob sas token for shared access when used to be able to move to a direct azure location\n",
    "    * data_lake_path (str, optional): Path inside of the created stage used when stage isn't the direct path\n",
    "    * azure_path (str, optional): Currently not supported do to not having a storage itergration, but this\n",
    "    would be a direct azure url path\n",
    "    * sf_query (str, optional): the sf query to use to dump to adls\n",
    "    * table_name (str, optional): full table name make sure you add the database.schema unless the sf is initialized\n",
    "    properly\n",
    "    * field_delimiter (str, optional): file type deliminter like ; or /t\n",
    "    * partition_by (str, optional): Currently not tested, but this would allow you to dump file structure to adls in\n",
    "    a partitioned manner\n",
    "    * max_file_size (str, optional): this allows snowflake to make files as big as this integer number or as small\n",
    "    * header (str, optional): True = Give the file columns names | False = Skip header and only dump data\n",
    "    * encoding (str, optional): file encoding engine to be read. Defaults to None.\n",
    "    * file_type (str, optional): . Defaults to None.\n",
    "    * field_optionally_enclosed_by (str, optional): . Defaults to None.\n",
    "    * skip_header (str, optional): this is to skip rows in a file. Defaults to None.\n",
    "    * compression (str, optional): what type of compression is the file in. Defaults to None.\n",
    "    * over_write (str, optional): True = overwrite files that are named the same | False = if file is there fail\n",
    "    Returns:\n",
    "    * str: Snowflake Query\n",
    "    \"\"\"\n",
    "    values = vars()\n",
    "    file_sql = 'copy_into_adls_from_sf_stage.sql' if azure_path is None else 'copy_into_adls_from_sf.sql'\n",
    "    with open(os.path.join(files.__path__[0], file_sql), 'r') as f:\n",
    "        lines = f.read()\n",
    "        f.close()\n",
    "    lines = lines.replace(\"type =\", '') if file_type is None else lines\n",
    "    for k, v in values.items():\n",
    "        if v is not None:\n",
    "            lines = lines.replace(f'<{k.upper()}>', v)\n",
    "        else:\n",
    "            lines = lines.replace(f\"<{k.upper()}>\", '').replace(f\"'<{k.upper()}>'\", '').replace(f\"{k} =\", '').replace(r\"''\", '')\n",
    "            lines = lines.replace(\"partition by = ()\", '')\n",
    "    if azure_path is None:\n",
    "        logger.info(f'\\n{lines}')\n",
    "    else:\n",
    "        logger.info(f'\\n{lines.replace(azure_sas_token, \"**MASKED**\")}')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf9a48ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:35.398069Z",
     "start_time": "2022-02-04T00:17:35.369003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"copy_into_adls_query_generator\" class=\"doc_header\"><code>copy_into_adls_query_generator</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>copy_into_adls_query_generator</code>(**`stage_name`**:`str`=*`None`*, **`azure_sas_token`**:`str`=*`None`*, **`data_lake_path`**:`str`=*`None`*, **`azure_path`**:`str`=*`None`*, **`sf_query`**:`str`=*`None`*, **`table_name`**:`str`=*`None`*, **`field_delimiter`**:`str`=*`None`*, **`partition_by`**:`str`=*`None`*, **`max_file_size`**:`str`=*`None`*, **`header`**:`str`=*`None`*, **`encoding`**:`str`=*`None`*, **`file_type`**:`str`=*`None`*, **`field_optionally_enclosed_by`**:`str`=*`None`*, **`skip_header`**:`str`=*`None`*, **`compression`**:`str`=*`None`*, **`over_write`**:`str`=*`None`*)\n",
       "\n",
       "Generate query to dump snowflake data to an adls stage that has already been created.\n",
       "There are a lot of optional arguements to allow the user to have a pleasurable experience\n",
       "unlocking more than a user typically needs in our current sdsde stage of technology.\n",
       "\n",
       "How To Use:\n",
       "\n",
       "Note: that the sf_query could also be sf.execute_file(custom_query) to allow for more complex\n",
       "queries to dump to azure for the use case at hand.\n",
       "\n",
       "```python\n",
       "sg_query = copy_into_adls_query_generator(stage_name='sdsdestage_test',\n",
       "                                          azure_sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
       "                                          sf_query=r'''SELECT * FROM BIDE_EDWDB_ARA_PROD.dbo.FactScan\n",
       "                                                   WHERE ECID = 84412913 LIMIT 100''',\n",
       "                                          data_lake_path='testing_stage/',\n",
       "                                          max_file_size = '32000',\n",
       "                                          header='True',\n",
       "                                          over_write='True')\n",
       "sf.run_str_query(sg_query)\n",
       "\n",
       "```\n",
       "Args:\n",
       "* stage_name (str): name of the stage in snowflake\n",
       "* azure_sas_token (str): blob sas token for shared access when used to be able to move to a direct azure location\n",
       "* data_lake_path (str, optional): Path inside of the created stage used when stage isn't the direct path\n",
       "* azure_path (str, optional): Currently not supported do to not having a storage itergration, but this\n",
       "would be a direct azure url path\n",
       "* sf_query (str, optional): the sf query to use to dump to adls\n",
       "* table_name (str, optional): full table name make sure you add the database.schema unless the sf is initialized\n",
       "properly\n",
       "* field_delimiter (str, optional): file type deliminter like ; or /t\n",
       "* partition_by (str, optional): Currently not tested, but this would allow you to dump file structure to adls in\n",
       "a partitioned manner\n",
       "* max_file_size (str, optional): this allows snowflake to make files as big as this integer number or as small\n",
       "* header (str, optional): True = Give the file columns names | False = Skip header and only dump data\n",
       "* encoding (str, optional): file encoding engine to be read. Defaults to None.\n",
       "* file_type (str, optional): . Defaults to None.\n",
       "* field_optionally_enclosed_by (str, optional): . Defaults to None.\n",
       "* skip_header (str, optional): this is to skip rows in a file. Defaults to None.\n",
       "* compression (str, optional): what type of compression is the file in. Defaults to None.\n",
       "* over_write (str, optional): True = overwrite files that are named the same | False = if file is there fail\n",
       "Returns:\n",
       "* str: Snowflake Query"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(copy_into_adls_query_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fe24c",
   "metadata": {},
   "source": [
    "##  ``copy_into_sf_query_generator``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0168094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:35.841049Z",
     "start_time": "2022-02-04T00:17:35.822452Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def copy_into_sf_query_generator(database: str,\n",
    "                                 schema: str,\n",
    "                                 table_name: str,\n",
    "                                 file_type: str,\n",
    "                                 stage_name: str = None,\n",
    "                                 data_lake_path: str = None,\n",
    "                                 azure_path: str = None,\n",
    "                                 azure_sas_token: str = None,\n",
    "                                 pattern: str = None,\n",
    "                                 skip_header: str = None,\n",
    "                                 compression: str = None,\n",
    "                                 field_delimiter: str = None,\n",
    "                                 encoding: str = None):\n",
    "    \"\"\"\n",
    "    Generates query to take data from a snowflake stage and puts the data\n",
    "    directly into a table requested by this function and only if the table in sf\n",
    "    doesn't exisit it will fail in the how to will show you how to not have it fail\n",
    "    out, but the notebook shows how to make a snowflake with the data that is coming\n",
    "    from adls.\n",
    "\n",
    "    How to use:\n",
    "\n",
    "    ```python\n",
    "    cp_query = copy_into_sf_query_generator(stage_name='sdsdestage_test/',\n",
    "                                            data_lake_path='testing_stage/',\n",
    "                                            table_name='sdsde_DELETE_TEST_TABLE',\n",
    "                                            database=sf.sfDatabase,\n",
    "                                            schema=sf.sfSchema,\n",
    "                                            skip_header='1',\n",
    "                                            field_delimiter=',',\n",
    "                                            encoding='UTF-8',\n",
    "                                            file_type='csv',\n",
    "                                            pattern='.*.csv')\n",
    "    try:\n",
    "        sf.run_str_query(cp_query)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error Created Trying to Copy Into sf table {e}')\n",
    "        logger.warning('Most this table needs to be initialized' )\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "    * stage_name (str): name of the stage in snowflake\n",
    "    * data_lake_path (str): Path inside of the created stage used when stage isn't the direct path\n",
    "    * database (str): snowflake database\n",
    "    * schema (str): snowflake schema\n",
    "    * table_name (str): snowflake table name that data will be put into\n",
    "    * file_type (str): file type that will be ingested\n",
    "    * pattern (str): either this is grabbing many files or the data_lake_path will be point to one file to ingest\n",
    "    if there is a patter like .*.csv it will use regex to find files with this regex pattern\n",
    "    * skip_header (str, optional): during development files with columns failed so skipping the header with column names is needed\n",
    "    * compression (str, optional): what type of compression is being used for this set of data\n",
    "    * field_delimiter (str, optional): file type deliminter like ; or /t\n",
    "    * encoding (str, optional): file encoding method used to parse files on snowflakes side\n",
    "\n",
    "    Returns:\n",
    "    * str: snowflake query to create stage\n",
    "    \"\"\"\n",
    "    values = vars()\n",
    "    file_sql = 'copy_into_sf_table.sql' if azure_path is None else 'copy_into_sf_table_direct.sql'\n",
    "    with open(os.path.join(files.__path__[0], file_sql), 'r') as f:\n",
    "        lines = f.read()\n",
    "        f.close()\n",
    "    for k, v in values.items():\n",
    "        if v is not None:\n",
    "            lines = lines.replace(f'<{k.upper()}>', v)\n",
    "        else:\n",
    "            lines = lines.replace(f'<{k.upper()}>', '').replace(f\"{k} = ''\", '').replace(f\"{k} =\", '')\n",
    "    logger.info(f'\\n{lines}')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298a07a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:36.256900Z",
     "start_time": "2022-02-04T00:17:36.229357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"copy_into_sf_query_generator\" class=\"doc_header\"><code>copy_into_sf_query_generator</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>copy_into_sf_query_generator</code>(**`database`**:`str`, **`schema`**:`str`, **`table_name`**:`str`, **`file_type`**:`str`, **`stage_name`**:`str`=*`None`*, **`data_lake_path`**:`str`=*`None`*, **`azure_path`**:`str`=*`None`*, **`azure_sas_token`**:`str`=*`None`*, **`pattern`**:`str`=*`None`*, **`skip_header`**:`str`=*`None`*, **`compression`**:`str`=*`None`*, **`field_delimiter`**:`str`=*`None`*, **`encoding`**:`str`=*`None`*)\n",
       "\n",
       "Generates query to take data from a snowflake stage and puts the data\n",
       "directly into a table requested by this function and only if the table in sf\n",
       "doesn't exisit it will fail in the how to will show you how to not have it fail\n",
       "out, but the notebook shows how to make a snowflake with the data that is coming\n",
       "from adls.\n",
       "\n",
       "How to use:\n",
       "\n",
       "```python\n",
       "cp_query = copy_into_sf_query_generator(stage_name='sdsdestage_test/',\n",
       "                                        data_lake_path='testing_stage/',\n",
       "                                        table_name='sdsde_DELETE_TEST_TABLE',\n",
       "                                        database=sf.sfDatabase,\n",
       "                                        schema=sf.sfSchema,\n",
       "                                        skip_header='1',\n",
       "                                        field_delimiter=',',\n",
       "                                        encoding='UTF-8',\n",
       "                                        file_type='csv',\n",
       "                                        pattern='.*.csv')\n",
       "try:\n",
       "    sf.run_str_query(cp_query)\n",
       "except Exception as e:\n",
       "    logger.error(f'Error Created Trying to Copy Into sf table {e}')\n",
       "    logger.warning('Most this table needs to be initialized' )\n",
       "```\n",
       "\n",
       "Args:\n",
       "* stage_name (str): name of the stage in snowflake\n",
       "* data_lake_path (str): Path inside of the created stage used when stage isn't the direct path\n",
       "* database (str): snowflake database\n",
       "* schema (str): snowflake schema\n",
       "* table_name (str): snowflake table name that data will be put into\n",
       "* file_type (str): file type that will be ingested\n",
       "* pattern (str): either this is grabbing many files or the data_lake_path will be point to one file to ingest\n",
       "if there is a patter like .*.csv it will use regex to find files with this regex pattern\n",
       "* skip_header (str, optional): during development files with columns failed so skipping the header with column names is needed\n",
       "* compression (str, optional): what type of compression is being used for this set of data\n",
       "* field_delimiter (str, optional): file type deliminter like ; or /t\n",
       "* encoding (str, optional): file encoding method used to parse files on snowflakes side\n",
       "\n",
       "Returns:\n",
       "* str: snowflake query to create stage"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(copy_into_sf_query_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec703a7",
   "metadata": {},
   "source": [
    "## ``parquet_copy_into_sf_query_generator``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0147ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:36.675322Z",
     "start_time": "2022-02-04T00:17:36.652603Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def parquet_copy_into_sf_query_generator(data_types: dict,\n",
    "                                         database: str,\n",
    "                                         schema: str,\n",
    "                                         table_name: str,\n",
    "                                         file_type: str,\n",
    "                                         stage_name: str = None,\n",
    "                                         data_lake_path: str = None,\n",
    "                                         azure_path: str = None,\n",
    "                                         azure_sas_token: str = None,\n",
    "                                         pattern: str = None,\n",
    "                                         skip_header: str = None,\n",
    "                                         compression: str = None,\n",
    "                                         field_delimiter: str = None,\n",
    "                                         encoding: str = None,\n",
    "                                         infer_dtypes: bool = False,\n",
    "                                         header: bool = True):\n",
    "    \"\"\"\n",
    "    Generates query to take data from a snowflake stage and puts the data\n",
    "    directly into a table requested by this function and only if the table in sf\n",
    "    doesn't exisit it will fail in the how to will show you how to not have it fail\n",
    "    out, but the notebook shows how to make a snowflake with the data that is coming\n",
    "    from adls.\n",
    "\n",
    "    How to use:\n",
    "\n",
    "    ```python\n",
    "    cp_query = copy_into_sf_query_generator(stage_name='sdsdestage_test/',\n",
    "                                            data_lake_path='testing_stage/',\n",
    "                                            table_name='sdsde_DELETE_TEST_TABLE',\n",
    "                                            database=sf.sfDatabase,\n",
    "                                            schema=sf.sfSchema,\n",
    "                                            skip_header='1',\n",
    "                                            field_delimiter=',',\n",
    "                                            encoding='UTF-8',\n",
    "                                            file_type='csv',\n",
    "                                            pattern='.*.csv')\n",
    "    try:\n",
    "        sf.run_str_query(cp_query)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error Created Trying to Copy Into sf table {e}')\n",
    "        logger.warning('Most this table needs to be initialized' )\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "    * stage_name (str): name of the stage in snowflake\n",
    "    * data_lake_path (str): Path inside of the created stage used when stage isn't the direct path\n",
    "    * database (str): snowflake database\n",
    "    * schema (str): snowflake schema\n",
    "    * table_name (str): snowflake table name that data will be put into\n",
    "    * file_type (str): file type that will be ingested\n",
    "    * pattern (str): either this is grabbing many files or the data_lake_path will be point to one file to ingest\n",
    "    if there is a patter like .*.csv it will use regex to find files with this regex pattern\n",
    "    * skip_header (str, optional): during development files with columns failed so skipping the header with column names is needed\n",
    "    * compression (str, optional): what type of compression is being used for this set of data\n",
    "    * field_delimiter (str, optional): file type deliminter like ; or /t\n",
    "    * encoding (str, optional): file encoding method used to parse files on snowflakes side\n",
    "\n",
    "    Returns:\n",
    "    * str: snowflake query to create stage\n",
    "    \"\"\"\n",
    "    values = vars()\n",
    "    del values['data_types']\n",
    "    del values['infer_dtypes']\n",
    "    del values['header']\n",
    "    file_sql = 'copy_into_sf_table_parquet.sql' if azure_path is None else 'copy_into_sf_table_direct_parquet.sql'\n",
    "    with open(os.path.join(files.__path__[0], file_sql), 'r') as f:\n",
    "        lines = f.read()\n",
    "        f.close()\n",
    "    for k, v in values.items():\n",
    "        if v is not None:\n",
    "            lines = lines.replace(f'<{k.upper()}>', v)\n",
    "        else:\n",
    "            lines = lines.replace(f'<{k.upper()}>', '').replace(f\"{k} = ''\", '').replace(f\"{k} =\", '')\n",
    "    query = \"\"\"SELECT FEATURES_HERE\"\"\"\n",
    "    ind = 0\n",
    "    columns = len(data_types.keys())\n",
    "    if header is True:\n",
    "        for k, v in data_types.items():\n",
    "            query = query.replace('FEATURES_HERE', f'$1:\"{k}\", FEATURES_HERE')\n",
    "            if ind < columns:\n",
    "                query = query.replace(', FEATURES_HERE \\n', '')\n",
    "            else:\n",
    "                query = query.replace('FEATURES_HERE \\n', '')\n",
    "            ind += 1\n",
    "    else:\n",
    "        for k, v in data_types.items():\n",
    "            query = query.replace('FEATURES_HERE', f'$1:\"_COL_{ind}::{return_sf_type(str(v), varchar=False, infer=infer_dtypes)}\" as {k}, FEATURES_HERE')\n",
    "            if ind < columns:\n",
    "                query = query.replace(', FEATURES_HERE \\n', '')\n",
    "            else:\n",
    "                query = query.replace('FEATURES_HERE \\n', '')\n",
    "            ind += 1\n",
    "    query = query.replace(', FEATURES_HERE', '')\n",
    "    lines = lines.replace('<SELECT_STATEMENT>', query)\n",
    "    if azure_path is None:\n",
    "        logger.info(f'\\n{lines}')\n",
    "    else:\n",
    "        logger.info(f'\\n{lines.replace(azure_sas_token, \"**MASKED**\")}')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045be1f",
   "metadata": {},
   "source": [
    "### ``create_sf_table_from_df`` & ``return_sf_type``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99542076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:37.330788Z",
     "start_time": "2022-02-04T00:17:37.308618Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def clean_special_chars(text):\n",
    "    \"\"\"\n",
    "    small nlp clean up tool to take odd characters that could be\n",
    "    in vendor data inside of column names and then replaces empty\n",
    "    spaces with ``_``\n",
    "\n",
    "    Args:\n",
    "        text (str): dataframe column names as strings\n",
    "\n",
    "    Returns:\n",
    "        str: clean column name\n",
    "    \"\"\"\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'  # noqa:\n",
    "    punct += '©^®` <→°€™› ♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'  # noqa:\n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')\n",
    "        text = text.replace(' ', '_')\n",
    "    return text\n",
    "\n",
    "def create_sf_table_from_df(df: pd.DataFrame, table_name_sf: str, varchar: bool):\n",
    "    \"\"\"\n",
    "    Dynamically create a table from a dataframe and\n",
    "    change the dtypes to snowflake dytpes this may have\n",
    "    a limitation, but can be added.\n",
    "\n",
    "    Args:\n",
    "    * df (pd.DataFrame): data frame to get dtypes\n",
    "    * table_name_sf (str): snowflake table name\n",
    "    * varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
    "    \"\"\"\n",
    "    select_query = f'''\n",
    "        create or replace table {table_name_sf} (FEATURES_HERE);\n",
    "        '''\n",
    "    for k, v in dict(df.dtypes).items():\n",
    "        select_query = select_query.replace('FEATURES_HERE', f'{clean_special_chars(k)} {return_sf_type(str(v), varchar=varchar)}, FEATURES_HERE')\n",
    "    select_query = select_query.replace(', FEATURES_HERE', '')\n",
    "    logger.info(select_query)\n",
    "    logger.warning('Note: Remember this is created data types for sf based on this file if')\n",
    "    return select_query\n",
    "\n",
    "def create_sf_table_from_dict(columns_and_types: dict, table_name_sf: str, varchar: bool, infer_types: bool = False):\n",
    "    \"\"\"\n",
    "    Dynamically create a table from a dataframe and\n",
    "    change the dtypes to snowflake dytpes this may have\n",
    "    a limitation, but can be added.\n",
    "\n",
    "    Args:\n",
    "    * df (pd.DataFrame): data frame to get dtypes\n",
    "    * table_name_sf (str): snowflake table name\n",
    "    * varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
    "    \"\"\"\n",
    "    select_query = f'''\n",
    "        create or replace table {table_name_sf} (FEATURES_HERE);\n",
    "        '''\n",
    "    for k, v in columns_and_types.items():\n",
    "        select_query = select_query.replace('FEATURES_HERE', f'{clean_special_chars(k)} {return_sf_type(str(v), infer=infer_types, varchar=varchar)}, FEATURES_HERE')\n",
    "    select_query = select_query.replace(', FEATURES_HERE', '')\n",
    "    logger.info(select_query)\n",
    "    logger.warning('Note: Remember this is created data types for sf based on this file if')\n",
    "    return select_query\n",
    "\n",
    "def return_sf_type(dtype: str, varchar: bool, infer: bool = True):\n",
    "    \"\"\"\n",
    "    Simple utility function that tries to make the process of making a\n",
    "    snowflake table dynamic and there are of course situtation this will fail\n",
    "    this is trying to solve 80% of all the data types\n",
    "\n",
    "    TODO: make more robust if possible, but for now complications will lead\n",
    "    to the user needing to just default everything to a VARCHAR search bool\n",
    "    types that didn't work at the time of creation of this function\n",
    "\n",
    "    Args:\n",
    "    * dtype (str): dtype from a df in sting form\n",
    "    * varchar (bool): to default all variables to VARCHAR\n",
    "\n",
    "    Returns:\n",
    "    * str: snowflake dtype\n",
    "    \"\"\"\n",
    "    if infer is True:\n",
    "        if varchar is True:\n",
    "            dtype = 'VARCHAR'\n",
    "        elif 'int' in dtype.lower():\n",
    "            dtype = 'NUMBER'\n",
    "        elif 'float' in dtype.lower():\n",
    "            dtype = 'FLOAT'\n",
    "        elif 'object' in dtype.lower():\n",
    "            dtype = 'VARCHAR'\n",
    "        elif 'bool' in dtype.lower():\n",
    "            dtype = 'VARCHAR'  # TODO: Limitation found before change once resloved by sf\n",
    "        elif 'date' in dtype.lower():\n",
    "            dtype = 'DATETIME'  # TODO: Might break with certain datetimes most generic\n",
    "        else:\n",
    "            logger.error('odd dtype not seen needs to be resloved...')\n",
    "            sys.exit()\n",
    "    else:\n",
    "        return dtype\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "584f238d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:37.563531Z",
     "start_time": "2022-02-04T00:17:37.539259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"return_sf_type\" class=\"doc_header\"><code>return_sf_type</code><a href=\"__main__.py#L63\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>return_sf_type</code>(**`dtype`**:`str`, **`varchar`**:`bool`, **`infer`**:`bool`=*`True`*)\n",
       "\n",
       "Simple utility function that tries to make the process of making a\n",
       "snowflake table dynamic and there are of course situtation this will fail\n",
       "this is trying to solve 80% of all the data types\n",
       "\n",
       "TODO: make more robust if possible, but for now complications will lead\n",
       "to the user needing to just default everything to a VARCHAR search bool\n",
       "types that didn't work at the time of creation of this function\n",
       "\n",
       "Args:\n",
       "* dtype (str): dtype from a df in sting form\n",
       "* varchar (bool): to default all variables to VARCHAR\n",
       "\n",
       "Returns:\n",
       "* str: snowflake dtype"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"create_sf_table_from_df\" class=\"doc_header\"><code>create_sf_table_from_df</code><a href=\"__main__.py#L21\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>create_sf_table_from_df</code>(**`df`**:`DataFrame`, **`table_name_sf`**:`str`, **`varchar`**:`bool`)\n",
       "\n",
       "Dynamically create a table from a dataframe and\n",
       "change the dtypes to snowflake dytpes this may have\n",
       "a limitation, but can be added.\n",
       "\n",
       "Args:\n",
       "* df (pd.DataFrame): data frame to get dtypes\n",
       "* table_name_sf (str): snowflake table name\n",
       "* varchar: (bool, optional): this will default all dytpes to varchars if True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(return_sf_type)\n",
    "show_doc(create_sf_table_from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8ea6d",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a923d24",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "This will ease the process of moving snowflake data to ADLS as well as ease the process of taking data from the ADLS to snowflake. This has been seen in a couple places like APT and adobe click stream and trying to get a good way of attacking these problems outside of using ``SnowflakeConnect`` and taking a little more advantage.\n",
    "\n",
    "Another reason for this is to we will be able to take in the data from ADLS for our models with this method. I haven't quite thought about how this is going to work, but I do know that this is a really good use case. This idea need fleshing out\n",
    "\n",
    "**Do We Need It?**\n",
    "\n",
    "Do we need anything that gets added to the sdsde Library not really we can always hack our way through, but the point of software is to make the 80-90% of actions simpler and unified so when things get upgrades or break that one fix will fix all down stream use cases. \n",
    "\n",
    "I am sure that a class will be written to make this all a lot simpler and less lines of code, but learning into *patches* is a gap in my current knowledge sphere.  I have seen/heard about monkey patching a function for these flexible use cases when I have time I will look into that as it's a good skill I believe. This is seen a lot in Dask/Snorkel.\n",
    "\n",
    "**Does it work?**\n",
    "\n",
    "Sure does, but there are probably things that need to be added and there are pieces that haven't been explored yet like partition by.\n",
    "\n",
    "**What are things to still look into?**\n",
    "\n",
    "Probably a lot, but right now the ability to name the files that are being dumped into azure for example APT has to have a naming convention for the files we drop into their SFTP this means the files can't be ``data_0_0.csv``, but currently haven't  looked into that much.\n",
    "\n",
    "Haven't tested complex queries in the SF to ADLS just did a simple where Brian Trost because he is a good candidate employee as well as a skier that has the worst skierability on the team and make the algo for skierability favor him (All Facts).\n",
    "\n",
    "\n",
    "###### USE Case 1: SF -> ADLS\n",
    "\n",
    "Step 1: Make A Data Stage In SnowFlake ``make_data_lake_stage``\n",
    "\n",
    "Step 2: Send data to adls stage ``copy_into_adls_query_generator``\n",
    "\n",
    "What you do from there is fully up to you, but if you have an stfp use case check out the SFTP.ipynb for help on that\n",
    "\n",
    "\n",
    "###### USE Case 2: ADLS-> SF\n",
    "\n",
    "Step 1: Make A Data Stage In SnowFlake ``make_data_lake_stage``\n",
    "\n",
    "Step 2: Create a table for data from ADLS to move into ``create_sf_table_from_df``\n",
    "\n",
    "Step 3: Move ADLS Data to Snowflake table ``copy_into_sf_query_generator``\n",
    "\n",
    "> In this notebook we take data from snowflake and move to to adls and then move that data to another table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5b271",
   "metadata": {},
   "source": [
    "## GZIP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42344ed3",
   "metadata": {},
   "source": [
    "### Recommended Stage Approach From SnowFlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de9cb70a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:25:28.390840Z",
     "start_time": "2022-02-02T01:25:01.424718Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:__main__:Datalake Stage path that copy into will use testing_stage/\n",
      "INFO:__main__:stage_query: \n",
      " create or replace stage sdsdestage_test\n",
      "url='azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = csv     compression = GZIP field_optionally_enclosed_by = '\"')\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Stage area SDSDESTAGE_TEST successfully created.\n",
      "INFO:__main__:\n",
      "COPY INTO @sdsdestage_test/testing_stage/\n",
      "FROM  (SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100)\n",
      "\n",
      "max_file_size = 32000\n",
      "overwrite = True\n",
      "header = True;\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:__main__:\n",
      "copy into MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE\n",
      "from @sdsdestage_test/testing_stage/\n",
      "file_format = (type = csv   compression = GZIP skip_header = 1)\n",
      "pattern = '.*.csv.gz';\n",
      "INFO:__main__:files in blob ['data_0_0_0.csv.gz']\n",
      "INFO:sdsde.azure.filehandling:testing_stage/testing_stage/data_0_0_0.csv.gz to ./data_0_0_0.csv.gz\n",
      "INFO:__main__:df size (100, 8)\n",
      "INFO:__main__:\n",
      "        create or replace table MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE (ECID VARCHAR, LIKELIHOODTORETURNRATE VARCHAR, LIKELIHOODTORETURNLABEL VARCHAR, FISCALYEAR VARCHAR, MODELTYPE VARCHAR, EXPERIMENT VARCHAR, MODELNAME VARCHAR, UPLOADTIME VARCHAR);\n",
      "        \n",
      "WARNING:__main__:Note: Remember this is created data types for sf based on this file if\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Table SDSDE_DELETE_TEST_TABLE successfully created.\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/testing_stage/data_0_0_0.csv.gz']\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/testing_stage']\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDE_DELETE_TEST_TABLE successfully dropped.\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDESTAGE_TEST successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "from sdsde.snowflake.query import SnowflakeConnect\n",
    "from sdsde.wrapper.azurewrapper import blob_puller\n",
    "from sdsde.azure.filehandling import FileHandling\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "sf = SnowflakeConnect(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "\n",
    "stage_query = make_data_lake_stage(stage_name='sdsdestage_test',\n",
    "                                   account=os.environ['azure_account'],\n",
    "                                   container='sdsdetesting',\n",
    "                                   data_lake_path='testing_stage/',\n",
    "                                   compression='GZIP',\n",
    "                                   sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                   file_type='csv',\n",
    "                                   field_optionally_enclosed_by= '\\042' # '\\042' # double quotes ascii # '\\047' single quote\n",
    "                                )\n",
    "sf.run_str_query(stage_query)\n",
    "sg_query = copy_into_adls_query_generator(stage_name='sdsdestage_test',\n",
    "                                          sf_query=r'SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100',\n",
    "                                          data_lake_path='testing_stage/',\n",
    "                                          max_file_size = '32000',\n",
    "                                          header='True',\n",
    "                                          over_write='True')\n",
    "sf.run_str_query(sg_query)\n",
    "cp_query = copy_into_sf_query_generator(stage_name='sdsdestage_test/',\n",
    "                                        data_lake_path='testing_stage/',\n",
    "                                        table_name='sdsde_DELETE_TEST_TABLE',\n",
    "                                        database=sf.sfDatabase,\n",
    "                                        schema=sf.sfSchema,\n",
    "                                        skip_header='1',\n",
    "                                        compression='GZIP',\n",
    "                                        file_type='csv',\n",
    "                                        pattern='.*.csv.gz')\n",
    "\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "file_list = fh.ls_blob(container_name='sdsdetesting',  path='testing_stage/testing_stage', recursive=True)\n",
    "logger.info(f'files in blob {file_list}')\n",
    "\n",
    "blob_puller(files=['testing_stage/testing_stage/' + file_list[0]],\n",
    "            connection_str=fh.connection_string,\n",
    "            container_name='sdsdetesting',\n",
    "            drop_location='.',\n",
    "            overwrite=True)\n",
    "\n",
    "df = pd.read_csv(file_list[0])\n",
    "logger.info(f'df size {df.shape}')\n",
    "create_table_sql = create_sf_table_from_df(df=df, \n",
    "                                           table_name_sf='MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE',\n",
    "                                           varchar=True)\n",
    "sf.run_str_query(create_table_sql)\n",
    "\n",
    "try:\n",
    "    sf.run_str_query(cp_query)\n",
    "except Exception as e:\n",
    "    logger.error(f'Error Created Trying to Copy Into sf table {e}')\n",
    "    logger.warning('Most this table needs to be initialized' )\n",
    "\n",
    "df = sf.run_str_query('SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE;')\n",
    "for f in fh.ls_blob(container_name='sdsdetesting', path='testing_stage/', recursive=True):\n",
    "    if f == 'testing_stage':\n",
    "        pass\n",
    "    else:\n",
    "        fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/' + f)\n",
    "fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/')\n",
    "assert df.shape == (100, 8), 'Copy Into Failed to Load Expected'\n",
    "sf.run_str_query(\"DROP TABLE MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE;\")\n",
    "sf.run_str_query('DROP STAGE sdsdestage_test;')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8f0b8",
   "metadata": {},
   "source": [
    "### Using Azure Paths Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90b29fd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:30:14.839711Z",
     "start_time": "2022-02-02T01:30:11.309540Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "COPY INTO 'azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/'\n",
      "FROM  (SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100)\n",
      "\n",
      "max_file_size = 32000\n",
      "OVERWRITE = True\n",
      "file_format = (type = CSV field_optionally_enclosed_by = '\"'     compression = GZIP  )\n",
      "credentials= (azure_sas_token = '**MASKED**')\n",
      "header = True;\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows_unloaded</th>\n",
       "      <th>input_bytes</th>\n",
       "      <th>output_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>12004</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rows_unloaded  input_bytes  output_bytes\n",
       "0            100        12004          1396"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "sg_query = copy_into_adls_query_generator(sf_query=r'SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100',\n",
    "                                          azure_sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                          azure_path='azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/',\n",
    "                                          max_file_size = '32000',\n",
    "                                          compression='GZIP',\n",
    "                                          file_type='CSV',\n",
    "                                          field_optionally_enclosed_by= '\\042',\n",
    "                                          header='True',\n",
    "                                          over_write='True')\n",
    "sf.run_str_query(sg_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddaa47",
   "metadata": {},
   "source": [
    "```python\n",
    "try:\n",
    "    sf.run_str_query(cp_query)\n",
    "except Exception as e:\n",
    "    logger.error(f'Error Created Trying to Copy Into sf table {e}')\n",
    "    logger.warning('Most this table needs to be initialized' )\n",
    "\n",
    "```\n",
    "\n",
    "> Note: This above failed telling the user that this action can not be completed when working with this I have allowed the python session to keep going on to the next thing as I have a try/catch here, but this is really to show the user what is happening if using this in production we would most likely want this to completely fail out with something like a sys.exit(1)\n",
    "\n",
    "This above is known however for now while this is in testing keeping this here is the ideal way of approaching this right now what I would do in a class is catch this error and do what happens below.\n",
    "\n",
    "> Note: Below there is only one file in the blob location what we would want is to do ``file_list[0][0]`` most likely to only grab one of the files.\n",
    "\n",
    "Also make sure you notice the create varchar = False what this means is that the table will not default to only VarChars and will do it's best to use pandas to understand the dtypes of the file and another hint here is make sure low_memory=False is on if you to not run into issues. Varchar default was created because vendors don't take care of their data i.e. MTA (nielson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2316dba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:30:20.543491Z",
     "start_time": "2022-02-02T01:30:17.433222Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:files in blob ['data_0_0_0.csv.gz']\n",
      "INFO:sdsde.azure.filehandling:testing_stage/data_0_0_0.csv.gz to ./data_0_0_0.csv.gz\n",
      "INFO:__main__:df size (100, 8)\n",
      "INFO:__main__:\n",
      "        create or replace table MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE (ECID VARCHAR, LIKELIHOODTORETURNRATE VARCHAR, LIKELIHOODTORETURNLABEL VARCHAR, FISCALYEAR VARCHAR, MODELTYPE VARCHAR, EXPERIMENT VARCHAR, MODELNAME VARCHAR, UPLOADTIME VARCHAR);\n",
      "        \n",
      "WARNING:__main__:Note: Remember this is created data types for sf based on this file if\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Table SDSDE_DELETE_TEST_TABLE successfully created.\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "file_list = fh.ls_blob(container_name='sdsdetesting',  path='testing_stage/', recursive=True)\n",
    "logger.info(f'files in blob {file_list}')\n",
    "\n",
    "blob_puller(files=['testing_stage/' + file_list[0]],\n",
    "            connection_str=fh.connection_string,\n",
    "            container_name='sdsdetesting',\n",
    "            drop_location='.',\n",
    "            overwrite=True)\n",
    "\n",
    "df = pd.read_csv(file_list[0])\n",
    "logger.info(f'df size {df.shape}')\n",
    "\n",
    "create_table_sql = create_sf_table_from_df(df=df, \n",
    "                                           table_name_sf='MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE',\n",
    "                                           varchar=True)\n",
    "sf.run_str_query(create_table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715aba1",
   "metadata": {},
   "source": [
    "Now that we have created the expected table of interest we are trying to create we are now ready to attempt this again to have an ADLS file be directly dumped into a snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aa6e476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:30:33.508198Z",
     "start_time": "2022-02-02T01:30:20.545421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "copy into MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE\n",
      "from 'azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/'\n",
      "file_format = (type = csv   compression = GZIP skip_header = 1)\n",
      "credentials= (azure_sas_token = '?sv=2019-12-12&ss=bfqt&srt=sco&sp=rwdlacupx&se=2031-01-22T06:17:14Z&st=2021-01-21T22:17:14Z&spr=https&sig=kIHogByJjyVWyL6XupA0CBUB1iw12%2FeXWFQiOj5fB5c%3D')\n",
      "pattern = '.*.csv.gz';\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDE_DELETE_TEST_TABLE successfully dropped.\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/data_0_0_0.csv.gz']\n",
      "WARNING:sdsde.azure.filehandling:location in blob is empty\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "cp_query = copy_into_sf_query_generator(azure_path='azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/',\n",
    "                                        azure_sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                        table_name='sdsde_DELETE_TEST_TABLE',\n",
    "                                        database=sf.sfDatabase,\n",
    "                                        schema=sf.sfSchema,\n",
    "                                        skip_header='1',\n",
    "                                        compression='GZIP',\n",
    "                                        file_type='csv',\n",
    "                                        pattern='.*.csv.gz')\n",
    "\n",
    "sf.run_str_query(cp_query)\n",
    "\n",
    "df = sf.run_str_query('SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE;')\n",
    "sf.run_str_query(\"DROP TABLE MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE;\")\n",
    "\n",
    "assert df.shape == (100, 8), 'Copy Into Failed to Load Expected'\n",
    "\n",
    "for f in fh.ls_blob(container_name='sdsdetesting', path='testing_stage/', recursive=True):\n",
    "    if f == 'testing_stage':\n",
    "        pass\n",
    "    else:\n",
    "        fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/' + f)\n",
    "fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/')\n",
    "os.unlink(file_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670975a",
   "metadata": {},
   "source": [
    "## CSV Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "676c524e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:30:40.643643Z",
     "start_time": "2022-02-02T01:30:33.509466Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Datalake Stage path that copy into will use testing_stage/\n",
      "INFO:__main__:stage_query: \n",
      " create or replace stage sdsdestage_test\n",
      "url='azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = csv field_delimiter = ',' encoding = 'UTF-8' compression = None  )\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Stage area SDSDESTAGE_TEST successfully created.\n",
      "INFO:__main__:\n",
      "COPY INTO @sdsdestage_test/testing_stage/\n",
      "FROM  (SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100)\n",
      "\n",
      "max_file_size = 32000\n",
      "overwrite = True\n",
      "header = True;\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows_unloaded</th>\n",
       "      <th>input_bytes</th>\n",
       "      <th>output_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>10578</td>\n",
       "      <td>10578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rows_unloaded  input_bytes  output_bytes\n",
       "0            100        10578         10578"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip\n",
    "stage_query = make_data_lake_stage(stage_name='sdsdestage_test',\n",
    "                                   account=os.environ['azure_account'],\n",
    "                                   container='sdsdetesting',\n",
    "                                   data_lake_path='testing_stage/',\n",
    "                                   field_delimiter=r\",\",\n",
    "                                   compression='None',\n",
    "                                   encoding='UTF-8',\n",
    "                                   sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                   file_type='csv'\n",
    "                                   )\n",
    "sf.run_str_query(stage_query)\n",
    "sg_query = copy_into_adls_query_generator(stage_name='sdsdestage_test',\n",
    "                                          azure_sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                          sf_query=r'SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100',\n",
    "                                          data_lake_path='testing_stage/',\n",
    "                                          max_file_size = '32000',\n",
    "                                          header='True',\n",
    "                                          over_write='True')\n",
    "sf.run_str_query(sg_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec9044fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:30:46.001927Z",
     "start_time": "2022-02-02T01:30:40.645368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "copy into MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE\n",
      "from @sdsdestage_test/testing_stage/\n",
      "file_format = (type = csv field_delimiter = ',' encoding = 'UTF-8'   skip_header = 1)\n",
      "pattern = '.*.csv';\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "ERROR:__main__:Error Created Trying to Copy Into sf table (snowflake.connector.errors.ProgrammingError) SQL compilation error:\n",
      "Table 'MACHINELEARNINGOUTPUTS.DEV.SDSDE_DELETE_TEST_TABLE' does not exist\n",
      "[SQL: copy into MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE\n",
      "from @sdsdestage_test/testing_stage/\n",
      "file_format = (type = csv field_delimiter = ',' encoding = 'UTF-8'   skip_header = 1)\n",
      "pattern = '.*.csv';]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n",
      "WARNING:__main__:Most this table needs to be initialized\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "cp_query = copy_into_sf_query_generator(stage_name='sdsdestage_test/',\n",
    "                                        data_lake_path='testing_stage/',\n",
    "                                        table_name='sdsde_DELETE_TEST_TABLE',\n",
    "                                        database=sf.sfDatabase,\n",
    "                                        schema=sf.sfSchema,\n",
    "                                        skip_header='1',\n",
    "                                        field_delimiter=',',\n",
    "                                        encoding='UTF-8',\n",
    "                                        file_type='csv',\n",
    "                                        pattern='.*.csv')\n",
    "try:\n",
    "    sf.run_str_query(cp_query)\n",
    "except Exception as e:\n",
    "    logger.error(f'Error Created Trying to Copy Into sf table {e}')\n",
    "    logger.warning('Most this table needs to be initialized' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51d9a7",
   "metadata": {},
   "source": [
    "In order fix this problem ``create_sf_table_from_df`` is a utility that you can use or you can do this any other way you see fit. There are many ways to go about doing this, but this was a nifty way that the STFP.py does this so using this here seemed right to allow users to have this in the utility belt.\n",
    "\n",
    "1. Here we are going to use sdsde to pull from the stage ``sdsdestage_test``, which is really just inside of azure data lake v2 not to be confused with the old azure blob. Remember that a stage is really just a location in azure ``azure://<your_azure_>.blob.core.windows.net/sdsdetesting/testing_stage/`` = ``sdsdestage_test``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97cd56a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:31:01.279475Z",
     "start_time": "2022-02-02T01:30:46.003565Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:files in blob ['data_0_0_0.csv']\n",
      "INFO:sdsde.azure.filehandling:testing_stage/testing_stage/data_0_0_0.csv to ./data_0_0_0.csv\n",
      "INFO:__main__:Query Size (100, 8)\n",
      "INFO:__main__:\n",
      "        create or replace table MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE (ECID NUMBER, LIKELIHOODTORETURNRATE FLOAT, LIKELIHOODTORETURNLABEL VARCHAR, FISCALYEAR VARCHAR, MODELTYPE VARCHAR, EXPERIMENT VARCHAR, MODELNAME VARCHAR, UPLOADTIME VARCHAR);\n",
      "        \n",
      "WARNING:__main__:Note: Remember this is created data types for sf based on this file if\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Table SDSDE_DELETE_TEST_TABLE successfully created.\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDE_DELETE_TEST_TABLE successfully dropped.\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDESTAGE_TEST successfully dropped.\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/testing_stage/data_0_0_0.csv']\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/testing_stage']\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "\n",
    "file_list = fh.ls_blob(container_name='sdsdetesting',  path='testing_stage/testing_stage', recursive=True)\n",
    "logger.info(f'files in blob {file_list}')\n",
    "blob_puller(files=['testing_stage/testing_stage/' + file_list[0]],\n",
    "            connection_str=fh.connection_string,\n",
    "            container_name='sdsdetesting',\n",
    "            drop_location='.',\n",
    "            overwrite=True)\n",
    "df = pd.read_csv(file_list[0])\n",
    "# df = pd.read_csv(file_list[0], header=None,\n",
    "#             names=sf.run_str_query(\"SELECT * FROM BIDE_EDWDB_ARA_PROD.dbo.FactScan LIMIT 2;\").columns)\n",
    "logger.info(f'Query Size {df.shape}')\n",
    "create_table_sql = create_sf_table_from_df(df=df, \n",
    "                                           table_name_sf='MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE',\n",
    "                                           varchar=False)\n",
    "sf.run_str_query(create_table_sql)\n",
    "sf.run_str_query(cp_query)\n",
    "sf.run_str_query(\"DROP TABLE MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE;\")\n",
    "sf.run_str_query('DROP STAGE sdsdestage_test;')\n",
    "for f in fh.ls_blob(container_name='sdsdetesting', path='testing_stage/', recursive=True):\n",
    "    if f == 'testing_stage':\n",
    "        pass\n",
    "    else:\n",
    "        fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/' + f)\n",
    "fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/')\n",
    "os.unlink(file_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a373e0",
   "metadata": {},
   "source": [
    "## Parquet  Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d68a6",
   "metadata": {},
   "source": [
    "### Stage Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2f5228d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T01:32:12.082279Z",
     "start_time": "2022-02-02T01:31:45.088386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Datalake Stage path that copy into will use testing_stage/\n",
      "INFO:__main__:stage_query: \n",
      " create or replace stage sdsdestage_test\n",
      "url='azure://vaildtscadls.blob.core.windows.net/sdsdetesting/testing_stage/'\n",
      "credentials=(azure_sas_token='**MASKED**')\n",
      "encryption=(type= 'NONE')\n",
      "file_format = (type = parquet        )\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Stage area SDSDESTAGE_TEST successfully created.\n",
      "INFO:__main__:\n",
      "COPY INTO @sdsdestage_test/testing_stage/\n",
      "FROM  (SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100)\n",
      "\n",
      "max_file_size = 32000\n",
      "overwrite = True\n",
      "header = True;\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:__main__:files in blob ['data_0_0_0.snappy.parquet']\n",
      "INFO:sdsde.azure.filehandling:testing_stage/testing_stage/data_0_0_0.snappy.parquet to ./data_0_0_0.snappy.parquet\n",
      "INFO:__main__:Query Size (100, 8)\n",
      "INFO:__main__:\n",
      "        create or replace table MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE (ECID VARCHAR, LIKELIHOODTORETURNRATE VARCHAR, LIKELIHOODTORETURNLABEL VARCHAR, FISCALYEAR VARCHAR, MODELTYPE VARCHAR, EXPERIMENT VARCHAR, MODELNAME VARCHAR, UPLOADTIME VARCHAR);\n",
      "        \n",
      "WARNING:__main__:Note: Remember this is created data types for sf based on this file if\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Table SDSDE_DELETE_TEST_TABLE successfully created.\n",
      "INFO:__main__:\n",
      "copy into MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE\n",
      "FROM ( SELECT $1:\"ECID\", $1:\"LIKELIHOODTORETURNRATE\", $1:\"LIKELIHOODTORETURNLABEL\", $1:\"FISCALYEAR\", $1:\"MODELTYPE\", $1:\"EXPERIMENT\", $1:\"MODELNAME\", $1:\"UPLOADTIME\"\n",
      "from @sdsdestage_test/testing_stage/ )\n",
      "file_format = (type = parquet      )\n",
      "pattern = '.*.parquet';\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDE_DELETE_TEST_TABLE successfully dropped.\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDESTAGE_TEST successfully dropped.\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/testing_stage/data_0_0_0.snappy.parquet']\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['testing_stage/testing_stage']\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "stage_query = make_data_lake_stage(stage_name='sdsdestage_test',\n",
    "                                   account=os.environ['azure_account'],\n",
    "                                   container='sdsdetesting',\n",
    "                                   data_lake_path='testing_stage/',\n",
    "                                   sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                   file_type='parquet'\n",
    "                                  )\n",
    "\n",
    "sf.run_str_query(stage_query)\n",
    "\n",
    "sg_query = copy_into_adls_query_generator(stage_name='sdsdestage_test',\n",
    "                                          azure_sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                          sf_query=r'SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100',\n",
    "                                          data_lake_path='testing_stage/',\n",
    "                                          max_file_size = '32000',\n",
    "                                          header='True',\n",
    "                                          over_write='True')\n",
    "\n",
    "sf.run_str_query(sg_query)\n",
    "\n",
    "from sdsde.wrapper.azurewrapper import blob_puller\n",
    "from sdsde.azure.filehandling import FileHandling\n",
    "\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "file_list = fh.ls_blob(container_name='sdsdetesting',  path='testing_stage/testing_stage', recursive=True)\n",
    "logger.info(f'files in blob {file_list}')\n",
    "blob_puller(files=['testing_stage/testing_stage/' + file_list[0]],\n",
    "            connection_str=fh.connection_string,\n",
    "            container_name='sdsdetesting',\n",
    "            drop_location='.',\n",
    "            overwrite=True)\n",
    "df = pd.read_parquet(file_list[0])\n",
    "\n",
    "logger.info(f'Query Size {df.shape}')\n",
    "\n",
    "create_table_sql = create_sf_table_from_df(df=df, \n",
    "                                           table_name_sf='MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE',\n",
    "                                           varchar=False)\n",
    "sf.run_str_query(create_table_sql)\n",
    "\n",
    "cp_query = parquet_copy_into_sf_query_generator(data_types=dict(df.dtypes),\n",
    "                                                stage_name='sdsdestage_test/',\n",
    "                                                data_lake_path='testing_stage/',\n",
    "                                                table_name='sdsde_DELETE_TEST_TABLE',\n",
    "                                                database=sf.sfDatabase,\n",
    "                                                schema=sf.sfSchema,\n",
    "                                                file_type='parquet',\n",
    "                                                pattern='.*.parquet',\n",
    "                                                infer_dtypes=True)\n",
    "sf.run_str_query(cp_query)\n",
    "sf.run_str_query(\"DROP TABLE MACHINELEARNINGOUTPUTS.DEV.sdsde_DELETE_TEST_TABLE;\")\n",
    "sf.run_str_query('DROP STAGE sdsdestage_test;')\n",
    "for f in fh.ls_blob(container_name='sdsdetesting', path='testing_stage/', recursive=True):\n",
    "    if f == 'testing_stage':\n",
    "        pass\n",
    "    else:\n",
    "        fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/' + f)\n",
    "fh.rm_files(container_name='sdsdetesting', delete_path='testing_stage/')\n",
    "os.unlink(file_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962662f",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "914059ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:36.797316Z",
     "start_time": "2022-02-04T00:18:35.952167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f2578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
