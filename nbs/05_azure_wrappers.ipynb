{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:23.496904Z",
     "start_time": "2022-02-01T23:46:23.495044Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp wrapper.azurewrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:23.664459Z",
     "start_time": "2022-02-01T23:46:23.662293Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:23.839400Z",
     "start_time": "2022-02-01T23:46:23.836496Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from sdsde.azure.filehandling import FileHandling\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blob_pusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:24.751601Z",
     "start_time": "2022-02-01T23:46:24.747113Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def blob_pusher(container_name: str,\n",
    "                connection_str: str,\n",
    "                file_path: list = None,\n",
    "                blob_dest: list = None,\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    function that will push file(s) to azure blob\n",
    "\n",
    "    Args:\n",
    "    * container_name (str): container name\n",
    "    * connection_str (str): connection str\n",
    "    * file_path (list, optional): file location of file(s). Defaults to None.\n",
    "    * blob_dest (list, optional): where to drop in azure blob. Defaults to container_name.\n",
    "\n",
    "    Returns:\n",
    "        str: file_path\n",
    "    \"\"\"\n",
    "    fh = FileHandling(connection_str)\n",
    "    blob_dest = [container_name] if blob_dest is None else blob_dest\n",
    "    if len(blob_dest) != len(file_path):\n",
    "        for f in file_path:\n",
    "            fh.upload(container_name=container_name,\n",
    "                      file_path=f,\n",
    "                      dest=os.path.join(blob_dest[0], f.split('/')[-1]),\n",
    "                      **kwargs)\n",
    "    else:\n",
    "        for f, p in zip(file_path, blob_dest):\n",
    "            fh.upload(container_name=container_name,\n",
    "                      file_path=f,\n",
    "                      dest=str(os.path.join(p, f.split('/')[-1])),\n",
    "                      **kwargs)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use case for this function is to be able to push multi files to different locations in blob. An example would be the data prep needs to one location and the data set should go to another a good example of this is inside of the ``ML_Reservation`` repo that uses this function in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:25.610583Z",
     "start_time": "2022-02-01T23:46:25.599112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"blob_pusher\" class=\"doc_header\"><code>blob_pusher</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>blob_pusher</code>(**`container_name`**:`str`, **`connection_str`**:`str`, **`file_path`**:`list`=*`None`*, **`blob_dest`**:`list`=*`None`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "function that will push file(s) to azure blob\n",
       "\n",
       "Args:\n",
       "* container_name (str): container name\n",
       "* connection_str (str): connection str\n",
       "* file_path (list, optional): file location of file(s). Defaults to None.\n",
       "* blob_dest (list, optional): where to drop in azure blob. Defaults to container_name.\n",
       "\n",
       "Returns:\n",
       "    str: file_path"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(blob_pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:32.606337Z",
     "start_time": "2022-02-01T23:46:31.717322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.azure.filehandling:sdsdetesting is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading testing/test_df.csv, to to Azure Storage snowflake_load_test/test_df.csv\n",
      "ERROR:sdsde.azure.filehandling:\"Error Message: BlobAlreadyExists\"\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n"
     ]
    }
   ],
   "source": [
    "data_loaders = ['testing/test_df.csv']\n",
    "container_name = 'sdsdetesting'\n",
    "blob_pusher(file_path=data_loaders,\n",
    "            container_name=container_name,\n",
    "            blob_dest=['snowflake_load_test'],\n",
    "            connection_str=os.environ['connection_str'])\n",
    "from sdsde.azure.filehandling import *\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "assert fh.ls_blob(container_name=container_name, path='snowflake_load_test') == ['test_df.csv'],' File should have made it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: urllib3 warning is a known bug from the community and it's has to to the the HTTP response the juice isn't worth the squeeze if it bothers people sdsde can suppress the warnings from urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blob_puller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:38.920716Z",
     "start_time": "2022-02-01T23:46:38.916033Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def blob_puller(files: list,\n",
    "                connection_str: str,\n",
    "                container_name: str,\n",
    "                drop_location: str = '.',\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    Can pull a list or one file from azure\n",
    "\n",
    "    Args:\n",
    "    * files (list): list of files or a file wrapped in []\n",
    "    * connection_str (str): connection string to azure blob storage\n",
    "    * container_name (str): container name\n",
    "    * drop_location (str, optional): where to drop file(s) locally. Defaults to ''.\n",
    "    \"\"\"\n",
    "    fh = FileHandling(connection_str)\n",
    "    drop_location = drop_location if drop_location.endswith('/') else drop_location + '/'\n",
    "    for f in files:\n",
    "        fh.download_file(container_name=container_name,\n",
    "                         file=f,\n",
    "                         file_path=drop_location,\n",
    "                         **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use case of this function is to be able to pull multi files down at a time that might be needs for say a train, valid, test set that is sitting in the same container and there is a desire to have them all be pulled down to a computer.\n",
    "\n",
    "> TODO: Add container locations as well as adding different drop locations similar to ``blob_pusher``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:40.081376Z",
     "start_time": "2022-02-01T23:46:40.069337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"blob_puller\" class=\"doc_header\"><code>blob_puller</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>blob_puller</code>(**`files`**:`list`, **`connection_str`**:`str`, **`container_name`**:`str`, **`drop_location`**:`str`=*`'.'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Can pull a list or one file from azure\n",
       "\n",
       "Args:\n",
       "* files (list): list of files or a file wrapped in []\n",
       "* connection_str (str): connection string to azure blob storage\n",
       "* container_name (str): container name\n",
       "* drop_location (str, optional): where to drop file(s) locally. Defaults to ''."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(blob_puller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:41.136658Z",
     "start_time": "2022-02-01T23:46:40.676293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.azure.filehandling:snowflake_load_test/test_df.csv to ./testing/test_df.csv\n",
      "INFO:sdsde.azure.filehandling:files to be removed ['snowflake_load_test/test_df.csv']\n"
     ]
    }
   ],
   "source": [
    "container_name = 'sdsdetesting'\n",
    "blob_puller(files=['snowflake_load_test/test_df.csv'],\n",
    "            connection_str=os.environ['connection_str'],\n",
    "            container_name=container_name,\n",
    "            drop_location='./testing',\n",
    "            overwrite=True)\n",
    "assert os.path.exists(data_loaders[0]) == True, 'above function did not run'\n",
    "fh.rm_files(container_name=container_name, delete_path='snowflake_load_test/', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unlink_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use case is used for cleaning your files up after a function is ran **s/o Clay Elmore** for making this something we do as a sdsde team and now has a supported function for this exact use case. This is used everywhere is the repos that Jeremy and Caly develop and by the time you read this I am sure the rest of the team will be doing this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:43.059701Z",
     "start_time": "2022-02-01T23:46:43.055261Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def unlink_files(files: list, file_path: str = './'):\n",
    "    \"\"\"\n",
    "    File Clean Up After Model Prediction\n",
    "\n",
    "    Args:\n",
    "    * files (list): file(s) name(s)\n",
    "    * file_path (str, optional): file(s) path(s). Defaults to './'.\n",
    "    \"\"\"\n",
    "    file_list = files\n",
    "    for x in file_list:\n",
    "        os.unlink(os.path.join(file_path, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:44.168043Z",
     "start_time": "2022-02-01T23:46:44.154150Z"
    }
   },
   "outputs": [],
   "source": [
    "dict1 = [{'ecid': 150, 'home': 'CA', 'avg_visits': 0.20, 'LTR': 6},\n",
    "         {'ecid': 151, 'home': 'LA', 'avg_visits': 10, 'LTR': 2},\n",
    "         {'ecid': 160, 'home': 'CO', 'avg_visits': 0.56, 'LTR': 4},\n",
    "         {'ecid': 100, 'home': 'LA', 'avg_visits': 2.0, 'LTR': 3}]\n",
    "df = pd.DataFrame(dict1)\n",
    "df.to_csv('df_file.csv')\n",
    "numpy_save = np.arange(10)\n",
    "np.save('np_file.npy', numpy_save)\n",
    "df_names = ['df_file.csv']\n",
    "np_names = ['np_file.npy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:44.675749Z",
     "start_time": "2022-02-01T23:46:44.670425Z"
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(df_names[0]) == True, 'above function did not run'\n",
    "assert os.path.exists(np_names[0]) == True, 'above function did not run'\n",
    "unlink_files(df_names + np_names)\n",
    "assert os.path.exists(df_names[0]) == False, 'file name change?'\n",
    "assert os.path.exists(np_names[0]) == False, 'file name change?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_and_push_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A use case for this is when you have a lot of in memory python object for example ``np.arrays`` ``pd.DataFrame`` or ``dict`` and you want to push these to a certain location in an azure blob container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:45.506129Z",
     "start_time": "2022-02-01T23:46:45.501539Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def save_and_push_data(container_name: str,\n",
    "                       connection_str: str,\n",
    "                       df_names: list = None,\n",
    "                       dfs: list = None,\n",
    "                       np_names: list = None,\n",
    "                       nps: list = None,\n",
    "                       blob_dest: str = None,\n",
    "                       parquet: bool = True,\n",
    "                       **kwargs):\n",
    "    \"\"\"\n",
    "    Takes panda dataframes and wirtes them to parquet files\n",
    "    Takes numpy arrays, list, and dictionaries and writes them out\n",
    "    as numpy files.\n",
    "\n",
    "    Note: to get the dictionary out upon load you need to add a .item()\n",
    "    this will return the dict as a not np array.\n",
    "\n",
    "    Args:\n",
    "    * container_name (str): location in blob storage\n",
    "    * connection_str (str): connect_str for azure\n",
    "    * df_names (list, optional): list of names for the files. Defaults to [].\n",
    "    * dfs (list, optional): list of panda dataframes. Defaults to [].\n",
    "    * np_names (list, optional): list of names for the files. Defaults to [].\n",
    "    * nps (list, optional): list of numpy arrays to write out. Defaults to [].\n",
    "    * blob_folder (str, optional): folder you would like. Defaults to None.\n",
    "    * parquet (bool): true means save df as parquet files. Defaults to True.\n",
    "    \"\"\"\n",
    "#     Once again Snowflake Parquet upload isn't easy will figure out\n",
    "    if parquet is True:\n",
    "        _ = [d.to_parquet(f\"{n}\") for d, n in zip(dfs, df_names)]\n",
    "    else:\n",
    "        _ = [d.to_csv(f\"{n}\") for d, n in zip(dfs, df_names)]\n",
    "    _ = [np.save(f'{n}', d) for d, n in zip(nps, np_names)]\n",
    "    files_list = np.concatenate([df_names, np_names]).tolist()\n",
    "    _ = blob_pusher(container_name=container_name,\n",
    "                    connection_str=connection_str,\n",
    "                    file_path=files_list,\n",
    "                    blob_dest=blob_dest,\n",
    "                    **kwargs)\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:47.631901Z",
     "start_time": "2022-02-01T23:46:46.992123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.azure.filehandling:sdsdetesting is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading df_file.csv, to to Azure Storage save_and_push/df_file.csv\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n",
      "INFO:sdsde.azure.filehandling:sdsdetesting is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading np_file.npy, to to Azure Storage save_and_push/np_file.npy\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n"
     ]
    }
   ],
   "source": [
    "dict1 = [{'ecid': 150, 'home': 'CA', 'avg_visits': 0.20, 'LTR': 6},\n",
    "         {'ecid': 151, 'home': 'LA', 'avg_visits': 10, 'LTR': 2},\n",
    "         {'ecid': 160, 'home': 'CO', 'avg_visits': 0.56, 'LTR': 4},\n",
    "         {'ecid': 100, 'home': 'LA', 'avg_visits': 2.0, 'LTR': 3}]\n",
    "df = pd.DataFrame(dict1)\n",
    "df.to_csv('df_file.csv')\n",
    "numpy_save = np.arange(10)\n",
    "np.save('np_file.npy', numpy_save)\n",
    "df_names = ['df_file.csv']\n",
    "np_names = ['np_file.npy']\n",
    "\n",
    "save_and_push_data(container_name=container_name,\n",
    "                   df_names=df_names,\n",
    "                   dfs=[df],\n",
    "                   np_names=np_names,\n",
    "                   nps=[numpy_save],\n",
    "                   blob_dest=['save_and_push'],\n",
    "                   connection_str=os.environ['connection_str'],\n",
    "                   parquet=False)\n",
    "\n",
    "assert fh.ls_blob(container_name=container_name, path='save_and_push') == ['df_file.csv', 'np_file.npy'], 'files sent'\n",
    "unlink_files(df_names + np_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:46:51.630762Z",
     "start_time": "2022-02-01T23:46:50.849963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted 08_yaml_ingestion_binary_classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
