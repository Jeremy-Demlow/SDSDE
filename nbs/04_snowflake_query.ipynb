{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:41.285599Z",
     "start_time": "2022-02-04T00:16:41.280860Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp snowflake.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:42.163255Z",
     "start_time": "2022-02-04T00:16:41.521925Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``SnowflakeConnect:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:43.107324Z",
     "start_time": "2022-02-04T00:16:42.239974Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# from sdsde.imports import *\n",
    "from functools import partial\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.meta import delegates\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake import connector\n",
    "from snowflake.connector.pandas_tools import write_pandas, pd_writer\n",
    "\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:43.271959Z",
     "start_time": "2022-02-04T00:16:43.214823Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class SnowflakeConnect:\n",
    "    \"\"\"\n",
    "    Class that holds basic snowflake functionality including testing connection\n",
    "    and running queries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sfAccount: str, sfUser: str, sfPswd: str,\n",
    "                 sfWarehouse: str, sfDatabase: str, sfSchema: str,\n",
    "                 sfRole: str, delimiter: str = ';', logger=None\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Creates a connection to snowflake account for a simple class\n",
    "        for a python users to take advantage of the computation power of\n",
    "        snowflake. This class is used in all snowflake .py files through\n",
    "        super().\n",
    "\n",
    "        Args:\n",
    "        * sfAccount (str): snowflake credential passed as string. Defaults to None.\n",
    "        * sfUser (str): snowflake credential passed as string. Defaults to None.\n",
    "        * sfPswd (str): snowflake credential passed as string. Defaults to None.\n",
    "        * sfWarehouse (str): snowflake credential passed as string. Defaults to None.\n",
    "        * sfDatabase (str): snowflake credential passed as string. Defaults to None.\n",
    "        * sfSchema (str): snowflake credential passed as string. Defaults to None.\n",
    "        * sfRole (str): snowflake credential passed as string. Defaults to None.\n",
    "        * delimiter (str, optional): parse delimiter for the query files. Defaults to ';'.\n",
    "        * logger (logger, optional): pass custom logger as many libraries are set to Warning. Defaults to None.\n",
    "        \"\"\"\n",
    "        store_attr()\n",
    "        self._logger = logger if logger is not None else logging.getLogger(__name__)\n",
    "        self.delimiter = delimiter\n",
    "        try:\n",
    "            self.test_connection()\n",
    "        except Exception as e:\n",
    "            self._logger.error(f'{e}')\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_sql_file(path: str, delimiter: str):\n",
    "        \"\"\"\n",
    "        Helper function to parse string on default delimiter ;\n",
    "        Args:\n",
    "        * path (str): path to file that will be parsed\n",
    "        * delimiter (str): string type to parse end of sql command\n",
    "\n",
    "        Returns:\n",
    "        * str: sting of the command\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        single_command = ''\n",
    "        sql_commands = []\n",
    "\n",
    "        for i in lines:\n",
    "            single_command = single_command + i\n",
    "            if f'{delimiter}' in i:\n",
    "                sql_commands.append(single_command)\n",
    "                single_command = ''\n",
    "\n",
    "        return sql_commands\n",
    "\n",
    "    @staticmethod\n",
    "    def single_cmd(sf_connection, sql_command):\n",
    "        \"\"\"\n",
    "        utility to ``execute_concurrent_query`` that handles\n",
    "        the request to snowflake via the class itself\n",
    "\n",
    "        Args:\n",
    "        * sf_connection (SnowFlake Engine): Calls internal class function\n",
    "        * sql_command (str): query to snowflake\n",
    "\n",
    "        Returns:\n",
    "        * Depends on response: pd.DataFrame or a String\n",
    "        \"\"\"\n",
    "        logging.info('new thread started')\n",
    "        response = sf_connection.run_str_query(sql_command)\n",
    "        logging.info(f\"Finishing {sql_command}\")\n",
    "        return response\n",
    "\n",
    "    def create_sf_engine(self, role=None, schema=None, password=None,\n",
    "                         user=None, warehouse=None, database=None):\n",
    "        \"Moving Engine from init to a function to see if this will help\"\n",
    "        engine = create_engine(URL(user=user if user else self.sfUser,\n",
    "                                   password=password if password else self.sfPswd,\n",
    "                                   account=self.sfAccount,\n",
    "                                   warehouse=warehouse if warehouse else self.sfWarehouse,\n",
    "                                   database=database if database else self.sfDatabase,\n",
    "                                   schema=schema if schema else self.sfSchema,\n",
    "                                   role=role if role else self.sfRole))\n",
    "        self._logger.info(\"sqlalchemy snowflake engine created\")\n",
    "        return engine\n",
    "\n",
    "    @delegates(create_sf_engine)\n",
    "    def test_connection(self, **kwargs):\n",
    "        \"Tests a connection for snowflake from instantiated object\"\n",
    "        engine = partial(self.create_sf_engine,\n",
    "                         role=kwargs.get('role') if kwargs.get('role') else self.sfRole,\n",
    "                         schema=kwargs.get('schema') if kwargs.get('schema') else self.sfSchema,\n",
    "                         password=kwargs.get('password') if kwargs.get('password') else self.sfPswd,\n",
    "                         user=kwargs.get('user') if kwargs.get('user') else self.sfUser,\n",
    "                         warehouse=kwargs.get('warehouse') if kwargs.get('warehouse') else self.sfWarehouse,\n",
    "                         database=kwargs.get('database') if kwargs.get('database') else self.sfDatabase\n",
    "                         )()\n",
    "        with engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    self._logger.info(\"connection to snowflake successful\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "            finally:\n",
    "                engine.dispose()\n",
    "                connection.close()\n",
    "        pass\n",
    "\n",
    "    @delegates(create_sf_engine)\n",
    "    def run_str_query(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        This function will connect to snowflake and run a query that is passed\n",
    "        in as a string.\n",
    "        Args:\n",
    "        * query (str): SQL Query String\n",
    "        Returns:\n",
    "        * pd.DataFrame/ None: dependent on the query string\n",
    "        \"\"\"\n",
    "        self._logger.info('testing connection')\n",
    "        engine = partial(self.create_sf_engine,\n",
    "                         role=kwargs.get('role') if kwargs.get('role') else self.sfRole,\n",
    "                         schema=kwargs.get('schema') if kwargs.get('schema') else self.sfSchema,\n",
    "                         password=kwargs.get('password') if kwargs.get('password') else self.sfPswd,\n",
    "                         user=kwargs.get('user') if kwargs.get('user') else self.sfUser,\n",
    "                         warehouse=kwargs.get('warehouse') if kwargs.get('warehouse') else self.sfWarehouse,\n",
    "                         database=kwargs.get('database') if kwargs.get('database') else self.sfDatabase\n",
    "                         )()\n",
    "        with engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    self._logger.info('executing query')\n",
    "                    query = query if str(query).endswith(';') else query + ';'\n",
    "                    df = pd.read_sql_query(query, engine)\n",
    "                    self._logger.info(\"data loaded from snowflake\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "            finally:\n",
    "                engine.dispose()\n",
    "                connection.close()\n",
    "                self._logger.info(\"connection to snowflake has been turned off\")\n",
    "        if df.columns[0] == 'status':\n",
    "            self._logger.info(df.status.values[0])\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "    @delegates(create_sf_engine)\n",
    "    def execute_file(self, query_path: str, sql: str = None, **kwargs):\n",
    "        \"\"\"\n",
    "        will run sql file or txt file that is seperated by self.delimenter\n",
    "        with the option to add a custom sql command if you run an sql command\n",
    "        then the sql command will be the returned response\n",
    "\n",
    "        Args:\n",
    "        * query_path (str): location to sql/txt file to execute. Defaults to None.\n",
    "        * sql (str, optional): Optional Command to add on top of the file being executed. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        * pd.DataFrame: returns query results\n",
    "        \"\"\"\n",
    "\n",
    "        self._logger.info(\"query being parsed\")\n",
    "        engine = partial(self.create_sf_engine,\n",
    "                         role=kwargs.get('role') if kwargs.get('role') else self.sfRole,\n",
    "                         schema=kwargs.get('schema') if kwargs.get('schema') else self.sfSchema,\n",
    "                         password=kwargs.get('password') if kwargs.get('password') else self.sfPswd,\n",
    "                         user=kwargs.get('user') if kwargs.get('user') else self.sfUser,\n",
    "                         warehouse=kwargs.get('warehouse') if kwargs.get('warehouse') else self.sfWarehouse,\n",
    "                         database=kwargs.get('database') if kwargs.get('database') else self.sfDatabase\n",
    "                         )()\n",
    "        sql_commands = self.parse_sql_file(query_path, self.delimiter)\n",
    "        commands = len(sql_commands)\n",
    "        i = 0\n",
    "        with engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    for z in sql_commands:\n",
    "                        if i < commands:\n",
    "                            i += 1\n",
    "                            df = pd.read_sql_query(z, engine)\n",
    "                    self._logger.info(\"sql or txt file excecuted\")\n",
    "                    if sql:\n",
    "                        self._logger.info(f\"Last query response {df}\")\n",
    "                        df = pd.read_sql_query(sql, engine)\n",
    "                        self._logger.info(\"data is loaded from snowflake\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "                    df = None\n",
    "                return df\n",
    "\n",
    "            finally:\n",
    "                engine.dispose()\n",
    "                connection.close()\n",
    "                self._logger.info(\"connection to snowflake has been turned off\")\n",
    "\n",
    "    @delegates(create_sf_engine)\n",
    "    def execute_large_query_dump_to_file(self, query_path: str, file_name: str,\n",
    "                                         chunk_size: int = 150000, **kwargs):\n",
    "        \"\"\"\n",
    "        will run sql file or txt file that is seperated by self.delimenter\n",
    "        with the option to add a custom sql command if you run an sql command\n",
    "        then the sql command will be the returned response\n",
    "\n",
    "        Args:\n",
    "        * query_path (str): location to sql/txt file to execute. Defaults to None.\n",
    "        * sql (str, optional): Optional Command to add on top of the file being executed. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        * pd.DataFrame: returns query results\n",
    "        \"\"\"\n",
    "\n",
    "        self._logger.info(\"query being parsed\")\n",
    "        engine = partial(self.create_sf_engine,\n",
    "                         role=kwargs.get('role') if kwargs.get('role') else self.sfRole,\n",
    "                         schema=kwargs.get('schema') if kwargs.get('schema') else self.sfSchema,\n",
    "                         password=kwargs.get('password') if kwargs.get('password') else self.sfPswd,\n",
    "                         user=kwargs.get('user') if kwargs.get('user') else self.sfUser,\n",
    "                         warehouse=kwargs.get('warehouse') if kwargs.get('warehouse') else self.sfWarehouse,\n",
    "                         database=kwargs.get('database') if kwargs.get('database') else self.sfDatabase\n",
    "                         )()\n",
    "        sql_commands = self.parse_sql_file(query_path, self.delimiter)\n",
    "        commands = len(sql_commands)\n",
    "        i = 0\n",
    "        with engine.connect() as connection:\n",
    "            try:\n",
    "                if connection:\n",
    "                    for z in sql_commands:\n",
    "                        if i < commands:\n",
    "                            i += 1\n",
    "                        chunk_sizer = 0\n",
    "                        for chunk_df in pd.read_sql_query(z, engine, chunksize=chunk_size):\n",
    "                            if not os.path.exists(file_name):\n",
    "                                # TODO: Add Parquet Logic here when needed pandas doesn't support\n",
    "                                # Pyarrow does sort of and parquet files aren't meant for appending\n",
    "                                self._logger.info(f'{file_name} being created')\n",
    "                                chunk_df.to_csv(file_name, index=False)\n",
    "                            else:\n",
    "                                self._logger.info(f'{file_name} appending to')\n",
    "                                chunk_df.to_csv(file_name, header=False, mode='a', index=False)\n",
    "                            chunk_sizer += chunk_size\n",
    "                            self._logger.info(f\"streamed data total {chunk_sizer}\")\n",
    "                    self._logger.info(f\"sql or txt file excecuted and pushed data to {file_name}\")\n",
    "                else:\n",
    "                    self._logger.info(\"connection to snowflake failed\")\n",
    "\n",
    "            finally:\n",
    "                engine.dispose()\n",
    "                connection.close()\n",
    "                self._logger.info(\"connection to snowflake has been turned off\")\n",
    "\n",
    "    def infer_to_snowflake(self,\n",
    "                           df: pd.DataFrame,\n",
    "                           table_name: str,\n",
    "                           if_exists: str = 'append',\n",
    "                           dtype: dict = None,\n",
    "                           chunk_size: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        * df (pd.DataFrame): The dataframe that is going to snowflake\n",
    "        * table_name (str): Table name that will be created in snowflake\n",
    "        * if_exists (str, optional): How to behave if the table already exists.\n",
    "                                    * fail: Raise a ValueError.\n",
    "                                    * replace: Drop the table before inserting new values.\n",
    "                                    * append: Insert new values to the existing table.\n",
    "                                    Defaults to 'fail'.\n",
    "        * dtype (dict, optional): Specifying the datatype for columns. If a dictionary is used, the\n",
    "                                keys should be the column names and the values should be the\n",
    "                                SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
    "                                scalar is provided, it will be applied to all columns.\n",
    "                                Defaults to None.\n",
    "        * chunk_size (int, optional): Specify the number of rows in each batch to be written\n",
    "                                    at a time. By default, all rows will be written at once.\n",
    "                                    Defaults to None.\n",
    "        \"\"\"\n",
    "        df.columns = [x.upper() for x in df.columns]\n",
    "        self._logger.info(f'Begining upload to {table_name}')\n",
    "        df.to_sql(table_name.upper(), self.create_sf_engine().connect(), index=False, chunksize=chunk_size,\n",
    "                  method=pd_writer, if_exists=if_exists)\n",
    "        self._logger.info(f'Dataframe uploaded to {table_name}')\n",
    "\n",
    "    @delegates(infer_to_snowflake)\n",
    "    def large_table_infer_sf(self, df: pd.DataFrame, table_name: str, split: int, **kwargs):\n",
    "        \"\"\"\n",
    "        This take infer_to_snowflake and give the ability to split the loads to snowflake\n",
    "        this isn't tested for efficiency, but might be useful when not wanting to wait\n",
    "        for a 100M+ memory object to make it to snowflake. A user can split this into\n",
    "        as many slices as they choose and it will fail if there is something wrong with\n",
    "        the upload.\n",
    "\n",
    "        Args:\n",
    "        * df (pd.DataFrame): in memory pandas dataframe\n",
    "        * table_name (str): table name for table being created\n",
    "        * split (int): how many splits to split the data frame up in\n",
    "        \"\"\"\n",
    "        # Don't know a good cut off point 3.6GB takes ~14 mins at split 7\n",
    "        if kwargs.get('if_exists') != 'append' and kwargs.get('if_exists') is not None:\n",
    "            self._logger.warning('only append is supported with large dataframes')\n",
    "            self._logger.info('if user is trying to replace or fail use infer_to_snowflake')\n",
    "            return\n",
    "        if df.shape[0]/split < 50000:\n",
    "            self._logger.warning('best to use infer_to_snowflake when splits are less than 50k')\n",
    "        if kwargs.get('if_exists') is None:\n",
    "            self._logger.info(f'if_exisits defaults to append to {table_name}')\n",
    "        if_exists = 'append'\n",
    "        for chunk in np.array_split(df, split):\n",
    "            gc.collect()\n",
    "            self.infer_to_snowflake(chunk,\n",
    "                                    table_name=table_name.upper(),\n",
    "                                    if_exists=if_exists)\n",
    "\n",
    "    @delegates(infer_to_snowflake)\n",
    "    def chunk_csv_infer_sf(self, file_path: str, table_name: str, chunk_size: int, **kwargs):\n",
    "        \"\"\"\n",
    "        Again takes from infer_to_snowflake, but give the user the ability to chunk\n",
    "        out a file from local machine. Currently not supporting parquet files, but\n",
    "        it also isn't tested and might be a simple switch to allow parquet reads.\n",
    "\n",
    "        Args:\n",
    "        * file_path (str): location of file to upload to snowflake\n",
    "        * table_name (str): table name for table being created\n",
    "        * chunk_size (int): number of rows to read from file at a time\n",
    "        \"\"\"\n",
    "        if kwargs.get('if_exists') != 'append' and kwargs.get('if_exists') is not None:\n",
    "            self._logger.warning('only append is supported with large dataframes')\n",
    "            self._logger.info('if user is trying to replace or fail use infer_to_snowflake')\n",
    "            return\n",
    "        if kwargs.get('if_exists') is None:\n",
    "            self._logger.info(f'if_exisits defaults to append to {table_name}')\n",
    "        if_exists = 'append'\n",
    "        for chunk in pd.read_csv(f'{file_path}', chunksize=chunk_size):\n",
    "            self.infer_to_snowflake(chunk,\n",
    "                                    table_name=table_name.upper(),\n",
    "                                    if_exists=if_exists)\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "    def pandas_to_snowflake(self,\n",
    "                            df: 'pd.DataFrame',\n",
    "                            table_name: str,\n",
    "                            table_check: str,\n",
    "                            chunk_size: int = None,\n",
    "                            create_table: bool = False,\n",
    "                            create_statement: str = None,\n",
    "                            parallel: int = 4):\n",
    "        \"\"\"\n",
    "        Function that expect a table has been created as the infer function mis\n",
    "        allocates the size of columns and isn't best practice not chooses the dytpes\n",
    "        of a column, but is much easier to call upon ``infer_to_snowflake`` recommended\n",
    "        when in development.\n",
    "        Args:\n",
    "        * df (pd.DataFrame): Dataframe that is going to be sent to snowflake\n",
    "        * table_name (str): Table name being created inside of snowflake\n",
    "        * chunk_size (int, optional): Number of elements to be inserted once, if not provided all elements will be dumped once.\n",
    "                                        Defaults to None.\n",
    "        * create_table (bool, optional): If true will create a table with create_statement. Defaults to False.\n",
    "        * create_statement (str, optional): Give create table statement and the table will\n",
    "                                            be created from this string. Defaults to None.\n",
    "        * parallel (int, optional): Number of threads to be used when uploading chunks. Defaults to None.\n",
    "        \"\"\"\n",
    "        _connection = connector.connect(user=self.sfUser,\n",
    "                                        password=self.sfPswd,\n",
    "                                        account=self.sfAccount,\n",
    "                                        warehouse=self.sfWarehouse,\n",
    "                                        database=self.sfDatabase,\n",
    "                                        schema=self.sfSchema)\n",
    "        if create_table:\n",
    "            self._logger.info('creating table')\n",
    "            self.run_str_query(create_statement)\n",
    "        insert_areas = ['INSERT_TABLE_NAME_HERE', 'INSERET_DATABASE_HERE', 'INSERT_SCHEMA_HERE']\n",
    "        inserts = [table_name, self.sfDatabase, self.sfSchema]\n",
    "        for k, v in zip(insert_areas, inserts):\n",
    "            table_check = table_check.replace(k, v)\n",
    "        check = self.run_str_query(table_check)\n",
    "        if check.shape[0] == 0:\n",
    "            self._logger.error('table not created')\n",
    "            raise\n",
    "        try:\n",
    "            df.columns = [x.upper() for x in df.columns]\n",
    "            response, nchunks, nrows, _ = write_pandas(conn=_connection, df=df, table_name=table_name.upper(),\n",
    "                                                       chunk_size=chunk_size, parallel=parallel)\n",
    "            self._logger.info(f'upload was a success: {response} & the number of rows loaded {nrows}')\n",
    "            if nrows != df.shape[1]:\n",
    "                self._logger.warning(f'Dataframe failed to load {nrows - df.shape[1]}')\n",
    "        except Exception as e:\n",
    "            self._logger.error(f'{e}')\n",
    "            raise\n",
    "        self._logger.info(f'dataframe uploaded to {self.sfDatabase}.{table_name}')\n",
    "\n",
    "    def execute_concurrent_query(self, query_path: str) -> list:\n",
    "        \"\"\"\n",
    "        Give a user the ability to call multiple sql commands to\n",
    "        snowflake on a concurrent method giving each command a thread\n",
    "        to communicate to snowflake to allow snowflake to do more than\n",
    "        one command at a time as the current methods wait until each query\n",
    "        is completed.\n",
    "\n",
    "        Args:\n",
    "        * query_path (str): file path\n",
    "\n",
    "        Returns:\n",
    "        * List: Returns Snowflake Response in a list\n",
    "        \"\"\"\n",
    "        sql_commands = self.parse_sql_file(query_path, delimiter=';')\n",
    "        return_df_list = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=len(sql_commands)) as executor:\n",
    "            future_to_url = {executor.submit(self.single_cmd, self, sql_command): sql_command for sql_command in sql_commands}\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                sql_command = future_to_url[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data is None:\n",
    "                        logging.info('snowflake has no response for this action')\n",
    "                    else:\n",
    "                        return_df_list.append(data)\n",
    "                except Exception as exc:\n",
    "                    return_df_list.append(exc)\n",
    "                    logging.info('%r generated an exception: %s' % (sql_command, exc))\n",
    "        return return_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:45.168894Z",
     "start_time": "2022-02-04T00:16:45.155896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SnowflakeConnect\" class=\"doc_header\"><code>class</code> <code>SnowflakeConnect</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SnowflakeConnect</code>(**`sfAccount`**:`str`, **`sfUser`**:`str`, **`sfPswd`**:`str`, **`sfWarehouse`**:`str`, **`sfDatabase`**:`str`, **`sfSchema`**:`str`, **`sfRole`**:`str`, **`delimiter`**:`str`=*`';'`*, **`logger`**=*`None`*)\n",
       "\n",
       "Class that holds basic snowflake functionality including testing connection\n",
       "and running queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:48.336809Z",
     "start_time": "2022-02-04T00:16:46.572343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:connection to snowflake successful\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:connection to snowflake successful\n"
     ]
    }
   ],
   "source": [
    "sf = SnowflakeConnect(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "assert sf.test_connection() == None, 'anything else the connection has failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:51.279148Z",
     "start_time": "2022-02-04T00:16:49.523474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Drop statement executed successfully (SDSDEQUERYTEST already dropped).\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from sdsde.utils.parseyaml import *\n",
    "from sdsde.imports import *\n",
    "sf.run_str_query('DROP TABLE IF EXISTS sdsdequerytest;')\n",
    "__file__ = './'\n",
    "dict1 = [{'ecid': 150, 'home': 'CA', 'avg_visits': 0.20, 'LTR': 6},\n",
    "         {'ecid': 151, 'home': 'LA', 'avg_visits': 10, 'LTR': 2},\n",
    "         {'ecid': 160, 'home': 'CO', 'avg_visits': 0.56, 'LTR': 4},\n",
    "         {'ecid': 100, 'home': 'LA', 'avg_visits': 2.0, 'LTR': 3}]\n",
    "df = pd.DataFrame(dict1)\n",
    "yaml = ParseYaml(os.path.join(os.path.dirname(__file__), 'testing/snowflake.yaml')).get_yaml(['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas_to_snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:51.294017Z",
     "start_time": "2022-02-04T00:16:51.281304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeConnect.pandas_to_snowflake\" class=\"doc_header\"><code>SnowflakeConnect.pandas_to_snowflake</code><a href=\"__main__.py#L346\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeConnect.pandas_to_snowflake</code>(**`df`**:`DataFrame`, **`table_name`**:`str`, **`table_check`**:`str`, **`chunk_size`**:`int`=*`None`*, **`create_table`**:`bool`=*`False`*, **`create_statement`**:`str`=*`None`*, **`parallel`**:`int`=*`4`*)\n",
       "\n",
       "Function that expect a table has been created as the infer function mis\n",
       "allocates the size of columns and isn't best practice not chooses the dytpes\n",
       "of a column, but is much easier to call upon ``infer_to_snowflake`` recommended\n",
       "when in development.\n",
       "Args:\n",
       "* df (pd.DataFrame): Dataframe that is going to be sent to snowflake\n",
       "* table_name (str): Table name being created inside of snowflake\n",
       "* chunk_size (int, optional): Number of elements to be inserted once, if not provided all elements will be dumped once.\n",
       "                                Defaults to None.\n",
       "* create_table (bool, optional): If true will create a table with create_statement. Defaults to False.\n",
       "* create_statement (str, optional): Give create table statement and the table will\n",
       "                                    be created from this string. Defaults to None.\n",
       "* parallel (int, optional): Number of threads to be used when uploading chunks. Defaults to None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect.pandas_to_snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:59.241245Z",
     "start_time": "2022-02-04T00:16:51.536962Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating table\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Table SDSDEQUERYTEST successfully created.\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:upload was a success: True & the number of rows loaded 4\n",
      "INFO:__main__:dataframe uploaded to MACHINELEARNINGOUTPUTS.sdsdequerytest\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n"
     ]
    }
   ],
   "source": [
    "sf.pandas_to_snowflake(df=df,\n",
    "                       table_name='sdsdequerytest',\n",
    "                       table_check=ParseYaml('testing/snowflake.yaml').get_yaml(['test']).get('check_table'),\n",
    "                       chunk_size = None,\n",
    "                       create_table= True,\n",
    "                       create_statement= yaml.get('create_query_table'))\n",
    "df = sf.run_str_query(\"SELECT * FROM sdsdequerytest\")\n",
    "assert df.shape == (4, 4), 'Query 4 observations w/ 4 columns from sdsdequerytest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infer_to_snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:59.253271Z",
     "start_time": "2022-02-04T00:16:59.243614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeConnect.infer_to_snowflake\" class=\"doc_header\"><code>SnowflakeConnect.infer_to_snowflake</code><a href=\"__main__.py#L260\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeConnect.infer_to_snowflake</code>(**`df`**:`DataFrame`, **`table_name`**:`str`, **`if_exists`**:`str`=*`'append'`*, **`dtype`**:`dict`=*`None`*, **`chunk_size`**:`int`=*`None`*)\n",
       "\n",
       "Args:\n",
       "* df (pd.DataFrame): The dataframe that is going to snowflake\n",
       "* table_name (str): Table name that will be created in snowflake\n",
       "* if_exists (str, optional): How to behave if the table already exists.\n",
       "                            * fail: Raise a ValueError.\n",
       "                            * replace: Drop the table before inserting new values.\n",
       "                            * append: Insert new values to the existing table.\n",
       "                            Defaults to 'fail'.\n",
       "* dtype (dict, optional): Specifying the datatype for columns. If a dictionary is used, the\n",
       "                        keys should be the column names and the values should be the\n",
       "                        SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n",
       "                        scalar is provided, it will be applied to all columns.\n",
       "                        Defaults to None.\n",
       "* chunk_size (int, optional): Specify the number of rows in each batch to be written\n",
       "                            at a time. By default, all rows will be written at once.\n",
       "                            Defaults to None."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect.infer_to_snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:11.488778Z",
     "start_time": "2022-02-04T00:16:59.255192Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:SDSDEQUERYTEST successfully dropped.\n",
      "INFO:__main__:Begining upload to sdsdequerytest\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "/home/azureuser/miniconda3/envs/dsde_upgrade/lib/python3.8/site-packages/pandas/io/sql.py:1685: UserWarning: The provided table name 'SDSDEQUERYTEST' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "INFO:__main__:Dataframe uploaded to sdsdequerytest\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:SDSDEQUERYTEST successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "sf.run_str_query('DROP TABLE IF EXISTS sdsdequerytest;')\n",
    "sf.infer_to_snowflake(df,\n",
    "                      table_name='sdsdequerytest',\n",
    "                      if_exists='append')\n",
    "df = sf.run_str_query(\"SELECT * FROM sdsdequerytest\");\n",
    "assert df.shape == (4, 4), 'Query 4 observations w/ 4 columns from sdsdequerytest'\n",
    "sf.run_str_query('DROP TABLE IF EXISTS sdsdequerytest;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large_table_infer_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:11.501090Z",
     "start_time": "2022-02-04T00:17:11.491138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeConnect.large_table_infer_sf\" class=\"doc_header\"><code>SnowflakeConnect.large_table_infer_sf</code><a href=\"__main__.py#L290\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeConnect.large_table_infer_sf</code>(**`df`**:`DataFrame`, **`table_name`**:`str`, **`split`**:`int`, **`if_exists`**:`str`=*`'append'`*, **`dtype`**:`dict`=*`None`*, **`chunk_size`**:`int`=*`None`*)\n",
       "\n",
       "This take infer_to_snowflake and give the ability to split the loads to snowflake\n",
       "this isn't tested for efficiency, but might be useful when not wanting to wait\n",
       "for a 100M+ memory object to make it to snowflake. A user can split this into\n",
       "as many slices as they choose and it will fail if there is something wrong with\n",
       "the upload.\n",
       "\n",
       "Args:\n",
       "* df (pd.DataFrame): in memory pandas dataframe\n",
       "* table_name (str): table name for table being created\n",
       "* split (int): how many splits to split the data frame up in"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect.large_table_infer_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:20.437384Z",
     "start_time": "2022-02-04T00:17:11.503155Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Drop statement executed successfully (SDSDEQUERYTEST already dropped).\n",
      "WARNING:__main__:best to use infer_to_snowflake when splits are less than 50k\n",
      "INFO:__main__:Begining upload to SDSDEQUERYTEST\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:Dataframe uploaded to SDSDEQUERYTEST\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:SDSDEQUERYTEST successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "sf.run_str_query('DROP TABLE IF EXISTS sdsdequerytest;')\n",
    "sf.large_table_infer_sf(df=df, table_name='sdsdequerytest', split=1, if_exists='append')\n",
    "df = sf.run_str_query(\"SELECT * FROM sdsdequerytest\");\n",
    "assert df.shape == (4, 4), 'Query 4 observations w/ 4 columns from sdsdequerytest'\n",
    "sf.run_str_query('DROP TABLE IF EXISTS sdsdequerytest;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this function is to allow a user to make create a table/View and then query the table with a specific command. This can be done the same way with ``execute_file_query``, but the final command will be a the end of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:20.447851Z",
     "start_time": "2022-02-04T00:17:20.439404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeConnect.execute_file\" class=\"doc_header\"><code>SnowflakeConnect.execute_file</code><a href=\"__main__.py#L156\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeConnect.execute_file</code>(**`query_path`**:`str`, **`sql`**:`str`=*`None`*, **`role`**=*`None`*, **`schema`**=*`None`*, **`password`**=*`None`*, **`user`**=*`None`*, **`warehouse`**=*`None`*, **`database`**=*`None`*)\n",
       "\n",
       "will run sql file or txt file that is seperated by self.delimenter\n",
       "with the option to add a custom sql command if you run an sql command\n",
       "then the sql command will be the returned response\n",
       "\n",
       "Args:\n",
       "* query_path (str): location to sql/txt file to execute. Defaults to None.\n",
       "* sql (str, optional): Optional Command to add on top of the file being executed. Defaults to None.\n",
       "\n",
       "Returns:\n",
       "* pd.DataFrame: returns query results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect.execute_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:25.565856Z",
     "start_time": "2022-02-04T00:17:20.449840Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:query being parsed\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:sql or txt file excecuted\n",
      "INFO:__main__:connection to snowflake has been turned off\n"
     ]
    }
   ],
   "source": [
    "df = sf.execute_file(query_path='testing/view_test.sql')\n",
    "assert df.status.values[0] == 'Table SDSDE_TEST successfully created.', 'Snowflake changed or credintials did'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:32.183440Z",
     "start_time": "2022-02-04T00:17:25.567660Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:query being parsed\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:sql or txt file excecuted\n",
      "INFO:__main__:Last query response                                    status\n",
      "0  Table SDSDE_TEST successfully created.\n",
      "INFO:__main__:data is loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:SDSDE_TEST successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "df = sf.execute_file(sql='SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.sdsde_test WHERE IsCurrent = 1', query_path='testing/view_test.sql')\n",
    "assert df.shape == (1, 38), 'Query 10 observations w/ 25 columns from DIMCUSTOMER'\n",
    "sf.run_str_query('DROP TABLE IF EXISTS sdsde_test;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:32.193213Z",
     "start_time": "2022-02-04T00:17:32.185219Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeConnect.run_str_query\" class=\"doc_header\"><code>SnowflakeConnect.run_str_query</code><a href=\"__main__.py#L119\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeConnect.run_str_query</code>(**`query`**:`str`, **`role`**=*`None`*, **`schema`**=*`None`*, **`password`**=*`None`*, **`user`**=*`None`*, **`warehouse`**=*`None`*, **`database`**=*`None`*)\n",
       "\n",
       "This function will connect to snowflake and run a query that is passed\n",
       "in as a string.\n",
       "Args:\n",
       "* query (str): SQL Query String\n",
       "Returns:\n",
       "* pd.DataFrame/ None: dependent on the query string"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect.run_str_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:33.611769Z",
     "start_time": "2022-02-04T00:17:32.195980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n"
     ]
    }
   ],
   "source": [
    "df = sf.run_str_query(\"SELECT * FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100\")\n",
    "assert df.shape == (100, 8), 'Query 10 observations w/ 38 columns from DIMCUSTOMER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent Query Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:33.621500Z",
     "start_time": "2022-02-04T00:17:33.613769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SnowflakeConnect.execute_concurrent_query\" class=\"doc_header\"><code>SnowflakeConnect.execute_concurrent_query</code><a href=\"__main__.py#L398\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SnowflakeConnect.execute_concurrent_query</code>(**`query_path`**:`str`)\n",
       "\n",
       "Give a user the ability to call multiple sql commands to\n",
       "snowflake on a concurrent method giving each command a thread\n",
       "to communicate to snowflake to allow snowflake to do more than\n",
       "one command at a time as the current methods wait until each query\n",
       "is completed.\n",
       "\n",
       "Args:\n",
       "* query_path (str): file path\n",
       "\n",
       "Returns:\n",
       "* List: Returns Snowflake Response in a list"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SnowflakeConnect.execute_concurrent_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "\n",
    "    Why is this something that you will need in your daily work life?\n",
    "    \n",
    "**Answer**\n",
    "\n",
    "   1. This can be used for reporting where there is a need to create multi tables at a time while all the ancillary tables are already ready to be queried. \n",
    "   \n",
    "\n",
    "\n",
    "**What this saves?**\n",
    "\n",
    "This gives you the ability to not call ``run_str_query`` multiple times and will do what ``execute_file`` does, but it will do it in concurrency so that snowflake query engine will be able to run multiple jobs with out waiting for each one to complete before finishing. This would save the ML_Pipeline 10-15 minutes off it's run if it was a part of the library during it's creation.\n",
    "\n",
    "This function will not be an extremely popular function in all reality because the majority of the time we will be query the feature store or complex CTE query logic that doesn't actually need this functionality, but when threading multiple calls is of interest of a user it will be possible with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:36.743810Z",
     "start_time": "2022-02-04T00:17:33.623299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:new thread started\n",
      "INFO:root:new thread started\n",
      "INFO:__main__:testing connection\n",
      "INFO:root:new thread started\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:root:Finishing SELECT ecid,\tlikelihoodtoreturnrate,\tlikelihoodtoreturnlabel FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100;\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Table SDSDE_TEST_2 successfully created.\n",
      "INFO:root:Finishing CREATE OR REPLACE TABLE sdsde_test_2 AS SELECT ecid,\tlikelihoodtoreturnrate,\tlikelihoodtoreturnlabel FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100;\n",
      "\n",
      "INFO:root:snowflake has no response for this action\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Table SDSDE_TEST successfully created.\n",
      "INFO:root:Finishing CREATE OR REPLACE TABLE sdsde_test AS SELECT ecid,\tlikelihoodtoreturnrate,\tlikelihoodtoreturnlabel FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100;\n",
      "\n",
      "INFO:root:snowflake has no response for this action\n"
     ]
    }
   ],
   "source": [
    "df = sf.execute_concurrent_query('./testing/multi.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just going to clean up the tables that were created and just another example of executing more than on SQL command in  an concurrent approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:41.161621Z",
     "start_time": "2022-02-04T00:17:38.634266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:new thread started\n",
      "INFO:root:new thread started\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:SDSDE_TEST successfully dropped.\n",
      "INFO:root:Finishing DROP TABLE sdsde_test;\n",
      "\n",
      "INFO:root:snowflake has no response for this action\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:SDSDE_TEST_2 successfully dropped.\n",
      "INFO:root:Finishing DROP TABLE sdsde_test_2;\n",
      "INFO:root:snowflake has no response for this action\n"
     ]
    }
   ],
   "source": [
    "df = sf.execute_concurrent_query('./testing/clean_multi.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:17:41.992248Z",
     "start_time": "2022-02-04T00:17:41.164265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
