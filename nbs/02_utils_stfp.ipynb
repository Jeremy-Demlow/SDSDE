{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:15.672603Z",
     "start_time": "2022-02-04T00:15:15.668418Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp utils.sftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:16.548306Z",
     "start_time": "2022-02-04T00:15:15.878029Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:17.494804Z",
     "start_time": "2022-02-04T00:15:16.550933Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import store_attr\n",
    "from fastcore.meta import delegates\n",
    "from sdsde.snowflake.query import SnowflakeConnect\n",
    "from sdsde.snowflake.copyinto import CopyInto\n",
    "from sdsde.azure.filehandling import unlink_files\n",
    "from sdsde.utils.parseyaml import ParseYaml\n",
    "from sdsde.wrapper.azurewrapper import blob_pusher\n",
    "\n",
    "import logging\n",
    "import pysftp\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:17.540352Z",
     "start_time": "2022-02-04T00:15:17.496898Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "@delegates()\n",
    "class SFTP:\n",
    "\n",
    "    def __init__(self, yaml_file: str = None, yaml_file_path: str = None, yaml_section: str = None,\n",
    "                 logger=None, **kwargs):\n",
    "        \"\"\"\n",
    "        __init__ SFTP sdsde is looking for a yaml file that can be seen in the\n",
    "        ``02_utils_stfp``, which has more than the needed variables to be be able to\n",
    "        uttlize this class. This class was created to unifiy the majority of the SFTP\n",
    "        calls that we have begun to accumulate and rather than have many functions\n",
    "        that are slightly different having one function that does 80-90% of what we\n",
    "        need as a sdsde & RDE Team was the goal of this function.\n",
    "\n",
    "        This function is in current development and will likely change, but the __init__\n",
    "        checks for proper connection to sftp host as well as gets the yaml variables need\n",
    "        that are passed through out this function as well as initializes a snowflake\n",
    "        connection.\n",
    "\n",
    "        >> Note: This needs a yaml file\n",
    "\n",
    "        TODO: write function to intake just a python\n",
    "        dictionary or give it the option. This would take just setting everything to\n",
    "        None and then giving a dict_only option simple, but don't want to write more\n",
    "        at the moment.\n",
    "\n",
    "        TODO: Re-write to have more delegation and work cleaner, the next time this\n",
    "        is visited this will change a lot to allow for more flexibility. This has been\n",
    "        proven to work with LIVERAMP as well as MTA for the needs that have decreased\n",
    "        the amount of development needed even in this state.\n",
    "\n",
    "        Args:\n",
    "        * yaml_file (str): Yaml file name\n",
    "        * yaml_file_path (str): Yaml file path\n",
    "        * yaml_section (str, optional): The Section is optional, but when giving\n",
    "        a section you will be able to pull specfic sections of the yaml file say\n",
    "        if you needed to loop this function and have multi sftp drops in one repo. Defaults to None.\n",
    "        * logger (optional): logger sdsde staple for most functions will default\n",
    "        to standard logging. Defaults to None.\n",
    "        \"\"\"\n",
    "        self._logger = logger if logger is not None else logging.getLogger(__name__)\n",
    "        yaml_file_path = yaml_file_path if yaml_file_path.endswith('/') else yaml_file_path + '/'\n",
    "        if yaml_section is None:\n",
    "            self.sftp_yaml = ParseYaml(os.path.join(yaml_file_path, yaml_file)).get_yaml()\n",
    "        else:\n",
    "            self.sftp_yaml = ParseYaml(os.path.join(yaml_file_path, yaml_file)).get_yaml([yaml_section])\n",
    "        self.cnopts = pysftp.CnOpts()\n",
    "        self.cnopts.hostkeys = None\n",
    "        try:\n",
    "            self.connection = pysftp.Connection(host=os.environ.get(self.sftp_yaml['sftp_host']),\n",
    "                                                username=os.environ.get(self.sftp_yaml['sftp_user']),\n",
    "                                                password=os.environ.get(self.sftp_yaml['sftp_pass']),\n",
    "                                                private_key=os.environ.get(self.sftp_yaml['sftp_pass']),\n",
    "                                                port=self.sftp_yaml['sftp_port'],\n",
    "                                                cnopts=self.cnopts)\n",
    "            self._logger.info('SFTP Connection Established...')\n",
    "        except pysftp.ConnectionException:\n",
    "            self._logger.error('failed to connect to SFTP')\n",
    "        self.connection.close()\n",
    "        self._logger.error('SFTP Test Connection Closed')\n",
    "        self._logger.info(f\"this is a {os.environ['prod_or_dev']} run..\")\n",
    "        self.sf = SnowflakeConnect(sfAccount=os.environ.get(self.sftp_yaml['sfAccount']),\n",
    "                                sfUser=os.environ.get(self.sftp_yaml['sfUser']),\n",
    "                                sfPswd=os.environ.get(self.sftp_yaml['sfPswd']),\n",
    "                                sfWarehouse=os.environ.get(self.sftp_yaml['sfWarehouse']),\n",
    "                                sfDatabase=os.environ.get(self.sftp_yaml['sfDatabase']),\n",
    "                                sfSchema=os.environ.get(self.sftp_yaml['sfSchema']),\n",
    "                                sfRole=os.environ.get(self.sftp_yaml['sfRole']))\n",
    "        store_attr()\n",
    "\n",
    "    def sftp_upload(self,\n",
    "                    upload_dir: str = '.',\n",
    "                    file_dir: str = '.',\n",
    "                    azure_blob_dest='.',\n",
    "                    suffix: str = '',\n",
    "                    query: bool = True,\n",
    "                    store: bool = True,\n",
    "                    overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        SFTP Upload for 80-90% of the needs that could be thought of at\n",
    "        time of development. This function allows the user to decide if they\n",
    "        want to be able to query from snowflake -> Azure Blob -> sftp.\n",
    "        Giving the option to the user, but defaults to option above\n",
    "\n",
    "        Args:\n",
    "        * upload_dir (str, optional): upload directory to sftp. Deafults to '.'.\n",
    "        * file_dir (str, optional): file directory for files needed (SQL). Defaults to '.'.\n",
    "        * azure_blob_dest(str, optional): for prod of dev in the drop. Defaults to '.'.\n",
    "        * suffix (str, optional): adding unique suffix to file name. Defaults to ''.\n",
    "        * query (bool, optional): Query Snowflake to get data. Defaults to False.\n",
    "        * store (bool, optional): Store in Azure Blob. Defaults to False.\n",
    "        * overwrite (bool, optional): Overwrite file in blob if same file name. Defaults to False.\n",
    "        \"\"\"\n",
    "        file_name = self.sftp_yaml['sftp_filename'] + suffix + self.sftp_yaml['sftp_filetype']\n",
    "        if query is True:\n",
    "            self.snowflake_query(file_name=file_name, file_dir=file_dir)\n",
    "        if store is True:\n",
    "            self.azure_load(file_name, azure_blob_dest, overwrite)\n",
    "        sftpconnection = self.sftp_connection()\n",
    "        with sftpconnection.cd(upload_dir):\n",
    "            self._logger.info(f'checking if {file_name} already exisits...')\n",
    "            if file_name in sftpconnection.listdir(upload_dir):\n",
    "                self._logger.warning('File Already Exisits And Will Be Over Written')\n",
    "            try:\n",
    "                sftpconnection.put(os.path.join(file_name))\n",
    "                if sftpconnection.exists(f\"{file_name}\"):\n",
    "                    self._logger.info(f\"{file_name} recieved and dropped to {upload_dir}\")\n",
    "            except AttributeError:\n",
    "                self._logger.error('file did not get dropped in sftp')\n",
    "                sys.exit()\n",
    "        unlink_files([file_name])\n",
    "        self._logger.info('sftp upload complete')\n",
    "\n",
    "    def sftp_grab(self,\n",
    "                  table_name_sf: str = '',\n",
    "                  prod_or_dev: str = '',\n",
    "                  file_name: str = '',\n",
    "                  download_dir: str = '.',\n",
    "                  suffix: str = '',\n",
    "                  azure_blob_dest='.',\n",
    "                  store: bool = True,\n",
    "                  create_snowflake_table: bool = True,\n",
    "                  varchar: bool = True,\n",
    "                  overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        SFTP function that will grab a file from and sftp host and\n",
    "        has the options to add it to azure blob and snowflake, but\n",
    "        if they are both set to False then it will be a simple grab\n",
    "        of a file from an sftp host.\n",
    "\n",
    "        Args:\n",
    "        * table_name_sf (str, optional): table name to load into snowflake\n",
    "        * prod_or_dev: (str, optional): to tell function if it's a dev run or not\n",
    "        * file_name (str, optional): optional because MTA needs flex. Defaults to None.\n",
    "        * download_dir (str, optional): upload directory to sftp. Defaults to '.'.\n",
    "        * suffix (str, optional): [description]. Defaults to ''.\n",
    "        * azure_blob_dest(str, optional): for prod of dev in the drop. Defaults to '.'.\n",
    "        * store (bool, optional): [description]. Defaults to True.\n",
    "        * create_snowflake_table (bool, optional): [description]. Defaults to True.\n",
    "        * varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
    "        * overwrite (bool, optional): [description]. Defaults to False.\n",
    "        \"\"\"\n",
    "        if file_name == '':\n",
    "            file_name = self.sftp_yaml['sftp_filename'] + suffix + self.sftp_yaml['sftp_filetype']\n",
    "        if suffix != '':\n",
    "            file_name = self.sftp_yaml['sftp_filename'] + suffix + self.sftp_yaml['sftp_filetype']\n",
    "        self._logger.info(f\"file name {file_name}\")\n",
    "        sftpconnection = self.sftp_connection()\n",
    "        with sftpconnection.cd(download_dir):\n",
    "            sftpconnection.get(file_name)\n",
    "            self._logger.info(f\"\"\"{file_name} recieved and dropped to\n",
    "                               {os.path.join(download_dir,file_name)}\"\"\")\n",
    "        self._logger.info('Beginning Azure and Snowflake push')\n",
    "        table_name = table_name_sf if prod_or_dev == 'dev' else \\\n",
    "            table_name_sf.lower().replace('dev', os.environ.get(self.sftp_yaml['sfSchema']))\n",
    "        if store is True:\n",
    "            self.azure_load(file_name, azure_blob_dest, overwrite)\n",
    "        if create_snowflake_table is True:\n",
    "            df = pd.read_csv(file_name, nrows=10000, low_memory=True)\n",
    "            unlink_files([file_name])\n",
    "            df.columns = [x.replace(' ', '_') for x in df.columns]\n",
    "            self.copy_into_snowflake(file_name, azure_blob_dest, table_name_sf, varchar, df)\n",
    "\n",
    "        self._logger.info(f\"\"\"query new table created...\n",
    "                            {self.sf.run_str_query(f\"SELECT TOP 10 * FROM {table_name};\").head(2)}\n",
    "                            \"\"\")\n",
    "        self._logger.info('sftp grab complete')\n",
    "\n",
    "    def azure_load(self, file_name: str, azure_blob_dest: str, overwrite: bool):\n",
    "        \"\"\"\n",
    "        This function sends the file of interest to the desired\n",
    "        location in Azure Blob.\n",
    "\n",
    "        >> Note: when using this with grab and if the file is the\n",
    "        same the copy into command knows that the meta data\n",
    "        hasn't change and will not work and will fail.\n",
    "\n",
    "        Args:\n",
    "        * file_name (str): file_name to send to azure\n",
    "        * overwrite (bool): overwrite file in azure\n",
    "        \"\"\"\n",
    "        blob_dest = azure_blob_dest if azure_blob_dest.endswith('/') else azure_blob_dest + '/'\n",
    "        self._logger.info(f\"saving file to azure blob at {blob_dest}\")\n",
    "        blob_pusher(container_name=self.sftp_yaml['container_name'],\n",
    "                    connection_str=os.environ.get(self.sftp_yaml['connection_string']),\n",
    "                    file_path=[file_name],\n",
    "                    blob_dest=[blob_dest],\n",
    "                    overwrite=overwrite)\n",
    "        self._logger.info('file to azure load complete')\n",
    "        self._logger.info('begining sftp upload/download')\n",
    "\n",
    "    def copy_into_snowflake(self,\n",
    "                            file_name: str,\n",
    "                            azure_blob_dest: str,\n",
    "                            table_name_sf: str,\n",
    "                            varchar: bool,\n",
    "                            df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Copy into function for SFTP this function will makes sure\n",
    "        that the table that is being appended to with the copy into\n",
    "        command exisits and will check if the table exisits in the\n",
    "        current schema that the snowflake connection is connected to.\n",
    "\n",
    "        Args:\n",
    "        * file_name (str): file name in azure blob to append (CopyInto)\n",
    "        * azure_blob_dest (str): [description]\n",
    "        * table_name_sf (str): [description]\n",
    "        * varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
    "        * df (pd.DataFrame): for when table doesn't exisit will use dytpes\n",
    "        \"\"\"\n",
    "        blob_dest = azure_blob_dest[:-1] if azure_blob_dest.endswith('/') else azure_blob_dest\n",
    "        blob_dest = blob_dest[1:] if blob_dest.startswith('/') else blob_dest\n",
    "        ci = CopyInto(sfAccount=os.environ.get(self.sftp_yaml['sfAccount']),\n",
    "                      sfUser=os.environ.get(self.sftp_yaml['sfUser']),\n",
    "                      sfPswd=os.environ.get(self.sftp_yaml['sfPswd']),\n",
    "                      sfWarehouse=os.environ.get(self.sftp_yaml['sfWarehouse']),\n",
    "                      sfDatabase=os.environ.get(self.sftp_yaml['sfDatabase']),\n",
    "                      sfSchema=os.environ.get(self.sftp_yaml['sfSchema']),\n",
    "                      sfRole=os.environ.get(self.sftp_yaml['sfRole']))\n",
    "        if table_name_sf.split('.')[-1].upper() not in self.sf.run_str_query(f\"Show tables in SCHEMA {self.sf.sfDatabase.upper()}.{self.sf.sfSchema.upper()};\").name.tolist():\n",
    "            self._logger.info('new snowflake table being created')\n",
    "            self.create_sf_table_from_df(df, table_name_sf, varchar)\n",
    "        else:\n",
    "            self._logger.warning('Table Already Exist will only appened to current table')\n",
    "        try:\n",
    "            self._logger.info('Pushing Top Ranked file to Snowflake')\n",
    "            ci.insert_csv(blob_name=file_name,\n",
    "                          blob_path=blob_dest,\n",
    "                          storage_account=self.sftp_yaml['storage_account'],\n",
    "                          container_name=self.sftp_yaml['container_name'],\n",
    "                          table_name=table_name_sf,\n",
    "                          sas_token=os.environ.get(self.sftp_yaml['insert_token']),\n",
    "                          encoding=self.sftp_yaml['encoding'],\n",
    "                          delimiter=self.sftp_yaml['delimiter'],\n",
    "                          )\n",
    "        except AssertionError as e:\n",
    "            self._logger.error(f'snowflake upload failed {e}')\n",
    "            sys.exit()\n",
    "\n",
    "    def snowflake_query(self, file_name: str, file_dir: str):\n",
    "        \"\"\"\n",
    "        Function to query and write out query to .csv format\n",
    "        currently all use cases need this as the consumer and\n",
    "        vendors send .csv or .csv.gz.\n",
    "\n",
    "        Args:\n",
    "        * file_name (str): file name locally to execute\n",
    "        * file_dir (str): file directory that the sql file is in can be root level\n",
    "        \"\"\"\n",
    "        self._logger.info('beginning sftp query...')\n",
    "        df = self.sf.execute_file(os.path.join(file_dir, self.sftp_yaml['query_file']))\n",
    "        self._logger.info(f'Query result shape {df.shape}')\n",
    "        df.to_csv(file_name, index=False)\n",
    "        self._logger.info('file written to local root ready for ingestion...')\n",
    "\n",
    "    def create_sf_table_from_df(self, df: pd.DataFrame, table_name_sf: str, varchar: bool):\n",
    "        \"\"\"\n",
    "        Dynamically create a table from a dataframe and\n",
    "        change the dtypes to snowflake dytpes this may have\n",
    "        a limitation, but can be added.\n",
    "\n",
    "        Args:\n",
    "        * df (pd.DataFrame): data frame to get dtypes\n",
    "        * table_name_sf (str): snowflake table name\n",
    "        * varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
    "        \"\"\"\n",
    "        select_query = f'''\n",
    "            create or replace table {table_name_sf} (FEATURES_HERE);\n",
    "            '''\n",
    "        for k, v in dict(df.dtypes).items():\n",
    "            select_query = select_query.replace('FEATURES_HERE', f'{k} {self.return_sf_type(str(v), varchar=varchar)}, FEATURES_HERE')\n",
    "        select_query = select_query.replace(', FEATURES_HERE', '')\n",
    "        logger.info(select_query)\n",
    "        self.sf.run_str_query(select_query)\n",
    "\n",
    "    def return_sf_type(self, dtype: str, varchar: bool):\n",
    "        \"\"\"\n",
    "        simple function to convert dytpes to snowflake dtypes this\n",
    "        will be come a very useful thing to have as this will dtype\n",
    "        data from 10,000 rows from a dataframe, and will fail out\n",
    "        on the push to snowflake. This means the vendor or the current\n",
    "        SFTP needs to have a cleaning added to it to allow for data\n",
    "        to make it through the pipeline. TODO: Figure out how to accepet\n",
    "        a ETL clean of the data from a vendor that can be placed in place\n",
    "\n",
    "        Args:\n",
    "        * dtype (str): dtype from a df in sting form\n",
    "        * varchar (bool): to default all variables to VARCHAR\n",
    "        this happens due to bad vendor data and can't be resloved\n",
    "        with out reading in the whole data set with low_memory=False\n",
    "\n",
    "        Returns:\n",
    "        * str: snowflake dtype\n",
    "        \"\"\"\n",
    "        if varchar is True:\n",
    "            dtype = 'VARCHAR'\n",
    "        elif 'int' in dtype.lower():\n",
    "            dtype = 'NUMBER'\n",
    "        elif 'float' in dtype.lower():\n",
    "            dtype = 'FLOAT'\n",
    "        elif 'object' in dtype.lower():\n",
    "            dtype = 'VARCHAR'\n",
    "        elif 'bool' in dtype.lower():\n",
    "            dtype = 'VARCHAR'  # TODO: Limitation found before change once resloved by sf\n",
    "        elif 'date' in dtype.lower():\n",
    "            dtype = 'DATETIME'  # TODO: Might break with certain datetimes most generic\n",
    "        else:\n",
    "            logging.error('odd dtype not seen needs to be resloved...')\n",
    "            sys.exit()\n",
    "        return dtype\n",
    "\n",
    "    def sftp_connection(self):\n",
    "        \"\"\"\n",
    "        SFTP connection due to time out and limitations to pysftp\n",
    "        we will be using this function in the connection to pysftp\n",
    "        \"\"\"\n",
    "        connection = pysftp.Connection(host=os.environ.get(self.sftp_yaml['sftp_host']),\n",
    "                                       username=os.environ.get(self.sftp_yaml['sftp_user']),\n",
    "                                       password=os.environ.get(self.sftp_yaml['sftp_pass']),\n",
    "                                       private_key=os.environ.get(self.sftp_yaml['sftp_pass']),\n",
    "                                       port=self.sftp_yaml['sftp_port'],\n",
    "                                       cnopts=self.cnopts)\n",
    "        return connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFTP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:17.561928Z",
     "start_time": "2022-02-04T00:15:17.544187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SFTP\" class=\"doc_header\"><code>class</code> <code>SFTP</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SFTP</code>(**`yaml_file`**:`str`=*`None`*, **`yaml_file_path`**:`str`=*`None`*, **`yaml_section`**:`str`=*`None`*, **`logger`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SFTP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is expecting a yaml file to be created with the necessary information in it to be able to complete a SFTP successfully. The information in the yaml file created below is more than it needs for an upload or a grab, but together it will do all of the abilities. \n",
    "\n",
    "> **Note/Warning**: the keys must be the same name but the values can be different names as the function is reading from the yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml File Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:17.990391Z",
     "start_time": "2022-02-04T00:15:17.981843Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def yaml_parse(file_name_path: str, yaml_section: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Adding functionality to ParseYaml from sdsde lib creates a\n",
    "    simple call to pull parts of the yaml file for each function\n",
    "\n",
    "    Args:\n",
    "    * file_name (str): Yaml File Name\n",
    "    * yaml_section (str): Yaml section to read\n",
    "\n",
    "    Returns:\n",
    "        dict: yaml section to parse\n",
    "    \"\"\"\n",
    "    if yaml_section is not None:\n",
    "        yaml = ParseYaml(file_name_path).get_yaml([\n",
    "            os.environ.get('prod_or_dev', 'dev'), yaml_section])\n",
    "    else:\n",
    "        yaml = ParseYaml(file_name_path).get_yaml([os.environ.get('prod_or_dev', 'dev')])\n",
    "    return yaml\n",
    "\n",
    "\n",
    "def write_yaml_file(file_path: str, file_name: str, dictionary: dict):\n",
    "    with open(Path(file_path, file_name), 'w') as f:\n",
    "        yaml.dump(dictionary, f)\n",
    "\n",
    "\n",
    "def snowflake_query(sfAccount: str = os.environ.get('sfAccount', None),\n",
    "                    sfUser: str = os.environ.get('sfUser', None),\n",
    "                    sfPswd: str = os.environ.get('sfPswd', None),\n",
    "                    sfWarehouse: str = os.environ.get('sfWarehouse', None),\n",
    "                    sfDatabase: str = os.environ.get('sfDatabase', None),\n",
    "                    sfSchema: str = os.environ.get('sfSchema', None),\n",
    "                    sfRole: str = os.environ.get('sfRole', None)):\n",
    "    \"\"\"Easy Connection To SnowFlake When Environs are set\"\"\"\n",
    "    sf = SnowflakeConnect(sfAccount, sfUser, sfPswd, sfWarehouse,\n",
    "                       sfDatabase, sfSchema, sfRole)\n",
    "    return sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:35.247457Z",
     "start_time": "2022-02-04T00:15:35.243388Z"
    }
   },
   "outputs": [],
   "source": [
    "# skip\n",
    "sftp_dict = dict({\n",
    "    'sftp': {\n",
    "        'sfAccount': 'sfAccount',\n",
    "        'sfUser':  'sfUser',\n",
    "        'sfPswd':  'sfPswd',\n",
    "        'sfWarehouse':  'sfWarehouse',\n",
    "        'sfDatabase':  'sfDatabase',\n",
    "        'sfSchema':  'sfSchema',\n",
    "        'sfRole':  'sfRole',\n",
    "        'sfSchema_Prod' : 'prod',\n",
    "        'query_file': 'sftp_query.sql',\n",
    "        'insert_table_name' : 'MACHINELEARNINGOUTPUTS.DEV.SFTP_TEST_TABLE',\n",
    "        'delimiter' : ',',\n",
    "        'insert_token' : 'DATALAKE_SAS_TOKEN_SECRET',\n",
    "        'encoding' : 'UTF-8',\n",
    "        'storage_account':os.environ['azure_account'],\n",
    "        'connection_string': 'DATALAKE_CONN_STR_SECRET',\n",
    "        'blob_dest' : '/sftp_test/grab_test',\n",
    "        'container_name': 'taladls',\n",
    "        'sftp_dir_location' : '/uploads/TestNewLRProcess',\n",
    "        'sftp_host': 'sftp_host',\n",
    "        'sftp_pass': 'sftp_pass',\n",
    "        'sftp_user': 'sftp_user',\n",
    "        'sftp_filename': 'sftp_test',\n",
    "        'sftp_filetype': '.csv',\n",
    "        'sftp_port': 22,\n",
    "        'sftp_key' : 'sftp_key'\n",
    "             },\n",
    "})\n",
    "\n",
    "write_yaml_file('./testing/', 'sftp.yaml', sftp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIT SFTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:13:24.951518Z",
     "start_time": "2022-02-01T23:13:24.945787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SFTP\" class=\"doc_header\"><code>class</code> <code>SFTP</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SFTP</code>(**`yaml_file`**:`str`=*`None`*, **`yaml_file_path`**:`str`=*`None`*, **`yaml_section`**:`str`=*`None`*, **`logger`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SFTP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure you have an SFTP location to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:13:31.467328Z",
     "start_time": "2022-02-01T23:13:31.464847Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# skip\n",
    "os.environ['sftp_host']=<fill_in_yours>\n",
    "os.environ['sftp_user']=<fill_in_yours>\n",
    "os.environ['sftp_pass']=<fill_in_yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:13:38.153393Z",
     "start_time": "2022-02-01T23:13:36.121590Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paramiko.transport:Connected (version 2.0, client FTP)\n",
      "INFO:paramiko.transport:Auth banner: b'This system is solely for the use of authorized users for business purposes.\\r\\nIndividuals using this system are subject to having all of their activities\\r\\nmonitored and recorded by system personnel. Unauthorized access or use\\r\\nshall be subject to prosecution.\\r\\n\\r\\n'\n",
      "INFO:paramiko.transport:Authentication (password) successful!\n",
      "INFO:__main__:SFTP Connection Established...\n",
      "ERROR:__main__:SFTP Test Connection Closed\n",
      "INFO:__main__:this is a dev run..\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "test = SFTP(yaml_file='sftp.yaml', yaml_file_path='./testing/', yaml_section='sftp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFTP Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:40.717178Z",
     "start_time": "2022-02-04T00:15:40.705178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SFTP.sftp_upload\" class=\"doc_header\"><code>SFTP.sftp_upload</code><a href=\"__main__.py#L73\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SFTP.sftp_upload</code>(**`upload_dir`**:`str`=*`'.'`*, **`file_dir`**:`str`=*`'.'`*, **`azure_blob_dest`**=*`'.'`*, **`suffix`**:`str`=*`''`*, **`query`**:`bool`=*`True`*, **`store`**:`bool`=*`True`*, **`overwrite`**:`bool`=*`False`*)\n",
       "\n",
       "SFTP Upload for 80-90% of the needs that could be thought of at\n",
       "time of development. This function allows the user to decide if they\n",
       "want to be able to query from snowflake -> Azure Blob -> sftp.\n",
       "Giving the option to the user, but defaults to option above\n",
       "\n",
       "Args:\n",
       "* upload_dir (str, optional): upload directory to sftp. Deafults to '.'.\n",
       "* file_dir (str, optional): file directory for files needed (SQL). Defaults to '.'.\n",
       "* azure_blob_dest(str, optional): for prod of dev in the drop. Defaults to '.'.\n",
       "* suffix (str, optional): adding unique suffix to file name. Defaults to ''.\n",
       "* query (bool, optional): Query Snowflake to get data. Defaults to False.\n",
       "* store (bool, optional): Store in Azure Blob. Defaults to False.\n",
       "* overwrite (bool, optional): Overwrite file in blob if same file name. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SFTP.sftp_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:14:00.817148Z",
     "start_time": "2022-02-01T23:13:40.213864Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:beginning sftp query...\n",
      "INFO:sdsde.snowflake.query:query being parsed\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:sql or txt file excecuted\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:__main__:Query result shape (10, 38)\n",
      "INFO:__main__:file written to local root ready for ingestion...\n",
      "INFO:__main__:saving file to azure blob at /sftp_test/grab_test/\n",
      "INFO:sdsde.azure.filehandling:digitaladls is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading sftp_test_20220201.csv, to to Azure Storage /sftp_test/grab_test/sftp_test_20220201.csv\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n",
      "INFO:__main__:file to azure load complete\n",
      "INFO:__main__:begining sftp upload/download\n",
      "INFO:paramiko.transport:Connected (version 2.0, client FTP)\n",
      "INFO:paramiko.transport:Auth banner: b'This system is solely for the use of authorized users for business purposes.\\r\\nIndividuals using this system are subject to having all of their activities\\r\\nmonitored and recorded by system personnel. Unauthorized access or use\\r\\nshall be subject to prosecution.\\r\\n\\r\\n'\n",
      "INFO:paramiko.transport:Authentication (password) successful!\n",
      "INFO:paramiko.transport.sftp:[chan 0] Opened sftp connection (server version 3)\n",
      "INFO:__main__:checking if sftp_test_20220201.csv already exisits...\n",
      "WARNING:__main__:File Already Exisits And Will Be Over Written\n",
      "INFO:__main__:sftp_test_20220201.csv recieved and dropped to /uploads/TestNewLRProcess\n",
      "INFO:__main__:sftp upload complete\n",
      "INFO:paramiko.transport.sftp:[chan 0] sftp session closed.\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "from datetime import datetime\n",
    "test.sftp_upload(upload_dir=test.sftp_yaml['sftp_dir_location'],\n",
    "                 file_dir = 'testing/',\n",
    "                 azure_blob_dest = test.sftp_yaml['blob_dest'],\n",
    "                 suffix=f\"_{datetime.today().strftime('%Y%m%d')}\",\n",
    "                 query=True, store=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:14:01.263560Z",
     "start_time": "2022-02-01T23:14:00.819169Z"
    }
   },
   "outputs": [],
   "source": [
    "# skip\n",
    "from sdsde.azure.filehandling import FileHandling\n",
    "fh = FileHandling(os.environ.get('DATALAKE_CONN_STR_SECRET'))\n",
    "check = fh.ls_blob(test.sftp_yaml['container_name'], path=test.sftp_yaml['blob_dest'], recursive=True) \n",
    "check = [x.split('/')[-1] for x in check]\n",
    "assert 'sftp_test' + f\"_{datetime.today().strftime('%Y%m%d')}\" + '.csv' in check, 'file did not make it to azure blob'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFTP Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:15:44.148410Z",
     "start_time": "2022-02-04T00:15:44.135137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SFTP.sftp_grab\" class=\"doc_header\"><code>SFTP.sftp_grab</code><a href=\"__main__.py#L116\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SFTP.sftp_grab</code>(**`table_name_sf`**:`str`=*`''`*, **`prod_or_dev`**:`str`=*`''`*, **`file_name`**:`str`=*`''`*, **`download_dir`**:`str`=*`'.'`*, **`suffix`**:`str`=*`''`*, **`azure_blob_dest`**=*`'.'`*, **`store`**:`bool`=*`True`*, **`create_snowflake_table`**:`bool`=*`True`*, **`varchar`**:`bool`=*`True`*, **`overwrite`**:`bool`=*`False`*)\n",
       "\n",
       "SFTP function that will grab a file from and sftp host and\n",
       "has the options to add it to azure blob and snowflake, but\n",
       "if they are both set to False then it will be a simple grab\n",
       "of a file from an sftp host.\n",
       "\n",
       "Args:\n",
       "* table_name_sf (str, optional): table name to load into snowflake\n",
       "* prod_or_dev: (str, optional): to tell function if it's a dev run or not\n",
       "* file_name (str, optional): optional because MTA needs flex. Defaults to None.\n",
       "* download_dir (str, optional): upload directory to sftp. Defaults to '.'.\n",
       "* suffix (str, optional): [description]. Defaults to ''.\n",
       "* azure_blob_dest(str, optional): for prod of dev in the drop. Defaults to '.'.\n",
       "* store (bool, optional): [description]. Defaults to True.\n",
       "* create_snowflake_table (bool, optional): [description]. Defaults to True.\n",
       "* varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
       "* overwrite (bool, optional): [description]. Defaults to False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SFTP.sftp_grab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:14:21.858016Z",
     "start_time": "2022-02-01T23:14:04.563190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:file name sftp_test_20220201.csv\n",
      "INFO:paramiko.transport:Connected (version 2.0, client FTP)\n",
      "INFO:paramiko.transport:Auth banner: b'This system is solely for the use of authorized users for business purposes.\\r\\nIndividuals using this system are subject to having all of their activities\\r\\nmonitored and recorded by system personnel. Unauthorized access or use\\r\\nshall be subject to prosecution.\\r\\n\\r\\n'\n",
      "INFO:paramiko.transport:Authentication (password) successful!\n",
      "INFO:paramiko.transport.sftp:[chan 0] Opened sftp connection (server version 3)\n",
      "INFO:__main__:sftp_test_20220201.csv recieved and dropped to\n",
      "                               /uploads/TestNewLRProcess/sftp_test_20220201.csv\n",
      "INFO:__main__:Beginning Azure and Snowflake push\n",
      "INFO:__main__:saving file to azure blob at /sftp_test/grab_test/\n",
      "INFO:sdsde.azure.filehandling:digitaladls is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading sftp_test_20220201.csv, to to Azure Storage /sftp_test/grab_test/sftp_test_20220201.csv\n",
      "ERROR:sdsde.azure.filehandling:\"Error Message: BlobAlreadyExists\"\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n",
      "INFO:__main__:file to azure load complete\n",
      "INFO:__main__:begining sftp upload/download\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "WARNING:__main__:Table Already Exist will only appened to current table\n",
      "INFO:__main__:Pushing Top Ranked file to Snowflake\n",
      "INFO:sdsde.snowflake.copyinto:copy into MACHINELEARNINGOUTPUTS.DEV.SFTP_TEST_TABLE\n",
      "  from 'azure://dtscadls.blob.core.windows.net/digitaladls/sftp_test/grab_test/sftp_test_20220201.csv'\n",
      "  credentials=(azure_sas_token='**MASKED**')\n",
      "  encryption=(type= 'NONE')\n",
      "  file_format = (type = csv field_delimiter = ',' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' encoding = 'UTF-8' skip_header = 1)\n",
      "  on_error = continue;\n",
      "INFO:sdsde.snowflake.copyinto:testing connection\n",
      "INFO:sdsde.snowflake.copyinto:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.copyinto:executing query\n",
      "INFO:sdsde.snowflake.copyinto:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.copyinto:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.copyinto:snowflake insertion output:\n",
      "INFO:sdsde.snowflake.copyinto:\n",
      "                                                 file  status  rows_parsed  \\\n",
      "0  azure://dtscadls.blob.core.windows.net/vai...  LOADED           10   \n",
      "\n",
      "   rows_loaded  error_limit  errors_seen first_error first_error_line  \\\n",
      "0           10           10            0        None             None   \n",
      "\n",
      "  first_error_character first_error_column_name  \n",
      "0                  None                    None  \n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:__main__:query new table created...\n",
      "                               customerkey      ecid customertitle customername customerfirstname  \\\n",
      "0     50710853  84412913           MR.         None              None   \n",
      "1     61454663  84412913           MR.         None              None   \n",
      "\n",
      "  customermiddlename customerlastname customersuffix customerdateofbirth  \\\n",
      "0               None             None           None          0001-01-01   \n",
      "1               None             None           None          0001-01-01   \n",
      "\n",
      "  gendercode  ... phonenumber  phoneextension isphonecontactable ismailable  \\\n",
      "0          M  ...        None            None                  1          1   \n",
      "1          M  ...        None            None                  1          1   \n",
      "\n",
      "   iscurrent         recordcreatedate         recordupdatedate  \\\n",
      "0          0  2020-01-21 01:07:37.073  2021-02-25 01:17:41.564   \n",
      "1          1  2021-02-25 01:17:41.564  2021-02-25 01:17:41.564   \n",
      "\n",
      "          recordexpiredate isepicmixactivated birthdatekey  \n",
      "0  2020-01-31 01:17:20.141               True     19930314  \n",
      "1                     None               True     19930314  \n",
      "\n",
      "[2 rows x 38 columns]\n",
      "                            \n",
      "INFO:__main__:sftp grab complete\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "test.sftp_grab(download_dir=test.sftp_yaml['sftp_dir_location'],\n",
    "               table_name_sf=test.sftp_yaml['insert_table_name'],\n",
    "               suffix=f\"_{datetime.today().strftime('%Y%m%d')}\",\n",
    "               azure_blob_dest= test.sftp_yaml['blob_dest'],\n",
    "               store=True, create_snowflake_table=True,\n",
    "               varchar=False)\n",
    "# Test here is with in the actual function pull --> azure --> to sf check logs this doesn't need a test again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:14:48.216491Z",
     "start_time": "2022-02-01T23:14:35.459979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:paramiko.transport.sftp:[chan 0] sftp session closed.\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SFTP_TEST_TABLE successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "test.sf.run_str_query('DROP TABLE MACHINELEARNINGOUTPUTS.DEV.SFTP_TEST_TABLE;')\n",
    "\n",
    "test.connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:14:49.448651Z",
     "start_time": "2022-02-01T23:14:48.224024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted 08_yaml_ingestion_binary_classification.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
