{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "technical-trigger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:06.676427Z",
     "start_time": "2022-02-04T00:29:06.673183Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp modeling.premodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "whole-poland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:06.836802Z",
     "start_time": "2022-02-04T00:29:06.832406Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from sdsde import files\n",
    "from sdsde.snowflake.query import SnowflakeConnect\n",
    "from sdsde.wrapper.azurewrapper import blob_pusher\n",
    "from sdsde.wrapper.azurewrapper import blob_puller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-institution",
   "metadata": {},
   "source": [
    "# Pre-modeling Functionality\n",
    "\n",
    "These functions are designed to help with anything in the pre-modeling stage of the ML life cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interesting-removal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:07.138606Z",
     "start_time": "2022-02-04T00:29:07.132883Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-preservation",
   "metadata": {},
   "source": [
    "## Data Lake Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-desperate",
   "metadata": {},
   "source": [
    "### `stage_query_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ordinary-duncan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:07.900663Z",
     "start_time": "2022-02-04T00:29:07.895724Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def stage_query_generator(stage_name, url, sas_token, file_type='parquet'):\n",
    "    \"\"\"generates the snowflake query needed to create an external stage in\n",
    "    azure blob\n",
    "\n",
    "    Args:\n",
    "    * stage_name (str): name of the stage in snowflake\n",
    "    * url (str): azure formated string for account, container, and path\n",
    "    * sas_token (str): blob sas token for shared access\n",
    "    * file_type (str, optional): type of files expected in stage. Defaults to 'parquet'. Can use 'csv' as well.\n",
    "\n",
    "    Returns:\n",
    "    * str: snowflake query to create stage\n",
    "    \"\"\"\n",
    "    stage_template = '''\n",
    "    create or replace stage STAGE_NAME_HERE\n",
    "      url='URL_HERE'\n",
    "      credentials=(azure_sas_token='SAS_TOKEN_HERE')\n",
    "      encryption=(type= 'NONE')\n",
    "      file_format = (type = FILE_TYPE_HERE);\n",
    "    '''\n",
    "    stage_query = stage_template.replace('STAGE_NAME_HERE', stage_name)\n",
    "    stage_query = stage_query.replace('SAS_TOKEN_HERE', sas_token)\n",
    "    stage_query = stage_query.replace('URL_HERE', url)\n",
    "    stage_query = stage_query.replace('FILE_TYPE_HERE', file_type)\n",
    "    return stage_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "convertible-bouquet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:08.274542Z",
     "start_time": "2022-02-04T00:29:08.267792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"stage_query_generator\" class=\"doc_header\"><code>stage_query_generator</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>stage_query_generator</code>(**`stage_name`**, **`url`**, **`sas_token`**, **`file_type`**=*`'parquet'`*)\n",
       "\n",
       "generates the snowflake query needed to create an external stage in\n",
       "azure blob\n",
       "\n",
       "Args:\n",
       "* stage_name (str): name of the stage in snowflake\n",
       "* url (str): azure formated string for account, container, and path\n",
       "* sas_token (str): blob sas token for shared access\n",
       "* file_type (str, optional): type of files expected in stage. Defaults to 'parquet'. Can use 'csv' as well.\n",
       "\n",
       "Returns:\n",
       "* str: snowflake query to create stage"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(stage_query_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "distant-lindsay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:08.414151Z",
     "start_time": "2022-02-04T00:29:08.409337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    create or replace stage STAGE_NAME_HERE\n",
      "      url='SAS_TOKEN_HERE'\n",
      "      credentials=(azure_sas_token='SAS_TOKEN_HERE')\n",
      "      encryption=(type= 'NONE')\n",
      "      file_format = (type = parquet);\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = stage_query_generator('STAGE_NAME_HERE', 'SAS_TOKEN_HERE', 'URL_HERE')\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-multiple",
   "metadata": {},
   "source": [
    "### `make_data_lake_stage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "saved-india",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:09.085118Z",
     "start_time": "2022-02-04T00:29:09.080147Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def make_data_lake_stage(sf_connection,\n",
    "                         stage_name,\n",
    "                         account,\n",
    "                         container,\n",
    "                         data_lake_path,\n",
    "                         sas_token,\n",
    "                         file_type='parquet'):\n",
    "    \"\"\"creates a data lake staging environment from snowflake\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (``SnowflakeConnect``): snowflake connection\n",
    "    * stage_name (str): name of stage in snowflake\n",
    "    * account (str): blob storage account\n",
    "    * container (str): blob container\n",
    "    * data_lake_path (str): path in the container to stage in\n",
    "    * sas_token (str): shared access token for blob\n",
    "    * file_type (str, optional): type of files to stage. Defaults to 'parquet'.\n",
    "    \"\"\"\n",
    "    stage_url = f'azure://{account}.blob.core.windows.net/{container}/{data_lake_path}'\n",
    "    stage_query = stage_query_generator(stage_name, stage_url, sas_token, file_type='parquet')\n",
    "    sf_connection.run_str_query(stage_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "apart-paragraph",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:09.552474Z",
     "start_time": "2022-02-04T00:29:09.542161Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"make_data_lake_stage\" class=\"doc_header\"><code>make_data_lake_stage</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>make_data_lake_stage</code>(**`sf_connection`**, **`stage_name`**, **`account`**, **`container`**, **`data_lake_path`**, **`sas_token`**, **`file_type`**=*`'parquet'`*)\n",
       "\n",
       "creates a data lake staging environment from snowflake\n",
       "\n",
       "Args:\n",
       "* sf_connection (``SnowflakeConnect``): snowflake connection\n",
       "* stage_name (str): name of stage in snowflake\n",
       "* account (str): blob storage account\n",
       "* container (str): blob container\n",
       "* data_lake_path (str): path in the container to stage in\n",
       "* sas_token (str): shared access token for blob\n",
       "* file_type (str, optional): type of files to stage. Defaults to 'parquet'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(make_data_lake_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "illegal-information",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:12.409601Z",
     "start_time": "2022-02-04T00:29:10.003914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Stage area SDSDETEST successfully created.\n"
     ]
    }
   ],
   "source": [
    "sf = SnowflakeConnect(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "\n",
    "make_data_lake_stage(sf_connection=sf, \n",
    "                     stage_name='sdsdetest',\n",
    "                     account=os.environ['azure_account'], \n",
    "                     container='sdsdetesting', \n",
    "                     data_lake_path='sdsde_library/testing', \n",
    "                     sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-capture",
   "metadata": {},
   "source": [
    "## Feature Set Pulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-hotel",
   "metadata": {},
   "source": [
    "### `pull_static_feature_set_to_data_lake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "soviet-conducting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:13.804414Z",
     "start_time": "2022-02-04T00:29:13.797658Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def pull_static_feature_set_to_data_lake(sf_connection: object,\n",
    "                                         stage_name: str,\n",
    "                                         data_lake_path: str,\n",
    "                                         features: list,\n",
    "                                         grain='ECID',\n",
    "                                         limit_statement='',\n",
    "                                         overwrite=True,\n",
    "                                         ):\n",
    "    \"\"\"given a list of features and a modeling gain this pulls data from snowflake to a\n",
    "    data lake in raw file format. data will be in the format specified by the stage.\n",
    "    parquet with snappy is recommended.\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (object): snowflake connection\n",
    "    * stage_name (str): stage name (already created)\n",
    "    * data_lake_path (str): where in the data lake to dump data\n",
    "    * features (list): feature set\n",
    "    * grain (str, optional): gain from feature store for rows. Defaults to 'ECID'.\n",
    "    * limit_statement (str, optional): limit statement to insert to SQL ie \"limit 1000\". Defaults to ''. Used for debugging.\n",
    "    * overwrite (bool, optional): overwrite existing data or not. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    full_stage_path = os.path.join(stage_name, data_lake_path)\n",
    "    select_query = f'''\n",
    "    copy into @{full_stage_path} from\n",
    "    (\n",
    "        select\n",
    "            {grain}\n",
    "            FEATURES_HERE\n",
    "        from \"MACHINELEARNINGFEATURES\".\"PROD\".FEATURESTORE_{grain}\n",
    "        {limit_statement}\n",
    "    )\n",
    "    overwrite = {overwrite}\n",
    "    '''\n",
    "    logger.info(f'Pulling {len(features)} {grain} features to the datalake at path {data_lake_path}')\n",
    "    for feature in features:\n",
    "        select_query = select_query.replace('FEATURES_HERE', f', {feature}FEATURES_HERE')\n",
    "    select_query = select_query.replace('FEATURES_HERE', '')\n",
    "    response = sf_connection.run_str_query(select_query)\n",
    "    logger.info(f'Copy response\\n{response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "complimentary-seller",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:14.553465Z",
     "start_time": "2022-02-04T00:29:14.544237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"pull_static_feature_set_to_data_lake\" class=\"doc_header\"><code>pull_static_feature_set_to_data_lake</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>pull_static_feature_set_to_data_lake</code>(**`sf_connection`**:`object`, **`stage_name`**:`str`, **`data_lake_path`**:`str`, **`features`**:`list`, **`grain`**=*`'ECID'`*, **`limit_statement`**=*`''`*, **`overwrite`**=*`True`*)\n",
       "\n",
       "given a list of features and a modeling gain this pulls data from snowflake to a\n",
       "data lake in raw file format. data will be in the format specified by the stage.\n",
       "parquet with snappy is recommended.\n",
       "\n",
       "Args:\n",
       "* sf_connection (object): snowflake connection\n",
       "* stage_name (str): stage name (already created)\n",
       "* data_lake_path (str): where in the data lake to dump data\n",
       "* features (list): feature set\n",
       "* grain (str, optional): gain from feature store for rows. Defaults to 'ECID'.\n",
       "* limit_statement (str, optional): limit statement to insert to SQL ie \"limit 1000\". Defaults to ''. Used for debugging.\n",
       "* overwrite (bool, optional): overwrite existing data or not. Defaults to True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(pull_static_feature_set_to_data_lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "important-delhi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:17.883987Z",
     "start_time": "2022-02-04T00:29:15.325016Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:__main__:Pulling 2 ECID features to the datalake at path features/static/ecid/\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:__main__:Copy response\n",
      "   rows_unloaded  input_bytes  output_bytes\n",
      "0           1000        10105         10105\n"
     ]
    }
   ],
   "source": [
    "sf = SnowflakeConnect(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "\n",
    "pull_static_feature_set_to_data_lake(sf_connection=sf,\n",
    "                                     stage_name='sdsdetest', \n",
    "                                     data_lake_path='features/static/ecid/',\n",
    "                                     features=['MARKETINGZONE', 'TOTALSEASONSSCANNED'],\n",
    "                                     grain='ECID',\n",
    "                                     limit_statement='limit 1000',\n",
    "                                     overwrite=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-norwegian",
   "metadata": {},
   "source": [
    "### `temporal_and_static_dump_data_to_datalake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "later-combat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:18.871029Z",
     "start_time": "2022-02-04T00:29:18.862140Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def temporal_and_static_dump_data_to_datalake(sf_connection: object,\n",
    "                                              stage_name: str,\n",
    "                                              data_lake_path: str,\n",
    "                                              feature_dict: list,\n",
    "                                              base_query: str,\n",
    "                                              grain='ECID',\n",
    "                                              limit_statement='',\n",
    "                                              overwrite=True):\n",
    "    \"\"\"\n",
    "    Populates stages datalake via a snowflake query.\n",
    "\n",
    "    Args:\n",
    "        sf_connection ([type]): SnowFlake Connection\n",
    "        yaml_file (str, optional): Yaml file name . Defaults to 'dataload.yaml'.\n",
    "        yaml_section (str, optional): Yaml section to read. Defaults to 'inputdata'.\n",
    "        data_set (str, optional): Training or Test Set. Defaults to 'train_set'.\n",
    "        overwrite (bool, optional): Overwrite exisiting file or not. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: query used to created the dump\n",
    "    \"\"\"\n",
    "    full_stage_path = os.path.join(stage_name, data_lake_path)\n",
    "    # copy snowflake data into the stage\n",
    "    select_query = f'''\n",
    "    copy into @{full_stage_path} from\n",
    "    (\n",
    "    SELECT\n",
    "        FEATURES_HERE\n",
    "        TEMPORAL_HERE\n",
    "    FROM\n",
    "    (\n",
    "     {base_query}\n",
    "    ) base\n",
    "    LEFT JOIN \"MACHINELEARNINGFEATURES\".\"PROD\".\"FEATURESTORE_{grain}\" mlf ON base.\"{grain}\" = mlf.{grain}\n",
    "    {limit_statement}\n",
    "    )\n",
    "    OVERWRITE={overwrite}\n",
    "    ;\n",
    "    '''\n",
    "    for ind, feature in enumerate(feature_dict['static_features'].keys()):\n",
    "        if ind == 0:\n",
    "            if feature == 'ECID':\n",
    "                select_query = select_query.replace('FEATURES_HERE', f'base.{feature}FEATURES_HERE')\n",
    "            else:\n",
    "                select_query = select_query.replace('FEATURES_HERE', f'mlf.{feature}FEATURES_HERE')\n",
    "        else:\n",
    "            if feature == 'ECID':\n",
    "                select_query = select_query.replace('FEATURES_HERE', f', base.{feature}FEATURES_HERE')\n",
    "            else:\n",
    "                select_query = select_query.replace('FEATURES_HERE', f', mlf.{feature}FEATURES_HERE')\n",
    "    for feature, values in feature_dict['temporal_features'].items():\n",
    "        select_query = select_query.replace('TEMPORAL_HERE', f', machinelearningfeatures.{os.environ.get(\"prod_or_dev\", \"dev\")}.{feature}({\" , \".join(values[\"args\"])})TEMPORAL_HERE')\n",
    "    select_query = select_query.replace('FEATURES_HERE', '')\n",
    "    select_query = select_query.replace('TEMPORAL_HERE', '')\n",
    "    logging.info(f'query {select_query}')\n",
    "    response = sf_connection.run_str_query(select_query)\n",
    "    logging.info(f'Copy response\\n{response}')\n",
    "    return select_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "angry-metro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:31.471152Z",
     "start_time": "2022-02-04T00:29:19.831370Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Stage area SDSDETEST successfully created.\n",
      "INFO:root:query \n",
      "    copy into @sdsdetest/classification/feature_set/train_set from\n",
      "    (\n",
      "    SELECT\n",
      "        base.ECID, mlf.IsEpicMixActivated\n",
      "        , machinelearningfeatures.dev.AvgResortsPerSeason_ECID_temporal(base.ECID , 20161101 , 20210422)\n",
      "    FROM\n",
      "    (\n",
      "     SELECT ECID FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100\n",
      "    ) base\n",
      "    LEFT JOIN \"MACHINELEARNINGFEATURES\".\"PROD\".\"FEATURESTORE_ECID\" mlf ON base.\"ECID\" = mlf.ECID\n",
      "    LIMIT 10\n",
      "    )\n",
      "    OVERWRITE=True\n",
      "    ;\n",
      "    \n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:root:Copy response\n",
      "   rows_unloaded  input_bytes  output_bytes\n",
      "0             10          754           754\n"
     ]
    }
   ],
   "source": [
    "feature_dict = dict({\n",
    "    'temporal_features': {\n",
    "        'AvgResortsPerSeason_ECID_temporal': \n",
    "          {'args' : ['base.ECID', '20161101', '20210422'],\n",
    "          'variabl_type': 'cont'}, \n",
    "    },\n",
    "    'static_features': {\n",
    "        'ECID': {'variable_type' : 'y'},\n",
    "        'IsEpicMixActivated': {'variable_type' :'cat'}\n",
    "    }\n",
    "})\n",
    "\n",
    "sf = SnowflakeConnect(sfAccount = os.environ.get('sfAccount', None), \n",
    "                   sfUser = os.environ.get('sfUser', None), \n",
    "                   sfPswd = os.environ.get('sfPswd', None), \n",
    "                   sfWarehouse = os.environ.get('sfWarehouse', None),\n",
    "                   sfDatabase = os.environ.get('sfDatabase', None),\n",
    "                   sfSchema = os.environ.get('sfSchema', None), \n",
    "                   sfRole = os.environ.get('sfRole', None))\n",
    "\n",
    "make_data_lake_stage(sf_connection=sf, \n",
    "                     stage_name='sdsdetest',\n",
    "                     account=os.environ['azure_account'], \n",
    "                     container='sdsdetesting', \n",
    "                     data_lake_path='projects/ltr/model-runs/latest', \n",
    "                     sas_token=os.environ.get('DATALAKE_SAS_TOKEN_SECRET', None))\n",
    "\n",
    "base_query = \"\"\"SELECT ECID FROM MACHINELEARNINGOUTPUTS.DEV.DL_LTR LIMIT 100\"\"\"\n",
    "_ = temporal_and_static_dump_data_to_datalake(sf_connection=sf, stage_name='sdsdetest',\n",
    "                                              data_lake_path='classification/feature_set/train_set',\n",
    "                                              feature_dict=feature_dict, base_query=base_query,\n",
    "                                              grain='ECID', overwrite=True, limit_statement='LIMIT 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-canvas",
   "metadata": {},
   "source": [
    "## Feature Set Query from Data Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-specific",
   "metadata": {},
   "source": [
    "### `query_feature_set_from_data_lake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "north-weapon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:35.975519Z",
     "start_time": "2022-02-04T00:29:35.968432Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def query_feature_set_from_data_lake(sf_connection: object,\n",
    "                                     stage_name: str,\n",
    "                                     data_lake_path: str,\n",
    "                                     features: list,\n",
    "                                     limit_statement='',\n",
    "                                     ):\n",
    "    \"\"\"once data resides in the data lake, this will allow you to query the data\n",
    "    into python RAM\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (object): snowflake connection\n",
    "    * stage_name (str): data lake stage in snowflake\n",
    "    * data_lake_path (str): extention path in the data lake\n",
    "    * features (list): list of features\n",
    "    * limit_statement (str, optional): limit statement to insert to SQL ie \"limit 1000\". Defaults to ''. Used for debugging.\n",
    "\n",
    "    Returns:\n",
    "    * [DataFrame]: feature set\n",
    "    \"\"\"\n",
    "\n",
    "    # create query string\n",
    "    query = f'''\n",
    "        select\n",
    "            FEATURES_HERE\n",
    "        from @{os.path.join(stage_name, data_lake_path)}\n",
    "        {limit_statement}\n",
    "    '''\n",
    "    for ind, feature in enumerate(features):\n",
    "        if ind == 0:\n",
    "            query = query.replace('FEATURES_HERE', f'$1:\"_COL_{ind}\" as {feature}FEATURES_HERE')\n",
    "        else:\n",
    "            query = query.replace('FEATURES_HERE', f', $1:\"_COL_{ind}\" as {feature}FEATURES_HERE')\n",
    "    query = query.replace('FEATURES_HERE', '')\n",
    "\n",
    "    # query data\n",
    "    logger.info(f'Querying Feature Set From Data Lake {query}')\n",
    "\n",
    "    df = sf_connection.run_str_query(query)\n",
    "    df.columns = [i.upper() for i in df.columns]\n",
    "    logger.info(f'Final Dataset Shape - {df.shape}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "growing-nelson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:36.410034Z",
     "start_time": "2022-02-04T00:29:36.400245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"query_feature_set_from_data_lake\" class=\"doc_header\"><code>query_feature_set_from_data_lake</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>query_feature_set_from_data_lake</code>(**`sf_connection`**:`object`, **`stage_name`**:`str`, **`data_lake_path`**:`str`, **`features`**:`list`, **`limit_statement`**=*`''`*)\n",
       "\n",
       "once data resides in the data lake, this will allow you to query the data\n",
       "into python RAM\n",
       "\n",
       "Args:\n",
       "* sf_connection (object): snowflake connection\n",
       "* stage_name (str): data lake stage in snowflake\n",
       "* data_lake_path (str): extention path in the data lake\n",
       "* features (list): list of features\n",
       "* limit_statement (str, optional): limit statement to insert to SQL ie \"limit 1000\". Defaults to ''. Used for debugging.\n",
       "\n",
       "Returns:\n",
       "* [DataFrame]: feature set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(query_feature_set_from_data_lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "narrative-effect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:31:57.186089Z",
     "start_time": "2022-02-04T00:31:54.586329Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:__main__:Querying Feature Set From Data Lake \n",
      "        select\n",
      "            $1:\"_COL_0\" as ECID, $1:\"_COL_1\" as MARKETINGZONE, $1:\"_COL_2\" as TOTALSEASONSSCANNED\n",
      "        from @dsdetest/features/static/ecid/\n",
      "        limit 100\n",
      "    \n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:__main__:Final Dataset Shape - (0, 3)\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "sf = SnowflakeConnect(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "\n",
    "df = query_feature_set_from_data_lake(sf_connection=sf, \n",
    "                                      stage_name='sdsdetest', \n",
    "                                      data_lake_path='features/static/ecid/',\n",
    "                                      features=['ECID', 'MARKETINGZONE', 'TOTALSEASONSSCANNED'],\n",
    "                                      limit_statement='limit 100'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-medicare",
   "metadata": {},
   "source": [
    "### ``query_feature_set_from_data_lake_dt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accepted-specification",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:39.536297Z",
     "start_time": "2022-02-04T00:29:39.529225Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def query_feature_set_from_data_lake_dt(sf_connection: object,\n",
    "                                        stage_name: str,\n",
    "                                        data_lake_path: str,\n",
    "                                        features: list,\n",
    "                                        dtypes_list: list,\n",
    "                                        limit_statement='',\n",
    "                                        ):\n",
    "    \"\"\"once data resides in the data lake, this will allow you to query the data\n",
    "    into python RAM\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (object): snowflake connection\n",
    "    * stage_name (str): data lake stage in snowflake\n",
    "    * data_lake_path (str): extention path in the data lake\n",
    "    * features (list): list of features\n",
    "    * limit_statement (str, optional): limit statement to insert to SQL ie \"limit 1000\". Defaults to ''. Used for debugging.\n",
    "\n",
    "    Returns:\n",
    "    * [DataFrame]: feature set\n",
    "    \"\"\"\n",
    "    query = f'''\n",
    "        select\n",
    "            FEATURES_HERE\n",
    "        from @{os.path.join(stage_name, data_lake_path)}\n",
    "        {limit_statement}\n",
    "    '''\n",
    "    for ind, feature in enumerate(zip(features, dtypes_list)):\n",
    "        if ind == 0:\n",
    "            query = query.replace('FEATURES_HERE', f'$1:\"_COL_{ind}\"::{feature[1].upper()} as {feature[0]}FEATURES_HERE')\n",
    "        else:\n",
    "            query = query.replace('FEATURES_HERE', f', $1:\"_COL_{ind}\"::{feature[1].upper()} as {feature[0]}FEATURES_HERE')\n",
    "    query = query.replace('FEATURES_HERE', '')\n",
    "\n",
    "    logging.info(f'Querying Feature Set From Data Lake {query}')\n",
    "    df = sf_connection.run_str_query(query)\n",
    "    df.columns = [i.upper() for i in df.columns]\n",
    "    logging.info(f'Final Dataset Shape - {df.shape}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-accreditation",
   "metadata": {},
   "source": [
    "### `query_pushed_parquet_table_data_lake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "established-thanksgiving",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:39.571460Z",
     "start_time": "2022-02-04T00:29:39.538397Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def query_pushed_parquet_table_data_lake(sf_connection: object,\n",
    "                                         stage_name: str,\n",
    "                                         data_lake_path: str,\n",
    "                                         feature_dict: dict,\n",
    "                                         limit_statement='',\n",
    "                                         pattern='.*parquet'):\n",
    "    \"\"\"\n",
    "    Function is used when there is a parquet table in azure datalake\n",
    "    that need to be brought into memory for exploration\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (object): Snowflake Engine\n",
    "    * stage_name (str): Azure stage name\n",
    "    * data_lake_path (str): Data Lake path\n",
    "    * feature_dict (dict): feature dictionary\n",
    "    * limit_statement (str, optional): limit statment. Defaults to ''.\n",
    "    * pattern (str, optional): pattern to read partitions. Defaults to '.*parquet'.\n",
    "\n",
    "    Returns:\n",
    "    * pd.DataFrame: Data Frame\n",
    "    \"\"\"\n",
    "    if not stage_name.endswith('/'):\n",
    "        stage_name += '/'\n",
    "    if data_lake_path.startswith('/'):\n",
    "        logging.error('data_lake_path should not start with / please remove and re-run')\n",
    "        sys.exit()\n",
    "    query = f'''\n",
    "        select\n",
    "            FEATURES_HERE\n",
    "        from @{os.path.join(stage_name, data_lake_path)} (pattern=>'{pattern}')\n",
    "        {limit_statement}\n",
    "    '''\n",
    "    features = feature_dict.keys()\n",
    "    for ind, feature in enumerate(features):\n",
    "        if ind == 0:\n",
    "            query = query.replace('FEATURES_HERE', f'$1:\"{feature.lower()}\" as {feature}FEATURES_HERE')\n",
    "        else:\n",
    "            query = query.replace('FEATURES_HERE', f', $1:\"{feature.lower()}\" as {feature}FEATURES_HERE')\n",
    "    query = query.replace('FEATURES_HERE', '')\n",
    "    logger.info(f'Querying Feature Set From Data Lake {query}')\n",
    "    df = sf_connection.run_str_query(query)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    logger.info('fixing dtypes')\n",
    "    for k, v in feature_dict.items():\n",
    "        if v['variable_type'] == 'cont':\n",
    "            df[k] = df[k].astype('float')\n",
    "    logger.info(f'Final Dataset Shape - {df.shape}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-raise",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "parallel-demonstration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:29:44.154578Z",
     "start_time": "2022-02-04T00:29:43.347196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2a203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
