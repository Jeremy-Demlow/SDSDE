{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:08.444116Z",
     "start_time": "2022-02-04T00:16:08.437609Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp snowflake.copyinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:09.520320Z",
     "start_time": "2022-02-04T00:16:08.592220Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:10.140236Z",
     "start_time": "2022-02-04T00:16:09.522698Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.basics import *\n",
    "from sdsde.snowflake.query import SnowflakeConnect\n",
    "from sdsde import files\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"azure.core\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"snowflake.connector\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``CopyInto``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:10.156298Z",
     "start_time": "2022-02-04T00:16:10.142275Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class CopyInto(SnowflakeConnect):\n",
    "\n",
    "    def __init__(self, sfAccount: str, sfUser: str,\n",
    "                 sfPswd: str, sfWarehouse: str, sfDatabase: str,\n",
    "                 sfSchema: str, sfRole: str, logger=None):\n",
    "        \"\"\"\n",
    "        Instatiation of snowflake mover class inheriting the Snowflake_\n",
    "        class from utils.\n",
    "        Args:\n",
    "        * sfUser (str, optional): snowflake credential passed as string\n",
    "        * sfPswd (str, optional): snowflake credential passed as string\n",
    "        * sfWarehouse (str, optional): snowflake credential passed as string\n",
    "        * sfDatabase (str, optional): snowflake credential passed as string\n",
    "        * sfSchema (str, optional): snowflake credential passed as string\n",
    "        * sfRole (str, optional): snowflake credential passed as string\n",
    "        * logger ([type], optional): pass custom logger as many libraries are set to Warning. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__(sfAccount,\n",
    "                         sfUser,\n",
    "                         sfPswd,\n",
    "                         sfWarehouse,\n",
    "                         sfDatabase,\n",
    "                         sfSchema,\n",
    "                         sfRole)\n",
    "        store_attr()\n",
    "        self._logger = logger if logger is not None else logging.getLogger(__name__)\n",
    "\n",
    "    def insert_csv(self,\n",
    "                   blob_name: str,\n",
    "                   blob_path: str,\n",
    "                   storage_account: str,\n",
    "                   container_name: str,\n",
    "                   table_name: str,\n",
    "                   sas_token: str,\n",
    "                   fail_on_no_insert: bool = False,\n",
    "                   delimiter: str = ',',\n",
    "                   encoding: str = 'UTF-8',\n",
    "                   escape_unenclosed: str = None,\n",
    "                   ):\n",
    "        \"\"\"Insert CSV function Below is how to use it\n",
    "\n",
    "        ```python\n",
    "        ci = CopyInto(sfAccount=os.environ['sfAccount'],\n",
    "              sfUser=os.environ['sfUser'],\n",
    "              sfPswd=os.environ['sfPswd'],\n",
    "              sfWarehouse=os.environ['sfWarehouse'],\n",
    "              sfDatabase=os.environ['sfDatabase'],\n",
    "              sfSchema=os.environ['sfSchema'],\n",
    "              sfRole=os.environ['sfRole'])\n",
    "        assert ci.test_connection() == None, 'anything else the connection has failed'\n",
    "        response  = ci.insert_csv(blob_name=file[0],\n",
    "                                blob_path='copyinto/',\n",
    "                                storage_account='os.environ['azure_account']azure_account']',\n",
    "                                container_name='sdsdetesting',\n",
    "                                table_name='test_insert_csv_sdsde',\n",
    "                                sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                                fail_on_no_insert=False,\n",
    "                                delimiter=',')\n",
    "        ```\n",
    "\n",
    "        Args:\n",
    "        * blob_name (str, optional): file name in blob.\n",
    "        * blob_path (str, optional): azure blob path.\n",
    "        * storage_account (str, optional): azure storage account.\n",
    "        * container_name (str, optional): azure container name.\n",
    "        * table_name (str, optional): snowflake table name to create.\n",
    "        * sas_token (str, optional): sas token for azure data lake.\n",
    "        * fail_on_no_insert (bool, optional): if True then it will allow failure. Defaults to False.\n",
    "        * delimiter (str, optional): file delimiter. Defaults to ','.\n",
    "        * encoding (str, optional): fild encoding. Defaults to 'UTF-8'.\n",
    "        * escape_unenclosed (str, optional): special file types like mta. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        * pd.DataFrame: Response from snowflake copy into statement\n",
    "        \"\"\"\n",
    "\n",
    "        # make blob name here if a path is given\n",
    "        if blob_path:\n",
    "            blob_path = blob_path if not blob_path.endswith('/') else blob_path[:-1]\n",
    "            blob_name = blob_path + '/' + blob_name\n",
    "\n",
    "        # read the sql file from the libary\n",
    "        inserts = ['INSERT_AZURE_STORAGE_ACCOUNT_NAME_HERE',\n",
    "                   'INSERT_TABLE_NAME_HERE',\n",
    "                   'INSERT_CONTAINER_NAME_HERE',\n",
    "                   'INSERT_FILE_NAME_HERE',\n",
    "                   'INSERT_AZURE_SAS_TOKEN_HERE',\n",
    "                   'INSERT_DELIMITER_HERE',\n",
    "                   'INSERT_ENCODING_HERE']\n",
    "        insert = [storage_account, table_name, container_name, blob_name,\n",
    "                  sas_token, delimiter, encoding]\n",
    "        if escape_unenclosed == 'None':\n",
    "            insert.append(escape_unenclosed)\n",
    "            inserts.append('INSERT_ESCAPE_UNENCLOSED_FIELD_HERE')\n",
    "            sql_file = os.path.join(os.path.abspath(files.__path__[0]), 'import_data_unenclosed.sql')\n",
    "        else:\n",
    "            sql_file = os.path.join(os.path.abspath(files.__path__[0]), 'import_data_csv.sql')\n",
    "        with open(sql_file) as file:\n",
    "            query = ' '.join(file.readlines())\n",
    "        for k, v in zip(inserts, insert):\n",
    "            query = query.replace(k, v)\n",
    "        self._logger.info(query.replace(sas_token, '**MASKED**'))\n",
    "        # execute the snowflake command\n",
    "        response = self.run_str_query(query)\n",
    "\n",
    "        # output query results\n",
    "        self._logger.info('snowflake insertion output:')\n",
    "        self._logger.info(f'\\n {response}')\n",
    "\n",
    "        if response.loc[0].status == 'Copy executed with 0 files processed.':\n",
    "            self._logger.info('No files uploaded to snowflake')\n",
    "            if fail_on_no_insert:\n",
    "                raise('fail_on_no_insert was equal TRUE, so program raised error')\n",
    "        else:\n",
    "            # check if the load was executed correctly\n",
    "            assert (response['rows_parsed'][0] == response['rows_loaded'][0]), \\\n",
    "                \"Rows loaded and parsed are not equal\"\n",
    "\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:10.180851Z",
     "start_time": "2022-02-04T00:16:10.158624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CopyInto\" class=\"doc_header\"><code>class</code> <code>CopyInto</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CopyInto</code>(**`sfAccount`**:`str`, **`sfUser`**:`str`, **`sfPswd`**:`str`, **`sfWarehouse`**:`str`, **`sfDatabase`**:`str`, **`sfSchema`**:`str`, **`sfRole`**:`str`, **`logger`**=*`None`*) :: [`SnowflakeConnect`](/sdsdesnowflake_query.html#SnowflakeConnect)\n",
       "\n",
       "Class that holds basic snowflake functionality including testing connection\n",
       "and running queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CopyInto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:10.199539Z",
     "start_time": "2022-02-04T00:16:10.182586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CopyInto.insert_csv\" class=\"doc_header\"><code>CopyInto.insert_csv</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CopyInto.insert_csv</code>(**`blob_name`**:`str`, **`blob_path`**:`str`, **`storage_account`**:`str`, **`container_name`**:`str`, **`table_name`**:`str`, **`sas_token`**:`str`, **`fail_on_no_insert`**:`bool`=*`False`*, **`delimiter`**:`str`=*`','`*, **`encoding`**:`str`=*`'UTF-8'`*, **`escape_unenclosed`**:`str`=*`None`*)\n",
       "\n",
       "Insert CSV function Below is how to use it\n",
       "\n",
       "```python\n",
       "ci = CopyInto(sfAccount=os.environ['sfAccount'],\n",
       "      sfUser=os.environ['sfUser'],\n",
       "      sfPswd=os.environ['sfPswd'],\n",
       "      sfWarehouse=os.environ['sfWarehouse'],\n",
       "      sfDatabase=os.environ['sfDatabase'],\n",
       "      sfSchema=os.environ['sfSchema'],\n",
       "      sfRole=os.environ['sfRole'])\n",
       "assert ci.test_connection() == None, 'anything else the connection has failed'\n",
       "response  = ci.insert_csv(blob_name=file[0],\n",
       "                        blob_path='copyinto/',\n",
       "                        storage_account='os.environ['azure_account']azure_account']',\n",
       "                        container_name='sdsdetesting',\n",
       "                        table_name='test_insert_csv_sdsde',\n",
       "                        sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
       "                        fail_on_no_insert=False,\n",
       "                        delimiter=',')\n",
       "```\n",
       "\n",
       "Args:\n",
       "* blob_name (str, optional): file name in blob.\n",
       "* blob_path (str, optional): azure blob path.\n",
       "* storage_account (str, optional): azure storage account.\n",
       "* container_name (str, optional): azure container name.\n",
       "* table_name (str, optional): snowflake table name to create.\n",
       "* sas_token (str, optional): sas token for azure data lake.\n",
       "* fail_on_no_insert (bool, optional): if True then it will allow failure. Defaults to False.\n",
       "* delimiter (str, optional): file delimiter. Defaults to ','.\n",
       "* encoding (str, optional): fild encoding. Defaults to 'UTF-8'.\n",
       "* escape_unenclosed (str, optional): special file types like mta. Defaults to None.\n",
       "\n",
       "Returns:\n",
       "* pd.DataFrame: Response from snowflake copy into statement"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CopyInto.insert_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``clean_special_chars``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:10.405299Z",
     "start_time": "2022-02-04T00:16:10.400785Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_special_chars(text):\n",
    "        \"\"\"\n",
    "        small nlp clean up tool to take odd characters that could be\n",
    "        in vendor data inside of column names and then replaces empty\n",
    "        spaces with ``_``\n",
    "\n",
    "        Args:\n",
    "            text (str): dataframe column names as strings\n",
    "\n",
    "        Returns:\n",
    "            str: clean column name\n",
    "        \"\"\"\n",
    "        punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'  # noqa:\n",
    "        punct += '©^®` <→°€™› ♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'  # noqa:\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "            text = text.replace(' ', '_')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:10.830284Z",
     "start_time": "2022-02-04T00:16:10.822967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"clean_special_chars\" class=\"doc_header\"><code>clean_special_chars</code><a href=\"__main__.py#L1\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>clean_special_chars</code>(**`text`**)\n",
       "\n",
       "small nlp clean up tool to take odd characters that could be\n",
       "in vendor data inside of column names and then replaces empty\n",
       "spaces with ``_``\n",
       "\n",
       "Args:\n",
       "    text (str): dataframe column names as strings\n",
       "\n",
       "Returns:\n",
       "    str: clean column name"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(clean_special_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``create_sf_table_from_df``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:11.227782Z",
     "start_time": "2022-02-04T00:16:11.221705Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sf_table_from_df(sf, df: pd.DataFrame, table_name_sf: str, varchar: bool):\n",
    "        \"\"\"\n",
    "        Dynamically create a table from a dataframe and\n",
    "        change the dtypes to snowflake dytpes this may have\n",
    "        a limitation, but can be added.\n",
    "\n",
    "        Args:\n",
    "        * df (pd.DataFrame): data frame to get dtypes\n",
    "        * table_name_sf (str): snowflake table name\n",
    "        * varchar: (bool, optional): this will default all dytpes to varchars if True.\n",
    "        \"\"\"\n",
    "        select_query = f'''\n",
    "            create or replace table {table_name_sf} (FEATURES_HERE);\n",
    "            '''\n",
    "        for k, v in dict(df.dtypes).items():\n",
    "            select_query = select_query.replace('FEATURES_HERE', f'{k} {return_sf_type(str(v), varchar=varchar)}, FEATURES_HERE')\n",
    "        select_query = select_query.replace(', FEATURES_HERE', '')\n",
    "        logging.info(select_query)\n",
    "        sf.run_str_query(select_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:11.654570Z",
     "start_time": "2022-02-04T00:16:11.646728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"create_sf_table_from_df\" class=\"doc_header\"><code>create_sf_table_from_df</code><a href=\"__main__.py#L1\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>create_sf_table_from_df</code>(**`sf`**, **`df`**:`DataFrame`, **`table_name_sf`**:`str`, **`varchar`**:`bool`)\n",
       "\n",
       "Dynamically create a table from a dataframe and\n",
       "change the dtypes to snowflake dytpes this may have\n",
       "a limitation, but can be added.\n",
       "\n",
       "Args:\n",
       "* df (pd.DataFrame): data frame to get dtypes\n",
       "* table_name_sf (str): snowflake table name\n",
       "* varchar: (bool, optional): this will default all dytpes to varchars if True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(create_sf_table_from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ``return_sf_type``\n",
    "(Adding For Test Eventually Will Move To sdsde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:12.497290Z",
     "start_time": "2022-02-04T00:16:12.491221Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_sf_type(dtype: str, varchar: bool):\n",
    "        \"\"\"\n",
    "        simple function to convert dytpes to snowflake dtypes this\n",
    "        will be come a very useful thing to have as this will dtype\n",
    "        data from 10,000 rows from a dataframe, and will fail out\n",
    "        on the push to snowflake. This means the vendor or the current\n",
    "        SFTP needs to have a cleaning added to it to allow for data\n",
    "        to make it through the pipeline. TODO: Figure out how to accepet\n",
    "        a ETL clean of the data from a vendor that can be placed in place\n",
    "\n",
    "        Args:\n",
    "        * dtype (str): dtype from a df in sting form\n",
    "        * varchar (bool): to default all variables to VARCHAR\n",
    "        this happens due to bad vendor data and can't be resloved\n",
    "        with out reading in the whole data set with low_memory=False\n",
    "\n",
    "        Returns:\n",
    "        * str: snowflake dtype\n",
    "        \"\"\"\n",
    "        if varchar is True:\n",
    "            dtype = 'VARCHAR'\n",
    "        elif 'int' in dtype.lower():\n",
    "            dtype = 'NUMBER'\n",
    "        elif 'float' in dtype.lower():\n",
    "            dtype = 'FLOAT'\n",
    "        elif 'object' in dtype.lower():\n",
    "            dtype = 'VARCHAR'\n",
    "        elif 'bool' in dtype.lower():\n",
    "            dtype = 'VARCHAR'  # TODO: Limitation found before change once resloved by sf\n",
    "        elif 'date' in dtype.lower():\n",
    "            dtype = 'DATETIME'  # TODO: Might break with certain datetimes most generic\n",
    "        else:\n",
    "            logging.error('odd dtype not seen needs to be resloved...')\n",
    "            sys.exit()\n",
    "        return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:12.734189Z",
     "start_time": "2022-02-04T00:16:12.726741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"return_sf_type\" class=\"doc_header\"><code>return_sf_type</code><a href=\"__main__.py#L1\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>return_sf_type</code>(**`dtype`**:`str`, **`varchar`**:`bool`)\n",
       "\n",
       "simple function to convert dytpes to snowflake dtypes this\n",
       "will be come a very useful thing to have as this will dtype\n",
       "data from 10,000 rows from a dataframe, and will fail out\n",
       "on the push to snowflake. This means the vendor or the current\n",
       "SFTP needs to have a cleaning added to it to allow for data\n",
       "to make it through the pipeline. TODO: Figure out how to accepet\n",
       "a ETL clean of the data from a vendor that can be placed in place\n",
       "\n",
       "Args:\n",
       "* dtype (str): dtype from a df in sting form\n",
       "* varchar (bool): to default all variables to VARCHAR\n",
       "this happens due to bad vendor data and can't be resloved\n",
       "with out reading in the whole data set with low_memory=False\n",
       "\n",
       "Returns:\n",
       "* str: snowflake dtype"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(return_sf_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:13.712641Z",
     "start_time": "2022-02-04T00:16:13.701742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CopyInto\" class=\"doc_header\"><code>class</code> <code>CopyInto</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CopyInto</code>(**`sfAccount`**:`str`, **`sfUser`**:`str`, **`sfPswd`**:`str`, **`sfWarehouse`**:`str`, **`sfDatabase`**:`str`, **`sfSchema`**:`str`, **`sfRole`**:`str`, **`logger`**=*`None`*) :: [`SnowflakeConnect`](/sdsdesnowflake_query.html#SnowflakeConnect)\n",
       "\n",
       "Class that holds basic snowflake functionality including testing connection\n",
       "and running queries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CopyInto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:13.935377Z",
     "start_time": "2022-02-04T00:16:13.919368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CopyInto.insert_csv\" class=\"doc_header\"><code>CopyInto.insert_csv</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CopyInto.insert_csv</code>(**`blob_name`**:`str`, **`blob_path`**:`str`, **`storage_account`**:`str`, **`container_name`**:`str`, **`table_name`**:`str`, **`sas_token`**:`str`, **`fail_on_no_insert`**:`bool`=*`False`*, **`delimiter`**:`str`=*`','`*, **`encoding`**:`str`=*`'UTF-8'`*, **`escape_unenclosed`**:`str`=*`None`*)\n",
       "\n",
       "Insert CSV function Below is how to use it\n",
       "\n",
       "```python\n",
       "ci = CopyInto(sfAccount=os.environ['sfAccount'],\n",
       "      sfUser=os.environ['sfUser'],\n",
       "      sfPswd=os.environ['sfPswd'],\n",
       "      sfWarehouse=os.environ['sfWarehouse'],\n",
       "      sfDatabase=os.environ['sfDatabase'],\n",
       "      sfSchema=os.environ['sfSchema'],\n",
       "      sfRole=os.environ['sfRole'])\n",
       "assert ci.test_connection() == None, 'anything else the connection has failed'\n",
       "response  = ci.insert_csv(blob_name=file[0],\n",
       "                        blob_path='copyinto/',\n",
       "                        storage_account='os.environ['azure_account']azure_account']',\n",
       "                        container_name='sdsdetesting',\n",
       "                        table_name='test_insert_csv_sdsde',\n",
       "                        sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
       "                        fail_on_no_insert=False,\n",
       "                        delimiter=',')\n",
       "```\n",
       "\n",
       "Args:\n",
       "* blob_name (str, optional): file name in blob.\n",
       "* blob_path (str, optional): azure blob path.\n",
       "* storage_account (str, optional): azure storage account.\n",
       "* container_name (str, optional): azure container name.\n",
       "* table_name (str, optional): snowflake table name to create.\n",
       "* sas_token (str, optional): sas token for azure data lake.\n",
       "* fail_on_no_insert (bool, optional): if True then it will allow failure. Defaults to False.\n",
       "* delimiter (str, optional): file delimiter. Defaults to ','.\n",
       "* encoding (str, optional): fild encoding. Defaults to 'UTF-8'.\n",
       "* escape_unenclosed (str, optional): special file types like mta. Defaults to None.\n",
       "\n",
       "Returns:\n",
       "* pd.DataFrame: Response from snowflake copy into statement"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CopyInto.insert_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unenclosed Example for MTA for example there are certain files that will lead to issues just another example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:59:28.543483Z",
     "start_time": "2022-02-01T22:58:19.942539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Container was deleted remaking example file...\n",
      "INFO:sdsde.azure.filehandling:nielsen/dev/TMW/Daily/Raw/_Resorts_TM_Warehouse_Feed_286019_20210830.csv to ./_Resorts_TM_Warehouse_Feed_286019_20210830.csv\n",
      "INFO:sdsde.azure.filehandling:sdsdetesting is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading test_nielson_copy_into.csv, to to Azure Storage copyinto/test_nielson_copy_into.csv\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n",
      "INFO:__main__:Pulling Copy Into File To Create Temp Snowflake Table for Test\n",
      "INFO:sdsde.azure.filehandling:copyinto/test_nielson_copy_into.csv to ./test_nielson_copy_into.csv\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:connection to snowflake successful\n",
      "INFO:root:\n",
      "            create or replace table test_insert_csv_sdsde (Date VARCHAR, CONV_TYPE VARCHAR, Advertiser_Id VARCHAR, Buy_Id VARCHAR, Site_Id VARCHAR, Page_Id VARCHAR, Creative_Id VARCHAR, Ad_Id VARCHAR, Channel VARCHAR, Campaign VARCHAR, Publisher VARCHAR, AdGroup VARCHAR, Placement_keyword VARCHAR, Creative_Name VARCHAR, Match_Type VARCHAR, Brand VARCHAR, Product_Affinity VARCHAR, Audience_Segment_1 VARCHAR, Funnel_Stage VARCHAR, Audience_Segment_3 VARCHAR, LOB VARCHAR, Messaging_Pillar VARCHAR, Campaign_Tactic VARCHAR, Campaign_Category VARCHAR, Region VARCHAR, Impressions VARCHAR, Clicks VARCHAR, Cost VARCHAR, Conversions VARCHAR, True_Conversions VARCHAR, Revenue VARCHAR, True_Revenue VARCHAR, Units VARCHAR, True_Units VARCHAR);\n",
      "            \n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Table TEST_INSERT_CSV_SDSDE successfully created.\n",
      "INFO:__main__:copy into test_insert_csv_sdsde\n",
      "  from 'azure://dtscadls.blob.core.windows.net/sdsdetesting/copyinto/test_nielson_copy_into.csv'\n",
      "  credentials=(azure_sas_token='**MASKED**')\n",
      "  encryption=(type= 'NONE')\n",
      "  file_format = (type = csv field_delimiter = ',' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' ESCAPE_UNENCLOSED_FIELD = None encoding = 'UTF-8' skip_header = 1)\n",
      "  on_error = continue;\n",
      "\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:snowflake insertion output:\n",
      "INFO:__main__:\n",
      "                                                 file  status  rows_parsed  \\\n",
      "0  azure://dtscadls.blob.core.windows.net/sds...  LOADED        10000   \n",
      "\n",
      "   rows_loaded  error_limit  errors_seen first_error first_error_line  \\\n",
      "0        10000        10000            0        None             None   \n",
      "\n",
      "  first_error_character first_error_column_name  \n",
      "0                  None                    None  \n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:TEST_INSERT_CSV_SDSDE successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "from sdsde.azure.filehandling import FileHandling\n",
    "from sdsde.wrapper.azurewrapper import blob_pusher, blob_puller, unlink_files\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "fh = FileHandling(os.environ['connection_str'])\n",
    "if fh.ls_blob(container_name='dsdetesting', path='copyinto')[0] in ['test_nielson_copy_into.csv']:\n",
    "    logger.info('file already exists....')\n",
    "else:\n",
    "    logger.error('Container was deleted remaking example file...')\n",
    "    container_name = 'digitaladls'\n",
    "    file = ['nielsen/dev/TMW/Daily/Raw/_Resorts_TM_Warehouse_Feed_286019_20210830.csv']\n",
    "    blob_puller(files=file,\n",
    "                connection_str=os.environ['connection_str'],\n",
    "                container_name=container_name,\n",
    "                drop_location='.',\n",
    "                overwrite=True)\n",
    "    assert os.path.exists(file[0].split('/')[-1]) == True, 'blob puller failed to grab data must have been deleted from dev change to prod'\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('_Resorts_TM_Warehouse_Feed_286019_20210830.csv')\n",
    "    df = df[:10000]\n",
    "    df.head()\n",
    "    df.to_csv('test_nielson_copy_into.csv', index=False)\n",
    "    file = ['test_nielson_copy_into.csv']\n",
    "    container_name = 'sdsdetesting'\n",
    "    blob_pusher(file_path=file,\n",
    "                container_name=container_name,\n",
    "                blob_dest=['copyinto'],\n",
    "                connection_str=os.environ['connection_str'],\n",
    "                overwrite=True)\n",
    "    unlink_files([file[0].split('/')[-1]])\n",
    "logger.info('Pulling Copy Into File To Create Temp Snowflake Table for Test')\n",
    "blob_puller(files=['copyinto/test_nielson_copy_into.csv'],\n",
    "            connection_str=os.environ['connection_str'],\n",
    "            container_name='sdsdetesting',\n",
    "            drop_location='.',\n",
    "            overwrite=True)\n",
    "ci = CopyInto(sfAccount=os.environ['sfAccount'],\n",
    "              sfUser=os.environ['sfUser'],\n",
    "              sfPswd=os.environ['sfPswd'],\n",
    "              sfWarehouse=os.environ['sfWarehouse'],\n",
    "              sfDatabase=os.environ['sfDatabase'],\n",
    "              sfSchema=os.environ['sfSchema'],\n",
    "              sfRole=os.environ['sfRole'])\n",
    "assert ci.test_connection() == None, 'anything else the connection has failed'\n",
    "df = pd.read_csv('test_nielson_copy_into.csv', nrows=150, low_memory=True)\n",
    "df.columns = [clean_special_chars(x) for x in df.columns]\n",
    "create_sf_table_from_df(ci, df, 'test_insert_csv_sdsde', True) \n",
    "unlink_files(['test_nielson_copy_into.csv'])\n",
    "connect_str = os.environ['connection_str']\n",
    "response  = ci.insert_csv(blob_name='test_nielson_copy_into.csv', \n",
    "                          blob_path='copyinto/',\n",
    "                          storage_account=os.environ['azure_account'],\n",
    "                          container_name='sdsdetesting',\n",
    "                          table_name='test_insert_csv_sdsde', \n",
    "                          sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                          fail_on_no_insert=False,\n",
    "                          delimiter=',',\n",
    "                          escape_unenclosed = 'None')\n",
    "assert response.rows_loaded[0] == 10000, 'Insert function failed to insert the 10,000 observations'\n",
    "df =  ci.run_str_query(\"SELECT TOP 10 * FROM test_insert_csv_sdsde;\")\n",
    "assert df.shape == (10, 34), 'Table Did not load properly go to 04_snowflake_copyinto'\n",
    "ci.run_str_query('DROP TABLE test_insert_csv_sdsde;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular File Being Tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T23:13:02.327343Z",
     "start_time": "2022-02-01T23:12:46.037817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:root:\n",
      "            create or replace table test_insert_csv_sdsde (ecid VARCHAR, likelihoodtoreturnrate VARCHAR, likelihoodtoreturnlabel VARCHAR, fiscalyear VARCHAR, modeltype VARCHAR, experiment VARCHAR, modelname VARCHAR, uploadtime VARCHAR);\n",
      "            \n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:Table TEST_INSERT_CSV_SDSDE successfully created.\n",
      "INFO:sdsde.azure.filehandling:sdsdetesting is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading regular_file.csv, to to Azure Storage copyinto/regular_file.csv\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:connection to snowflake successful\n",
      "INFO:__main__:copy into test_insert_csv_sdsde\n",
      "  from 'azure://dtscadls.blob.core.windows.net/sdsdetesting/copyinto/regular_file.csv'\n",
      "  credentials=(azure_sas_token='**MASKED**')\n",
      "  encryption=(type= 'NONE')\n",
      "  file_format = (type = csv field_delimiter = ',' FIELD_OPTIONALLY_ENCLOSED_BY = '\"' encoding = 'UTF-8' skip_header = 1)\n",
      "  on_error = continue;\n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:snowflake insertion output:\n",
      "INFO:__main__:\n",
      "                                                 file  status  rows_parsed  \\\n",
      "0  azure://dtscadls.blob.core.windows.net/sds...  LOADED         1000   \n",
      "\n",
      "   rows_loaded  error_limit  errors_seen first_error first_error_line  \\\n",
      "0         1000         1000            0        None             None   \n",
      "\n",
      "  first_error_character first_error_column_name  \n",
      "0                  None                    None  \n",
      "INFO:__main__:testing connection\n",
      "INFO:__main__:sqlalchemy snowflake engine created\n",
      "INFO:__main__:executing query\n",
      "INFO:__main__:data loaded from snowflake\n",
      "INFO:__main__:connection to snowflake has been turned off\n",
      "INFO:__main__:TEST_INSERT_CSV_SDSDE successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "#skip\n",
    "df = ci.run_str_query(\"SELECT * FROM machinelearningoutputs.dev.dl_ltr Limit 1000;\")\n",
    "df.columns = [clean_special_chars(x) for x in df.columns]\n",
    "create_sf_table_from_df(ci, df, 'test_insert_csv_sdsde', True)\n",
    "df.to_csv('regular_file.csv', index=False)\n",
    "file = ['regular_file.csv']\n",
    "container_name = 'sdsdetesting'\n",
    "blob_pusher(file_path=file,\n",
    "        container_name=container_name,\n",
    "        blob_dest=['copyinto'],\n",
    "        connection_str=os.environ['connection_str'],\n",
    "        overwrite=True)\n",
    "ci = CopyInto(sfAccount=os.environ['sfAccount'],\n",
    "              sfUser=os.environ['sfUser'],\n",
    "              sfPswd=os.environ['sfPswd'],\n",
    "              sfWarehouse=os.environ['sfWarehouse'],\n",
    "              sfDatabase=os.environ['sfDatabase'],\n",
    "              sfSchema=os.environ['sfSchema'],\n",
    "              sfRole=os.environ['sfRole'])\n",
    "assert ci.test_connection() == None, 'anything else the connection has failed'\n",
    "response  = ci.insert_csv(blob_name=file[0], \n",
    "                          blob_path='copyinto/',\n",
    "                          storage_account=os.environ['azure_account'],\n",
    "                          container_name='sdsdetesting',\n",
    "                          table_name='test_insert_csv_sdsde', \n",
    "                          sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],\n",
    "                          fail_on_no_insert=False,\n",
    "                          delimiter=',')\n",
    "assert response.rows_loaded[0] == 1000, 'Insert function failed to insert the 10,000 observations'\n",
    "ci.run_str_query('DROP TABLE test_insert_csv_sdsde;')\n",
    "unlink_files(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:16:30.963818Z",
     "start_time": "2022-02-04T00:16:30.108249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
