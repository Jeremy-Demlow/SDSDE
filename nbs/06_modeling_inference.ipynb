{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "technical-trigger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:43.596332Z",
     "start_time": "2022-02-04T00:18:43.591553Z"
    }
   },
   "outputs": [],
   "source": [
    "#default_exp modeling.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "whole-poland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:45.561417Z",
     "start_time": "2022-02-04T00:18:43.628373Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from sklearn import datasets\n",
    "from sdsde.azure.filehandling import FileHandling\n",
    "from sdsde.snowflake.query import SnowflakeConnect\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-institution",
   "metadata": {},
   "source": [
    "# Inference Functionality\n",
    "\n",
    "These functions are designed to help with anything in the Inference stage of the ML life cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charitable-certification",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:45.583072Z",
     "start_time": "2022-02-04T00:18:45.563758Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pickle\n",
    "import pyarrow\n",
    "import shutil\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from sdsde.wrapper.azurewrapper import blob_pusher, blob_puller\n",
    "from sdsde.modeling.premodel import make_data_lake_stage\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-surprise",
   "metadata": {},
   "source": [
    "## Model Pulling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-texture",
   "metadata": {},
   "source": [
    "### `pull_sklearn_object_from_data_lake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "positive-flooring",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:45.608372Z",
     "start_time": "2022-02-04T00:18:45.584876Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def pull_sklearn_object_from_data_lake(file_name: str, path: str, container: str, connection_str: str):\n",
    "    \"\"\"pulls a pickeld sklearn object from azure data lake to memory\n",
    "\n",
    "    Args:\n",
    "    * file_name (str): name of file\n",
    "    * path (str): data lake path\n",
    "    * container (str): data lake container\n",
    "    * connection_str (str): azure connection string for the account\n",
    "\n",
    "    Returns:\n",
    "    * (sklearn object): sklearn object loaded from azure\n",
    "    \"\"\"\n",
    "    logger.info(f'Loading Sklearn Object: {os.path.join(path, file_name)}')\n",
    "    blob_puller(files=[os.path.join(path, file_name)],\n",
    "                connection_str=connection_str,\n",
    "                container_name=container,\n",
    "                drop_location='.',\n",
    "                overwrite=True)\n",
    "    with open(file_name, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    os.unlink(file_name)\n",
    "    logger.info('Sklearn Object Loaded')\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "current-chase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:45.629142Z",
     "start_time": "2022-02-04T00:18:45.610829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"pull_sklearn_object_from_data_lake\" class=\"doc_header\"><code>pull_sklearn_object_from_data_lake</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>pull_sklearn_object_from_data_lake</code>(**`file_name`**:`str`, **`path`**:`str`, **`container`**:`str`, **`connection_str`**:`str`)\n",
       "\n",
       "pulls a pickeld sklearn object from azure data lake to memory\n",
       "\n",
       "Args:\n",
       "* file_name (str): name of file\n",
       "* path (str): data lake path\n",
       "* container (str): data lake container\n",
       "* connection_str (str): azure connection string for the account\n",
       "\n",
       "Returns:\n",
       "* (sklearn object): sklearn object loaded from azure"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(pull_sklearn_object_from_data_lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "knowing-strength",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:46.733152Z",
     "start_time": "2022-02-04T00:18:46.531060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Sklearn Object: dsde_library/testing/models/RandomForestExample.pickle\n",
      "INFO:sdsde.azure.filehandling:dsde_library/testing/models/RandomForestExample.pickle to ./RandomForestExample.pickle\n",
      "/home/azureuser/miniconda3/envs/dsde_upgrade/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.0.2 when using version 1.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/azureuser/miniconda3/envs/dsde_upgrade/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 1.0.2 when using version 1.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "INFO:__main__:Sklearn Object Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pull_sklearn_object_from_data_lake(file_name='RandomForestExample.pickle',\n",
    "                                           path='dsde_library/testing/models/', \n",
    "                                           container='dsdetesting',\n",
    "                                           connection_str=os.environ['DATALAKE_CONN_STR_SECRET'],\n",
    "                                          )\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-layer",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-aspect",
   "metadata": {},
   "source": [
    "### `push_dataframe_to_data_lake_as_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unique-apache",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:49.046900Z",
     "start_time": "2022-02-04T00:18:49.036364Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def push_dataframe_to_data_lake_as_parquet(df, path, container, connection_str,\n",
    "                                           partition_cols: list = [\"partitionidx\"], overwrite=True):\n",
    "    \"\"\"takes a pandas dataframe and writes it to azure via pyarrow with parquet files\n",
    "\n",
    "    Args:\n",
    "    * df (pd.DataFame): dataframe\n",
    "    * path (str): data lake path\n",
    "    * container (str): data lake container\n",
    "    * connection_str (str): azure connection string\n",
    "    * partition_cols (list, optional): how to partition. fake partitions for speed make on default. Defaults to [\"partitionidx\"].\n",
    "    * overwrite (bool, optional): do you overwrite what is there now. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        logger.info(f'Removing existing files to write a new batch from {path}')\n",
    "\n",
    "    if partition_cols[0] == \"partitionidx\":\n",
    "        n_partition = int(np.ceil(df.shape[0] / 50000))\n",
    "        df[\"partitionidx\"] = np.random.choice(range(n_partition), size=df.shape[0])\n",
    "        logger.info(f'Partitioning column created for distribution with {n_partition} partitions')\n",
    "\n",
    "    table = pyarrow.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_to_dataset(table, root_path=path, partition_cols=partition_cols)\n",
    "    logger.info('Parquet file staged in local disk memory')\n",
    "\n",
    "    all_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(path) for f in filenames if os.path.splitext(f)[1] == '.parquet']\n",
    "    for file_name in all_files:\n",
    "        logger.info(f'Moving File: {file_name}')\n",
    "        blob_pusher(container_name=container,\n",
    "                    connection_str=connection_str,\n",
    "                    file_path=[file_name],\n",
    "                    blob_dest=[os.path.dirname(file_name)],\n",
    "                    overwrite=overwrite)\n",
    "\n",
    "    shutil.rmtree(path)\n",
    "    logger.info('Local parquet files removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reserved-recipe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:49.445425Z",
     "start_time": "2022-02-04T00:18:49.436974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"push_dataframe_to_data_lake_as_parquet\" class=\"doc_header\"><code>push_dataframe_to_data_lake_as_parquet</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>push_dataframe_to_data_lake_as_parquet</code>(**`df`**, **`path`**, **`container`**, **`connection_str`**, **`partition_cols`**:`list`=*`['partitionidx']`*, **`overwrite`**=*`True`*)\n",
       "\n",
       "takes a pandas dataframe and writes it to azure via pyarrow with parquet files\n",
       "\n",
       "Args:\n",
       "* df (pd.DataFame): dataframe\n",
       "* path (str): data lake path\n",
       "* container (str): data lake container\n",
       "* connection_str (str): azure connection string\n",
       "* partition_cols (list, optional): how to partition. fake partitions for speed make on default. Defaults to [\"partitionidx\"].\n",
       "* overwrite (bool, optional): do you overwrite what is there now. Defaults to True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(push_dataframe_to_data_lake_as_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "otherwise-warren",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:50.123748Z",
     "start_time": "2022-02-04T00:18:49.812267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/envs/dsde_upgrade/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "INFO:__main__:Partitioning column created for distribution with 1 partitions\n",
      "INFO:__main__:Parquet file staged in local disk memory\n",
      "INFO:__main__:Moving File: dsde_library/testing/parquet/pyarrowpush/5481230/partitionidx=0/0005b7def3624fd6aba8d9f44b6a9af6.parquet\n",
      "INFO:sdsde.azure.filehandling:dsdetesting is a valid\n",
      "INFO:sdsde.azure.filehandling:ContainerAlreadyExists\n",
      "INFO:sdsde.azure.filehandling:Uploading dsde_library/testing/parquet/pyarrowpush/5481230/partitionidx=0/0005b7def3624fd6aba8d9f44b6a9af6.parquet, to to Azure Storage dsde_library/testing/parquet/pyarrowpush/5481230/partitionidx=0/0005b7def3624fd6aba8d9f44b6a9af6.parquet\n",
      "INFO:sdsde.azure.filehandling:Azure Upload Complete\n",
      "INFO:__main__:Local parquet files removed\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_boston()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "\n",
    "time = np.random.randint(0,100000000)\n",
    "\n",
    "push_dataframe_to_data_lake_as_parquet(df=df,\n",
    "                                       path=f'dsde_library/testing/parquet/pyarrowpush/{time}', \n",
    "                                       container='dsdetesting', \n",
    "                                       connection_str=os.environ['DATALAKE_CONN_STR_SECRET'],\n",
    "                                       partition_cols=['partitionidx']\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-suite",
   "metadata": {},
   "source": [
    "### `move_parquet_table_to_snowflake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "departmental-laundry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:50.740503Z",
     "start_time": "2022-02-04T00:18:50.733352Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def move_parquet_table_to_snowflake(sf_connection, table_name: str, stage_name: str,\n",
    "                                    path: dict, columns_and_types: dict,\n",
    "                                    pattern: str, replace_table: bool = True):\n",
    "    \"\"\"moves data sitting in a parquet format in ADLS to a snowflake table\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (SnowflakeConnect): snowflake connection\n",
    "    * table_name (str): table name\n",
    "    * stage_name (str): snowflake stage name\n",
    "    * path (str): path in ADLS to parquet data\n",
    "    * columns_and_types (dict): snowflake column namees and types\n",
    "    * pattern (str): pattern for reading files from ADLS\n",
    "    * replace_table (bool, optional): true does create or relace, false does insert. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    if replace_table is False:\n",
    "        select_query = f'''\n",
    "        insert into {table_name}\n",
    "            select\n",
    "                FEATURES_HERE\n",
    "            from @{stage_name + path} (pattern=>'{pattern}')\n",
    "        '''\n",
    "    else:\n",
    "        select_query = f'''\n",
    "        create or replace table {table_name} as\n",
    "            select\n",
    "                FEATURES_HERE\n",
    "            from @{stage_name + path} (pattern=>'{pattern}')\n",
    "        '''\n",
    "    for k, v in columns_and_types.items():\n",
    "        select_query = select_query.replace('FEATURES_HERE', f'$1:\"{k}\"::{v.upper()} as {k}, FEATURES_HERE')\n",
    "    select_query = select_query.replace(', FEATURES_HERE', '')\n",
    "    logger.info(select_query)\n",
    "    sf_connection.run_str_query(select_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "featured-watch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:18:50.992572Z",
     "start_time": "2022-02-04T00:18:50.981607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"move_parquet_table_to_snowflake\" class=\"doc_header\"><code>move_parquet_table_to_snowflake</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>move_parquet_table_to_snowflake</code>(**`sf_connection`**, **`table_name`**:`str`, **`stage_name`**:`str`, **`path`**:`dict`, **`columns_and_types`**:`dict`, **`pattern`**:`str`, **`replace_table`**:`bool`=*`True`*)\n",
       "\n",
       "moves data sitting in a parquet format in ADLS to a snowflake table\n",
       "\n",
       "Args:\n",
       "* sf_connection (SnowflakeConnect): snowflake connection\n",
       "* table_name (str): table name\n",
       "* stage_name (str): snowflake stage name\n",
       "* path (str): path in ADLS to parquet data\n",
       "* columns_and_types (dict): snowflake column namees and types\n",
       "* pattern (str): pattern for reading files from ADLS\n",
       "* replace_table (bool, optional): true does create or relace, false does insert. Defaults to True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(move_parquet_table_to_snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dress-romantic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:00.309352Z",
     "start_time": "2022-02-04T00:20:57.806586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:connection to snowflake successful\n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Stage area SDSDETEST successfully created.\n"
     ]
    }
   ],
   "source": [
    "sf = SnowflakeConnect(sfAccount=os.environ['sfAccount'],\n",
    "                   sfUser=os.environ['sfUser'],\n",
    "                   sfPswd=os.environ['sfPswd'],\n",
    "                   sfWarehouse=os.environ['sfWarehouse'],\n",
    "                   sfDatabase=os.environ['sfDatabase'],\n",
    "                   sfSchema=os.environ['sfSchema'],\n",
    "                   sfRole=os.environ['sfRole'])\n",
    "\n",
    "make_data_lake_stage(sf_connection=sf, \n",
    "                     stage_name='sdsdetest',\n",
    "                     account=os.environ['azure_account'], \n",
    "                     container='sdsdetesting', \n",
    "                     data_lake_path='sdsde_library/testing', \n",
    "                     sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "julian-punch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:04.682075Z",
     "start_time": "2022-02-04T00:21:00.311739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "        create or replace table sdsdelibparquettest as\n",
      "            select\n",
      "                $1:\"ZN\"::NUMBER as ZN, $1:\"INDUS\"::VARCHAR as INDUS\n",
      "            from @sdsdetest/parquet/pyarrowpush/5481230/ (pattern=>'.*.parquet')\n",
      "        \n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:Table SDSDELIBPARQUETTEST successfully created.\n",
      "INFO:__main__:\n",
      "        insert into sdsdelibparquettest\n",
      "            select\n",
      "                $1:\"ZN\"::NUMBER as ZN, $1:\"INDUS\"::VARCHAR as INDUS\n",
      "            from @sdsdetest/parquet/pyarrowpush/5481230/ (pattern=>'.*.parquet')\n",
      "        \n",
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n"
     ]
    }
   ],
   "source": [
    "cols = {'ZN': 'number', 'INDUS': 'varchar'}\n",
    "move_parquet_table_to_snowflake(sf_connection=sf, \n",
    "                                table_name='sdsdelibparquettest', \n",
    "                                stage_name='sdsdetest',\n",
    "                                path=f'/parquet/pyarrowpush/{time}/',\n",
    "                                columns_and_types=cols, \n",
    "                                pattern='.*.parquet',\n",
    "                                replace_table = True)\n",
    "move_parquet_table_to_snowflake(sf_connection=sf,\n",
    "                                table_name='sdsdelibparquettest', \n",
    "                                stage_name='sdsdetest',\n",
    "                                path=f'/parquet/pyarrowpush/{time}/',\n",
    "                                columns_and_types=cols, \n",
    "                                pattern='.*.parquet',\n",
    "                                replace_table = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1adb68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:06.126283Z",
     "start_time": "2022-02-04T00:21:04.684028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDELIBPARQUETTEST successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "sf.run_str_query('DROP TABLE sdsdelibparquettest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42694d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:07.637582Z",
     "start_time": "2022-02-04T00:21:06.128809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sdsde.snowflake.query:testing connection\n",
      "INFO:sdsde.snowflake.query:sqlalchemy snowflake engine created\n",
      "INFO:sdsde.snowflake.query:executing query\n",
      "INFO:sdsde.snowflake.query:data loaded from snowflake\n",
      "INFO:sdsde.snowflake.query:connection to snowflake has been turned off\n",
      "INFO:sdsde.snowflake.query:SDSDETEST successfully dropped.\n"
     ]
    }
   ],
   "source": [
    "sf.run_str_query('DROP STAGE sdsdetest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-prison",
   "metadata": {},
   "source": [
    "### `query_and_push_feature_set_to_data_lake`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-underwear",
   "metadata": {},
   "source": [
    "When a feature set isn't in the feature store there will be times where this happens and we will want to be able to use very similar mechanics to what all other projects that are in the feature store have. This is where this method will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "located-collectible",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:07.646555Z",
     "start_time": "2022-02-04T00:21:07.640195Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def query_and_push_feature_set_to_data_lake(sf_connection: object, query_file_path: str,\n",
    "                                            stage_name: str, account: str,\n",
    "                                            container: str, data_lake_path: str,\n",
    "                                            blob_path: str, sas_token: str,\n",
    "                                            connection_str: str, overwrite=True):\n",
    "    \"\"\"\n",
    "    Take a in RAM data set and parition out data set into parquet files\n",
    "    that are then sent to Azure Data Lake. This assumes that the feature\n",
    "    store isn't being used for this project. The use case for this is\n",
    "    to save training/test and predictions to azure that will then be\n",
    "    sent to snowflake.\n",
    "\n",
    "    Args:\n",
    "    * sf_connection (SnowFlake Engine): Snowflake connection\n",
    "    * query_file_path (str): Path to file to execute\n",
    "    * stage_name (str): Stage Name for snowflake\n",
    "    * account (str): Azure blob account name\n",
    "    * container (str): Container in blob account\n",
    "    * data_lake_path (str): root level for stage name allowing for re-use\n",
    "    * blob_path (str): path in container to store data\n",
    "    * sas_token (str): SAS token found in Access Keys\n",
    "    * connection_str (str): connection str to azure blob found in Access Keys in Azure\n",
    "    * overwrite (bool, optional): Overwrite files. Defaults to True.\n",
    "    \"\"\"\n",
    "    logger.info('creating datalake staging area')\n",
    "    make_data_lake_stage(sf_connection=sf_connection,\n",
    "                         stage_name=stage_name,\n",
    "                         account=account,\n",
    "                         container=container,\n",
    "                         data_lake_path=data_lake_path,\n",
    "                         sas_token=sas_token)\n",
    "    logger.info('begin query....')\n",
    "    df = sf_connection.execute_file(query_file_path)\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    logger.info(f'query complete files being written to {data_lake_path}')\n",
    "    push_dataframe_to_data_lake_as_parquet(df=df,\n",
    "                                           container=container,\n",
    "                                           path=blob_path,\n",
    "                                           connection_str=connection_str,\n",
    "                                           overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "golden-mandate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:07.683319Z",
     "start_time": "2022-02-04T00:21:07.648734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"query_and_push_feature_set_to_data_lake\" class=\"doc_header\"><code>query_and_push_feature_set_to_data_lake</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>query_and_push_feature_set_to_data_lake</code>(**`sf_connection`**:`object`, **`query_file_path`**:`str`, **`stage_name`**:`str`, **`account`**:`str`, **`container`**:`str`, **`data_lake_path`**:`str`, **`blob_path`**:`str`, **`sas_token`**:`str`, **`connection_str`**:`str`, **`overwrite`**=*`True`*)\n",
       "\n",
       "Take a in RAM data set and parition out data set into parquet files\n",
       "that are then sent to Azure Data Lake. This assumes that the feature\n",
       "store isn't being used for this project. The use case for this is\n",
       "to save training/test and predictions to azure that will then be\n",
       "sent to snowflake.\n",
       "\n",
       "Args:\n",
       "* sf_connection (SnowFlake Engine): Snowflake connection\n",
       "* query_file_path (str): Path to file to execute\n",
       "* stage_name (str): Stage Name for snowflake\n",
       "* account (str): Azure blob account name\n",
       "* container (str): Container in blob account\n",
       "* data_lake_path (str): root level for stage name allowing for re-use\n",
       "* blob_path (str): path in container to store data\n",
       "* sas_token (str): SAS token found in Access Keys\n",
       "* connection_str (str): connection str to azure blob found in Access Keys in Azure\n",
       "* overwrite (bool, optional): Overwrite files. Defaults to True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(query_and_push_feature_set_to_data_lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-kentucky",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "educated-despite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T00:21:09.632308Z",
     "start_time": "2022-02-04T00:21:08.835465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_azure.ipynb.\n",
      "Converted 02_utils_dataframes.ipynb.\n",
      "Converted 02_utils_parseyaml.ipynb.\n",
      "Converted 02_utils_stfp.ipynb.\n",
      "Converted 02_utils_traininghelpers.ipynb.\n",
      "Converted 02_utils_traininghelpers_fastai.ipynb.\n",
      "Converted 03_dstools_preparedata.ipynb.\n",
      "Converted 04_snowflake_copyinto.ipynb.\n",
      "Converted 04_snowflake_copyinto2.ipynb.\n",
      "Converted 04_snowflake_query.ipynb.\n",
      "Converted 05_azure_wrappers.ipynb.\n",
      "Converted 06_modeling_inference.ipynb.\n",
      "Converted 06_modeling_inference_fastai.ipynb.\n",
      "Converted 06_modeling_premodel.ipynb.\n",
      "Converted 06_modeling_preprocessing.ipynb.\n",
      "Converted 06_modeling_preprocessing_fastai.ipynb.\n",
      "Converted 06_modeling_training.ipynb.\n",
      "Converted 06_modeling_training_fastai.ipynb.\n",
      "Converted 07_Binary_Classification_Fastai_Example_Notebook.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83fa78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
