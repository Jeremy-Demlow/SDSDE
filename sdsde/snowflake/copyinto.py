# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/04_snowflake_copyinto.ipynb (unless otherwise specified).

__all__ = ['logger', 'CopyInto']

# Cell
from fastcore.basics import *
from .query import SnowflakeConnect
from sdsde import files

import logging
import os

logging.basicConfig(level=logging.INFO)
logging.getLogger("azure.core").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.CRITICAL)
logging.getLogger("snowflake.connector").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# Cell


class CopyInto(SnowflakeConnect):

    def __init__(self, sfAccount: str, sfUser: str,
                 sfPswd: str, sfWarehouse: str, sfDatabase: str,
                 sfSchema: str, sfRole: str, logger=None):
        """
        Instatiation of snowflake mover class inheriting the Snowflake_
        class from utils.
        Args:
        * sfUser (str, optional): snowflake credential passed as string
        * sfPswd (str, optional): snowflake credential passed as string
        * sfWarehouse (str, optional): snowflake credential passed as string
        * sfDatabase (str, optional): snowflake credential passed as string
        * sfSchema (str, optional): snowflake credential passed as string
        * sfRole (str, optional): snowflake credential passed as string
        * logger ([type], optional): pass custom logger as many libraries are set to Warning. Defaults to None.
        """
        super().__init__(sfAccount,
                         sfUser,
                         sfPswd,
                         sfWarehouse,
                         sfDatabase,
                         sfSchema,
                         sfRole)
        store_attr()
        self._logger = logger if logger is not None else logging.getLogger(__name__)

    def insert_csv(self,
                   blob_name: str,
                   blob_path: str,
                   storage_account: str,
                   container_name: str,
                   table_name: str,
                   sas_token: str,
                   fail_on_no_insert: bool = False,
                   delimiter: str = ',',
                   encoding: str = 'UTF-8',
                   escape_unenclosed: str = None,
                   ):
        """Insert CSV function Below is how to use it

        ```python
        ci = CopyInto(sfAccount=os.environ['sfAccount'],
              sfUser=os.environ['sfUser'],
              sfPswd=os.environ['sfPswd'],
              sfWarehouse=os.environ['sfWarehouse'],
              sfDatabase=os.environ['sfDatabase'],
              sfSchema=os.environ['sfSchema'],
              sfRole=os.environ['sfRole'])
        assert ci.test_connection() == None, 'anything else the connection has failed'
        response  = ci.insert_csv(blob_name=file[0],
                                blob_path='copyinto/',
                                storage_account='os.environ['azure_account']azure_account']',
                                container_name='sdsdetesting',
                                table_name='test_insert_csv_sdsde',
                                sas_token=os.environ['DATALAKE_SAS_TOKEN_SECRET'],
                                fail_on_no_insert=False,
                                delimiter=',')
        ```

        Args:
        * blob_name (str, optional): file name in blob.
        * blob_path (str, optional): azure blob path.
        * storage_account (str, optional): azure storage account.
        * container_name (str, optional): azure container name.
        * table_name (str, optional): snowflake table name to create.
        * sas_token (str, optional): sas token for azure data lake.
        * fail_on_no_insert (bool, optional): if True then it will allow failure. Defaults to False.
        * delimiter (str, optional): file delimiter. Defaults to ','.
        * encoding (str, optional): fild encoding. Defaults to 'UTF-8'.
        * escape_unenclosed (str, optional): special file types like mta. Defaults to None.

        Returns:
        * pd.DataFrame: Response from snowflake copy into statement
        """

        # make blob name here if a path is given
        if blob_path:
            blob_path = blob_path if not blob_path.endswith('/') else blob_path[:-1]
            blob_name = blob_path + '/' + blob_name

        # read the sql file from the libary
        inserts = ['INSERT_AZURE_STORAGE_ACCOUNT_NAME_HERE',
                   'INSERT_TABLE_NAME_HERE',
                   'INSERT_CONTAINER_NAME_HERE',
                   'INSERT_FILE_NAME_HERE',
                   'INSERT_AZURE_SAS_TOKEN_HERE',
                   'INSERT_DELIMITER_HERE',
                   'INSERT_ENCODING_HERE']
        insert = [storage_account, table_name, container_name, blob_name,
                  sas_token, delimiter, encoding]
        if escape_unenclosed == 'None':
            insert.append(escape_unenclosed)
            inserts.append('INSERT_ESCAPE_UNENCLOSED_FIELD_HERE')
            sql_file = os.path.join(os.path.abspath(files.__path__[0]), 'import_data_unenclosed.sql')
        else:
            sql_file = os.path.join(os.path.abspath(files.__path__[0]), 'import_data_csv.sql')
        with open(sql_file) as file:
            query = ' '.join(file.readlines())
        for k, v in zip(inserts, insert):
            query = query.replace(k, v)
        self._logger.info(query.replace(sas_token, '**MASKED**'))
        # execute the snowflake command
        response = self.run_str_query(query)

        # output query results
        self._logger.info('snowflake insertion output:')
        self._logger.info(f'\n {response}')

        if response.loc[0].status == 'Copy executed with 0 files processed.':
            self._logger.info('No files uploaded to snowflake')
            if fail_on_no_insert:
                raise('fail_on_no_insert was equal TRUE, so program raised error')
        else:
            # check if the load was executed correctly
            assert (response['rows_parsed'][0] == response['rows_loaded'][0]), \
                "Rows loaded and parsed are not equal"

            return response