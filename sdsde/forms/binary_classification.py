# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/08_yaml_ingestion_binary_classification.ipynb (unless otherwise specified).

__all__ = ['logger', 'yaml_parse', 'write_yaml_file', 'snowflake_query', 'binary_classification_experiment_table',
           'pull_load_timestamp', 'cat_cont_vars_save', 'pre_model_stage', 'model_training_dl', 'query_test_set',
           'predict_binary_fastai_learner', 'push_prediction_file_to_snowflake', 'pull_dl_train_from_data_lake',
           'bc_predict_hyperopt']

# Cell

from ..utils.parseyaml import ParseYaml
from ..snowflake.query import SnowflakeConnect
from pathlib import Path
from fastai.tabular.core import torch
from ..modeling.inferencefastai import pull_fastai_learner_from_data_lake, pull_transform_predict_sklearn
from ..modeling.inference import push_dataframe_to_data_lake_as_parquet, move_parquet_table_to_snowflake
from ..modeling.preprocessingfastai import generate_fastai_pytorch_dataloader, save_fastai_preprocess_to_data_lake, \
    save_dataloader_to_data_lake
from ..modeling.premodel import temporal_and_static_dump_data_to_datalake, query_feature_set_from_data_lake, \
    query_feature_set_from_data_lake_dt
from ..modeling.trainingfastai import train_fastai_tabular_model, save_fastai_model_to_data_lake
from ..azure.filehandling import FileHandling
from ..utils.traininghelpersfastai import change_dl_to_pandas_df
from ..utils.traininghelpers import binary_classification_reporter_sklearn
from ..wrapper.azurewrapper import blob_puller, blob_pusher
from fastai.tabular.all import *
from sklearn.metrics import auc, roc_curve, accuracy_score

import yaml
import os
import pandas as pd
import numpy as np
import logging
import fastai.tabular.core as core_tabular
import fastai.metrics as fast_metrics

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cell


def yaml_parse(file_name_path: str, yaml_section: str = None) -> dict:
    """
    Adding functionality to ParseYaml from sdsde lib creates a
    simple call to pull parts of the yaml file for each function

    Args:
    * file_name (str): Yaml File Name
    * yaml_section (str): Yaml section to read

    Returns:
        dict: yaml section to parse
    """
    if yaml_section is not None:
        yaml = ParseYaml(file_name_path).get_yaml([
            os.environ.get('prod_or_dev', 'dev'), yaml_section])
    else:
        yaml = ParseYaml(file_name_path).get_yaml([os.environ.get('prod_or_dev', 'dev')])
    return yaml


def write_yaml_file(file_path: str, file_name: str, dictionary: dict):
    with open(Path(file_path, file_name), 'w') as f:
        yaml.dump(dictionary, f)


def snowflake_query(sfAccount: str = os.environ.get('sfAccount', None),
                    sfUser: str = os.environ.get('sfUser', None),
                    sfPswd: str = os.environ.get('sfPswd', None),
                    sfWarehouse: str = os.environ.get('sfWarehouse', None),
                    sfDatabase: str = os.environ.get('sfDatabase', None),
                    sfSchema: str = os.environ.get('sfSchema', None),
                    sfRole: str = os.environ.get('sfRole', None)):
    """Easy Connection To SnowFlake When Environs are set"""
    sf = SnowflakeConnect(sfAccount, sfUser, sfPswd, sfWarehouse,
                       sfDatabase, sfSchema, sfRole)
    return sf


def binary_classification_experiment_table(snowflake_connection, df, time_stamp,
                                           table_name: str, container_name: str, connection_str: str,
                                           file_path: str, file_name: str):
    """
    Creates a Binary Classification Experiment table for a project by taking in a
    pandas dataframe and sending it to snowflake.

    Args:
        snowflake_connection:
        df (pd.DataFrame): table data to be sent to snowflake
        time_stamp ([type]): time stamp of the time this experiment was started.
        table_name (str): table name
        container_name (str): azure container name to save dataframe
        connection_str (str): azure datalake connection string
        file_path (str): path to file to save
        file_name (str): file namee.
    """
    schema_check = snowflake_connection.run_str_query("show schemas;")
    if snowflake_connection.sfSchema.upper() in list(schema_check.name):
        logger.info(f"{snowflake_connection.sfSchema} already exisit will not need to make a new schema")
    else:
        snowflake_connection.run_str_query(f"create or replace schema {snowflake_connection.sfSchema};")
    snowflake_connection.infer_to_snowflake(df=df,
                                            table_name=table_name.lower(),
                                            if_exists='append')
    os.makedirs(file_path, exist_ok=True)
    time_stamp_file_path = os.path.join(file_path, file_name)
    np.save(time_stamp_file_path, time_stamp)
    blob_pusher(container_name=container_name,
                connection_str=connection_str,
                file_path=[time_stamp_file_path],
                blob_dest=[file_path],
                overwrite=True)


def pull_load_timestamp(snowflake_connection, sfDatabase: str, sfSchema: str, table_name: str,
                        time_stamp_name: str, file_loc: str, container_name: str,
                        connection_str: str, pull_azure: bool = True):
    """
    TODO: Make Sure to work through this logic
    Function To Pull Time Stamp to Be Used In CI/CD or Local Run To Help Name experiment
    """
    fh = FileHandling(connection_str)
    fh.create_blob_container(container_name=container_name)
    if os.environ.get('CI_COMMIT_REF_SLUG') == 'master' and pull_azure is False:
        time_stamp = snowflake_connection.run_str_query(f"""SELECT timestamp FROM
                                                           {sfDatabase}.{sfSchema}.{table_name}
                                                           WHERE Production_Model = 'Yes';""")
        if len(time_stamp) == 0:
            logger.info('Currently there are not any production models for this project')
            sys.exit()
        else:
            time_stamp = datetime.strptime(time_stamp.timestamp[0], "%Y-%m-%d %H:%M:%S.%f")
    else:
        blob_puller(files=[os.path.join(file_loc, time_stamp_name)],
                    connection_str=connection_str,
                    container_name=container_name,
                    drop_location=file_loc,
                    overwrite=True)
        time_stamp = np.load(os.path.join(file_loc, time_stamp_name))
        time_stamp = datetime.strptime(str(time_stamp), "%Y-%m-%d %H:%M:%S.%f")
    return time_stamp

# Cell


def cat_cont_vars_save(feature_dict: dict, env: str = os.environ.get('prod_or_dev', 'dev')):
    """
    This function will parse the temporal and static features of a yaml file or dictionary with the first
    parse being for PROD or DEV typically in DEV you will have the training set and prod the test set.
    This function could be very specific so refer to the nbs for examples of how it is used.

    Args:
    * feature_dict (dict): the feature dictionary that needs to be parsed
    * env (str, optional): prod or dev. Defaults to os.environ.get('prod_or_dev', 'dev').

    Returns:
        list: cont_vars, cat_vars, y_var, feature_list, dtypes_list
    """
    cont_vars_temp = [k.split('_ECID')[0].upper() for k, v in feature_dict[env]['temporal_features'].items() if v['variable_type'] == 'cont']
    cat_vars_temp = [k.split('_ECID')[0].upper() for k, v in feature_dict[env]['temporal_features'].items() if v['variable_type'] == 'cat']
    y_var_temp = [k.split('_ECID')[0].upper() for k, v in feature_dict[env]['temporal_features'].items() if v['variable_type'] == 'y']
    cont_vars_static = [k.upper() for k, v in feature_dict[env]['static_features'].items() if v['variable_type'] == 'cont']
    cat_vars_static = [k.upper() for k, v in feature_dict[env]['static_features'].items() if v['variable_type'] == 'cat']
    y_var_static = [k.upper() for k, v in feature_dict[env]['static_features'].items() if v['variable_type'] == 'y']
    dtypes_temp = [v['dtype'] for k, v in feature_dict[env]['temporal_features'].items() if v['dtype'] is not None]
    dtypes_static = [v['dtype'] for k, v in feature_dict[env]['static_features'].items() if v['dtype'] is not None]
    static_features = [k.upper() for k in feature_dict[env]['static_features'].keys()]
    temporal_features = [k.split("_")[0].upper() for k in feature_dict[env]['temporal_features'].keys()]
    dtypes_list = dtypes_static + dtypes_temp
    logger.info(f'data types for project {dtypes_list}')
    cont_vars = cont_vars_static + cont_vars_temp
    logger.info(f'continous variables for project {cont_vars}')
    cat_vars = cat_vars_static + cat_vars_temp
    logger.info(f'categorical variables for project {cat_vars}')
    y_var = y_var_static + y_var_temp
    logger.info(f'dependent variable for project {y_var}')
    feature_list = static_features + temporal_features
    logger.info(f'feature list for project {feature_list}')
    return cont_vars, cat_vars, y_var, feature_list, dtypes_list


def pre_model_stage(snowflake_connection, data_lake_path: str, feature_dict: dict,
                    grain: str, stage_name: str, procs: list, base_query: str,
                    cat_vars: list, cont_vars: list, y_var: list, blob_path: str,
                    y_block: str, y_range: float, val_pct: float, bs: int, dl_name,
                    preprocessor_path: str, preprocessor_name: str, seed: int,
                    container_name: str, connection_str: str, limit_statement: str,
                    overwrite: bool, sas_token: str, feature_list: list,
                    splits: list = None):
    """
    This function will (Note: This can be done with out this section this was an attempt to make it one call):
    This function does a lot feel free to not use this function and look at 07_nbs for examples of how to do
    each step seperately
    1. Will query and dump data set of interest to data lake with ``temporal_and_static_dump_data_to_datalake``
    2. Query dumped datat from data lake ``query_feature_set_from_data_lake
    3. Fix issue with binary classifcation problems fixing data types
    4. Generate DataLoad, save dataloader and send it to the data lake

    Args:
    * snowflake_connection (snowflake connection): sf connection
    * data_lake_path (str): datalake short path
    * feature_dict (dict): feature dictionary
    * grain (str): ID like ECID or Resort
    * stage_name (str): Datalake stage name
    * procs (list): pre_processing needed for data set if needed
    * base_query (str): query to insert into the query to the feature store
    * cat_vars (list): list of categorical variables
    * cont_vars (list): list of continous variable
    * y_var (list): dependent variable
    * blob_path (str): blob destintation location path
    * y_block (str): type of problem for Fastai
    * y_range (float): Regression only not needed
    * val_pct (float): validation percentaage
    * bs (int): batch size
    * dl_name ([type]): data loaders name
    * preprocessor_path (str): drop location for preprocessoor
    * preprocessor_name (str): name of preprocessor
    * seed (int): seed number for split
    * container_name (str): azure blob container name
    * connection_str (str): connection string to azure blob
    * limit_statement (str): limit statement used for rapid testing
    * overwrite (bool): Overwrite files
    * sas_token (str): Sas token for azure blob
    * feature_list (list): list of feature names used
    * splits (list, optional): give custom splits if desired. Defaults to None.
    """
    fh = FileHandling(connection_str)
    fh.create_blob_container(container_name=container_name)
    logger.info('begining to query temporal_and_static_dump_data_to_datalake...')
    _ = temporal_and_static_dump_data_to_datalake(sf_connection=snowflake_connection, stage_name=stage_name,
                                                  data_lake_path=data_lake_path, feature_dict=feature_dict,
                                                  base_query=base_query, grain=grain,
                                                  limit_statement=limit_statement, overwrite=overwrite)
    logger.info('query dump complete querying datalake')
    df = query_feature_set_from_data_lake(sf_connection=snowflake_connection, stage_name=stage_name,
                                          data_lake_path=data_lake_path, features=feature_list,
                                          limit_statement=limit_statement)
    logger.info(f'preview data {df.head(3)}')
    # SOMETHING ABOUT THE FEATURE STORE SUCKS WITH DTYPES MIGHT NEED TO ::INT AND ::STR, ::FLOAT TO ABOVE FUNCTION
    df[y_var] = np.where(df[y_var].isna(), 0, df[y_var])
    df[y_var] = df[y_var].astype('int')
    for var in cont_vars:
        df[var] = np.where(df[var].isna(), np.NaN, df[var])
    for var in cat_vars:
        df[var] = np.where(df[var].isna(), ' ', df[var])
    for var in cont_vars:
        df[var] = pd.to_numeric(df[var], errors='coerce')
    logger.info('preparing pytorch dataloader')
    procs_list = []
    for x in procs:
        procs_list.append(getattr(core_tabular, x))
    dl_train, tab_train = generate_fastai_pytorch_dataloader(df, cat_vars=cat_vars, cont_vars=cont_vars, y_var=y_var,
                                                             y_block=getattr(core_tabular, y_block), y_range=y_range,
                                                             val_pct=val_pct, bs=bs, procs=procs_list, seed=seed)

    logger.info(f'saving pytorch dataloader preview check {dl_train.xs.head(2)}')
    save_fastai_preprocess_to_data_lake(preprocesser=tab_train, file_name=preprocessor_name, path=preprocessor_path,
                                        container=container_name, connection_str=connection_str, overwrite=overwrite)

    save_dataloader_to_data_lake(dl=dl_train, file_name=dl_name, path=blob_path, container=container_name,
                                 connection_str=connection_str, overwrite=overwrite)
    logger.info('pre model stage complete')

# Cell


def model_training_dl(layer_sizes: list, metrics: list, data_loader_path: str,
                      epochs: int, wd: float, model_name: str, save_metric_monitor: str,
                      save_comp, early_metric_monitor: str, early_comp, early_min_delta: float,
                      patience: int, project_location: str, plot: bool, connection_str: str,
                      overwrite: bool, container_name: str, y_range: float = None,
                      tab_config: dict = None):
    """
    Train a tabular deeplearning model from fastai this isn't dependent on anything and can be used
    for mutlipule types of tabular problems.

    Args:
    * layer_sizes (list): Layer sizes
    * metrics (list): Metrics of Interest for model to track
    * data_loader_path (str): dataloader path to load data in
    * epochs (int): Epochs
    * wd (float): weight decay
    * model_name (str): Model Name
    * save_metric_monitor (str): metric to save and evaluate on as most important not a custom loss function
    * save_comp ([type]): value that leads to saving np.Greater or np.lesser
    * early_metric_monitor (str): Monitor this metric to stop training
    * early_comp ([type]): values > or < depending on metric
    * early_min_delta (float): min change that is needed
    * patience (int): patience on how many epochs till it's time to stop training
    * project_location (str): location saves a fastai artifact
    * plot (bool): plot as you train or not
    * connection_str (str): azure connection string
    * overwrite (bool): overwrite files
    * container_name (str): azure container to save artifacts in
    * y_range (float, optional): regression type only. Defaults to None.
    * tab_config (dict, optional): add tab config for things like emb_drop. Defaults to None.

    Returns:
    * artifacts: dl_train, learn, probs, y, loss
    """
    logger.info('pulling dataloaders from azure')
    blob_puller(files=[data_loader_path],
                connection_str=connection_str,
                container_name=container_name,
                drop_location='.',
                overwrite=overwrite)
    dl_train = torch.load(data_loader_path.split('/')[-1])
    logger.info(f'size of training set {dl_train.xs.shape}')
    metrics_list = []
    for x in metrics:
        if x == 'accuracy':
            metrics_list.append(getattr(fast_metrics, x))
        else:
            metrics_list.append(getattr(fast_metrics, x)())
    learn, probs, y, loss = train_fastai_tabular_model(dl=dl_train, layer_sizes=layer_sizes, metrics=metrics_list,
                                                       epochs=epochs, wd=wd, model_name=model_name,
                                                       save_metric_monitor=save_metric_monitor, save_comp=save_comp,
                                                       early_metric_monitor=early_metric_monitor,
                                                       early_comp=early_comp, early_min_delta=early_min_delta,
                                                       patience=patience, tab_config=tab_config,
                                                       project_location=project_location, y_range=y_range, plot=plot)
    save_fastai_model_to_data_lake(learn,
                                   file_name=model_name,
                                   path=project_location,
                                   container=container_name,
                                   connection_str=connection_str,
                                   overwrite=overwrite)
    return dl_train, learn, probs, y, loss

# Cell


def query_test_set(sf_connection, cont_vars: list, cat_vars: list, y_var: list,
                   stage_name: str, data_lake_path: str, features: dict,
                   dtypes_list: list, dependent_var: str, limit_statement: str = ' '):
    """
    specfic to binary classifcatioon as it deals with data issue common to BC
    when working with the feature store

    Args:
    * sf_connection ([type]): sf connection
    * cont_vars (list): list of continous variables
    * cat_vars (list): list of categorical variables
    * y_var (list): dependent variable(s)
    * stage_name (str): data lake stage name
    * data_lake_path (str): datalake path
    * features (dict): feature list
    * dtypes_list (list): data type list
    * dependent_var (str): dependent variables name
    * limit_statement (str, optional): rapid testings. Defaults to ' '.

    Returns:
    * pd.DataFrames:  df_test, y
    """
    df_test = query_feature_set_from_data_lake_dt(sf_connection=sf_connection, dtypes_list=dtypes_list,
                                                  stage_name=stage_name, data_lake_path=data_lake_path,
                                                  features=features, limit_statement=limit_statement)
    df_test[y_var] = np.where(df_test[y_var].isna(), 0, df_test[y_var])
    df_test[y_var] = df_test[y_var].astype('int')
    for var in cont_vars:
        df_test[var] = np.where(df_test[var].isna(), np.NaN, df_test[var])
    for var in cat_vars:
        df_test[var] = np.where(df_test[var].isna(), ' ', df_test[var])
    y = df_test[dependent_var]
    if dependent_var in df_test.columns:
        df_test = df_test.drop(dependent_var, axis=1)
    return df_test, y


def predict_binary_fastai_learner(sf_connection, model_name: str, model_path: str, stage_name: str,
                                  container_name: str, connection_str, data_lake_path: str,
                                  dtypes_list: list, overwrite: bool, cpu: bool, features: list,
                                  dependent_var: str, with_label: bool, cont_vars: list,
                                  cat_vars: list, y_var: list, limit_statement: str = ' '):
    """
    predict for a binary classification model in fastai

    Args:
    * sf_connection:
    * model_name (str): model name
    * model_path (str): model path
    * stage_name (str): datalake stage name
    * container_name (str): azure container name
    * connection_str ([type]): azure connection string
    * data_lake_path (str): datalake path
    * dtypes_list (list): list of data types
    * overwrite (bool): overwrite files
    * cpu (bool): CPU = True, GPU = False
    * features (list): feature list
    * dependent_var (str): dependent variables name
    * with_label (bool): with labels is the dependent variable available
    * cont_vars (list): list of continous variables
    * cat_vars (list): list of categorical variables
    * y_var (list): dependent variable
    * limit_statement (str, optional): rapid testing. Defaults to ' '.

    Returns:
    *depends on if statement
    """
    learn_inf = pull_fastai_learner_from_data_lake(file_name=model_name,
                                                   path=model_path,
                                                   container=container_name,
                                                   connection_str=connection_str,
                                                   overwrite=overwrite,
                                                   cpu=cpu)
    logger.info('query test set from datalake')
    df_test, y = query_test_set(sf_connection=sf_connection, cont_vars=cont_vars, cat_vars=cat_vars, y_var=y_var,
                                stage_name=stage_name, data_lake_path=data_lake_path, dtypes_list=dtypes_list,
                                features=features, dependent_var=dependent_var, limit_statement=limit_statement)
    logger.info('preparing dataset')
    test_dl = learn_inf.dls.test_dl(df_test, with_label=False)
    logger.info('begining predictions')
    probs, _ = learn_inf.get_preds(dl=test_dl)
    probs = to_np(probs)
    preds = np.where(probs[:, 1] > 0.5, 1, 0)
    logger.info('predictions complete')
    df_test['baseline'] = 0
    if y is not None:
        baseline_acc = round(accuracy_score(y_true=y, y_pred=df_test.baseline.values), 4)
        model_acc = accuracy_score(y_true=y, y_pred=preds)
        logger.info(f'baseline/model {baseline_acc}/{model_acc} and actuals/predicted {sum(y)}/{sum(preds)}')
        acc_lift = model_acc - baseline_acc
        fpr, tpr, thresholds = roc_curve(y, probs[:, 1])
        actual_auc = auc(fpr, tpr)
        return df_test, probs, preds, model_acc, actual_auc, baseline_acc, acc_lift
    return df_test, probs, preds


def push_prediction_file_to_snowflake(sf_connection, df_dict: dict, pred_table_name: str,
                                      preds_path: str, short_stage_path: str, sf_col_types: dict,
                                      container_name: str, pattern: str, connection_str: str,
                                      stage_name: str, overwrite: bool):
    """
    Take prediction file and moves it directly to azure data lake to snowflake.

    Args:
    * sf_connection ([type]): Snowflake connection
    * df_dict (dict): data dictionary for dataframe to be created
    * pred_table_name (str): Prediction table name
    * preds_path (str): path to dump predictions
    * short_stage_path (str): datalake short path
    * sf_col_types (dict): col types
    * container_name (str): azure container name
    * pattern (str): pattern for files in datalake
    * connection_str (str): azure datalake connection
    * stage_name (str): stage name for datalake
    * overwrite (bool): overwritefiles
    """
    pred_df = pd.DataFrame(df_dict)
    logger.info(f'preview file {pred_df.head(2)}')
    push_dataframe_to_data_lake_as_parquet(pred_df,
                                           path=preds_path,
                                           container=container_name,
                                           connection_str=connection_str,
                                           overwrite=overwrite)
    logger.info('file sent to azure data lake and now preparing to go to snowflakee')
    move_parquet_table_to_snowflake(sf_connection=sf_connection,
                                    table_name=pred_table_name,
                                    stage_name=stage_name,
                                    path=short_stage_path,
                                    columns_and_types=sf_col_types,
                                    pattern=pattern,
                                    replace_table=True)
    logger.info(f'Preview {pred_table_name} {sf_connection.run_str_query(f"SELECT * FROM {pred_table_name} LIMIT 3;")}')

# Cell


def pull_dl_train_from_data_lake(dl_path: str, container: str,
                                 connection_str: str, overwrite: bool):
    """
    pulls data loader from datalake location and returns pd.DataFrames
    for other model users.

    Args:
    * dl_path (str): data loader path location
    * container (str): azure contatiner name
    * connection_str (str): azure connection string
    * overwrite (bool): overwrite file

    Returns:
    * pd.Dataframe: Training and Validatioon Sets return
    """
    blob_puller(files=[dl_path],
                connection_str=connection_str,
                container_name=container,
                drop_location='.',
                overwrite=overwrite)
    dl_train = torch.load(dl_path.split('/')[-1])
    data_sets = change_dl_to_pandas_df(dl_train)
    logger.info(f'{data_sets[0].shape, len(data_sets[1]), data_sets[2].shape, len(data_sets[3])}')
    return data_sets

# Cell


def bc_predict_hyperopt(snowflake_connection, x_valid, y_valid, model,
                        model_type: str, cont_vars: list, cat_vars: list, y_var: list,
                        stage_name: str, data_lake_path: str, features: dict,
                        dtypes_list: list, dependent_var: str, threshold: float,
                        connection_str: str, model_file_name: str, model_file_path: str,
                        container_name: str, transformer_path: str, transformer_name: str,
                        feature_importance: bool = True, limit_statement: str = None,
                        plot: bool = False, save_model: bool = False):
    """    Create a prediction for hyper opt sklearn binary classification problems
    it needs very little and will return basic infromationn when using this for something
    a user might want to use when working with a model and binary classification

    Args:
    * snowflake_connection (snowflake connection): connection to snowflake
    * x_valid (pd.DataFrame): validation set
    * y_valid (np.array): true values of the dependent variable
    * model (classifier): Sklearn Model
    * model_type (str): model type
    * cont_vars (list): list of continous variables
    * cat_vars (list): list of categorical variables
    * y_var (list): dependent variable
    * stage_name (str): datalake stage name where data is to be stored
    * data_lake_path (str): datalake path in stage to be stored
    * features (dict): features
    * dtypes_list (list): dtype list for the data
    * dependent_var (str): dependent variable name
    * threshold (float): logit cut off point
    * connection_str (str): connection string to azure datalake
    * model_file_name (str): model name to be saved as
    * model_file_path (str): where to save the model
    * container_name (str): container in azure to save
    * transformer_path (str): location to grab and save tranformer
    * transformer_name (str): name of transformer
    * feature_importance (bool, optional): do you want to get fi. Defaults to True.
    * limit_statement (str, optional): for rapid testing. Defaults to None.
    * plot (bool, optional): shows plots. Defaults to False.
    * save_model (bool, optional): not used. Defaults to False.

    Returns:
    * info: df_test, probs, preds, model, transformer, val_auc, val_bal_acc
    """
    logger.info(f'INFORMATION FOR {model_type}')
    report = binary_classification_reporter_sklearn(m=model, x=x_valid, y=y_valid, threshold=threshold,
                                                    plot=plot, feature_importance=feature_importance)
    if feature_importance is True:
        probs, preds, val_auc, val_bal_acc, fi_permutation = report
    else:
        probs, preds, val_auc, val_bal_acc, fi_permutation = report
    df_test, y = query_test_set(sf_connection=snowflake_connection, cont_vars=cont_vars, cat_vars=cat_vars, y_var=y_var,
                                stage_name=stage_name, data_lake_path=data_lake_path, dtypes_list=dtypes_list,
                                features=features, dependent_var=dependent_var, limit_statement=None)
    df_test[dependent_var] = y
    results = pull_transform_predict_sklearn(df=df_test, model=model, model_file_name=model_file_name,
                                             snowflake_connection=snowflake_connection, model_file_path=model_file_path,
                                             container=container_name, connection_str=connection_str,
                                             transformer_path=transformer_path, transformer_name=transformer_name,
                                             overwrite=True)
    df_test, probs, preds, model, transformer = results
    if y is not None:
        df_test['baseline'] = 0
        baseline_acc = round(accuracy_score(y_true=y, y_pred=df_test.baseline.values), 4)
        model_acc = accuracy_score(y_true=y, y_pred=preds)
        logger.info(f'baseline/model {baseline_acc}/{model_acc} and actuals/predicted {sum(y)}/{sum(preds)}')
        acc_lift = model_acc - baseline_acc
        fpr, tpr, thresholds = roc_curve(y, probs[:, 1])
        actual_auc = auc(fpr, tpr)
        return df_test, probs, preds, model, transformer, model_acc, actual_auc, \
            baseline_acc, acc_lift, val_auc, val_bal_acc
    return df_test, probs, preds, model, transformer, val_auc, val_bal_acc