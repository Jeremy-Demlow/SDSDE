# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_modeling_preprocessing_fastai.ipynb (unless otherwise specified).

__all__ = ['logger', 'load_pandas', 'generate_fastai_pytorch_dataloader', 'prepare_test_pre_model',
           'save_fastai_preprocess_to_data_lake', 'save_dataloader_to_data_lake']

# Cell
from ..wrapper.azurewrapper import blob_pusher
from fastai.tabular.core import Categorify, FillMissing, Normalize, RandomSplitter, range_of, CategoryBlock, torch
from fastai.tabular.data import TabularDataLoaders
from fastai.tabular.all import distrib_barrier
from fastai.tabular.core import TabularPandas
from pathlib import Path
from fastcore.basics import patch

import warnings
import pickle
import os
import logging
import pandas as pd
import numpy as np


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cell
@patch
def export(self: TabularPandas, fname='export.pkl', pickle_protocol=2):
    """
    Helper function it's a patch to fastai to allow the tabular preprocess
    to be pulled out and extraploated onto a new dataset with out the data
    this was a huge development.

    Args:
    * self (TabularPandas): TabularPandas
    * fname (str, optional): File Name and Path. Defaults to 'export.pkl'.
    * pickle_protocol (int, optional): Defaults to 2.
    """
    old_to = self
    self = self.new_empty()
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        pickle.dump(self, open(Path(fname), 'wb'), protocol=pickle_protocol)
        self = old_to

# Cell

def load_pandas(fname):
    "Load in a `TabularPandas` object from `fname`"
    distrib_barrier()
    res = pickle.load(open(fname, 'rb'))
    return res

# Cell
def generate_fastai_pytorch_dataloader(df: pd.DataFrame,
                                       cat_vars: list, cont_vars: list, y_var: list,
                                       y_block=CategoryBlock(), y_range: float = None,
                                       bs: int = 254, val_pct: float = 0.2, seed=None,
                                       impute: bool = True, splits: list = None,
                                       procs: list = [Categorify, FillMissing, Normalize]):
    """
    Active Development with Sklearn Pipeline, but currently when using the fastai dataloader
    we are using the DataLoaderAPI as well as the TabularPandas functions. The reason that
    both are in here is to allow the user to export the preprocess process for a model outside
    of the Fastai ecosystem.

    See notebook for more information on this process.

    Args:
    * df (pd.DataFrame): [description]
    * y_block ([type], optional): [description]. Defaults to CategoryBlock().
    * y_range (float, optional): This is giving the range of a prediction for the model and is logged automatically and reported back to the user np.exp. Defaults to None.
    * bs (int, optional): Batch Size. Defaults to 254.
    * val_pct (float, optional): Validation Size. Defaults to 0.2.
    * seed ([type], optional): Seed For Split. Defaults to None.
    * impute (bool, optional): Sklearn Impute Function. Defaults to True.
    * procs (list, optional): Defaults to most common methods. Defaults to [Categorify, FillMissing, Normalize].

    Returns:
    * Fastai: dl_train, tab_train
    """
    if splits is None:
        splits = RandomSplitter(valid_pct=val_pct, seed=seed)(range_of(df))
        logger.info(f'Training Data Size {len(splits[0])}')
        logger.info(f'Validation Data Size {len(splits[1])}')
    logger.info(f'Categorical Variable(s) For Project {cat_vars}')
    logger.info(f'Continuous Variable(s) For Project {cont_vars}')
    logger.info(f'Dependent Variable(s) For Project {y_var}')
    logger.info('dataloader being created')
    if y_range is not None:
        max_log_y = np.log(np.max(df[y_var]*y_range))
        min_log_y = np.log(np.min(df[y_var]))
        y_range = torch.tensor([min_log_y, max_log_y], device=None)
        logger.info(f'Model Prediction Range {np.exp(y_range)}')

    tab_train = TabularPandas(df, procs=procs, cat_names=cat_vars,
                              cont_names=cont_vars,
                              y_names=y_var, y_block=y_block,
                              splits=splits)

    dl_train = (TabularDataLoaders.from_df(df, procs=procs, y_range=y_range,
                                           cat_names=cat_vars, cont_names=cont_vars,
                                           y_names=y_var, y_block=y_block,
                                           valid_idx=splits[1], bs=bs))
    logger.info(dl_train.train.xs.head())
    return dl_train, tab_train

# Cell
def prepare_test_pre_model(df: pd.DataFrame, dl: TabularDataLoaders = None, label: bool = False):
    """
    helper function that takes a tabular dataloader and returns a prepared dataloader for a new
    datas set

    Args:
    * df (pd.DataFrame): data frame
    * dl (TabularDataLoaders, optional): tabulardataloader. Defaults to None.
    * label (bool, optional): Does the data set have the label of interest. Defaults to False.

    Returns:
    * TabularDataLoader
    """
    dl_test = dl.test_dl(df, with_label=label)
    logger.info(f'dl test {dl_test.xs.head()}')
    return dl_test

# Cell
def save_fastai_preprocess_to_data_lake(preprocesser, file_name: str, path: str,
                                        container: str, connection_str: str, overwrite: bool = False):
    """
    push preprocess object to azure datalake

    Args:
    * preprocesser (object): preprocessor
    * file_name (str): filename
    * path (str): path to save file
    * container (str): container name
    * connection_str (str): azure connection string
    * overwrite (bool, optional): overwrite files. Defaults to False.
    """
    logger.info(f'Pushing Fastai Preprocesser Object to Azure: {os.path.join(path, file_name)}')
    preprocesser.export(file_name)
    blob_pusher(container_name=container,
                connection_str=connection_str,
                file_path=[file_name],
                blob_dest=[path],
                overwrite=overwrite)
    os.unlink(file_name)

# Cell


def save_dataloader_to_data_lake(dl, file_name: str, path: str,
                                 container: str, connection_str: str, overwrite: bool = False):
    """
    push preprocess object to azure datalake

    Args:
    * dl (object): dataloader
    * file_name (str): filename
    * path (str): path to save file
    * container (str): container name
    * connection_str (str): azure connection string
    * overwrite (bool, optional): overwrite files. Defaults to False.
    """
    logger.info(f'Pushing DataLoader Object to Azure: {os.path.join(path, file_name)}')
    torch.save(dl, file_name)
    blob_pusher(container_name=container,
                connection_str=connection_str,
                file_path=[file_name],
                blob_dest=[path],
                overwrite=overwrite)
    os.unlink(file_name)