# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_modeling_premodel.ipynb (unless otherwise specified).

__all__ = ['logger', 'stage_query_generator', 'make_data_lake_stage', 'pull_static_feature_set_to_data_lake',
           'temporal_and_static_dump_data_to_datalake', 'query_feature_set_from_data_lake',
           'query_feature_set_from_data_lake_dt', 'query_pushed_parquet_table_data_lake']

# Cell
import os
import sys
import logging

logging.basicConfig(level=logging.INFO)
logging.getLogger("azure.core").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.CRITICAL)
logging.getLogger("snowflake.connector").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# Cell


def stage_query_generator(stage_name, url, sas_token, file_type='parquet'):
    """generates the snowflake query needed to create an external stage in
    azure blob

    Args:
    * stage_name (str): name of the stage in snowflake
    * url (str): azure formated string for account, container, and path
    * sas_token (str): blob sas token for shared access
    * file_type (str, optional): type of files expected in stage. Defaults to 'parquet'. Can use 'csv' as well.

    Returns:
    * str: snowflake query to create stage
    """
    stage_template = '''
    create or replace stage STAGE_NAME_HERE
      url='URL_HERE'
      credentials=(azure_sas_token='SAS_TOKEN_HERE')
      encryption=(type= 'NONE')
      file_format = (type = FILE_TYPE_HERE);
    '''
    stage_query = stage_template.replace('STAGE_NAME_HERE', stage_name)
    stage_query = stage_query.replace('SAS_TOKEN_HERE', sas_token)
    stage_query = stage_query.replace('URL_HERE', url)
    stage_query = stage_query.replace('FILE_TYPE_HERE', file_type)
    return stage_query

# Cell


def make_data_lake_stage(sf_connection,
                         stage_name,
                         account,
                         container,
                         data_lake_path,
                         sas_token,
                         file_type='parquet'):
    """creates a data lake staging environment from snowflake

    Args:
    * sf_connection (``SnowflakeConnect``): snowflake connection
    * stage_name (str): name of stage in snowflake
    * account (str): blob storage account
    * container (str): blob container
    * data_lake_path (str): path in the container to stage in
    * sas_token (str): shared access token for blob
    * file_type (str, optional): type of files to stage. Defaults to 'parquet'.
    """
    stage_url = f'azure://{account}.blob.core.windows.net/{container}/{data_lake_path}'
    stage_query = stage_query_generator(stage_name, stage_url, sas_token, file_type='parquet')
    sf_connection.run_str_query(stage_query)

# Cell


def pull_static_feature_set_to_data_lake(sf_connection: object,
                                         stage_name: str,
                                         data_lake_path: str,
                                         features: list,
                                         grain='ECID',
                                         limit_statement='',
                                         overwrite=True,
                                         ):
    """given a list of features and a modeling gain this pulls data from snowflake to a
    data lake in raw file format. data will be in the format specified by the stage.
    parquet with snappy is recommended.

    Args:
    * sf_connection (object): snowflake connection
    * stage_name (str): stage name (already created)
    * data_lake_path (str): where in the data lake to dump data
    * features (list): feature set
    * grain (str, optional): gain from feature store for rows. Defaults to 'ECID'.
    * limit_statement (str, optional): limit statement to insert to SQL ie "limit 1000". Defaults to ''. Used for debugging.
    * overwrite (bool, optional): overwrite existing data or not. Defaults to True.
    """

    full_stage_path = os.path.join(stage_name, data_lake_path)
    select_query = f'''
    copy into @{full_stage_path} from
    (
        select
            {grain}
            FEATURES_HERE
        from "MACHINELEARNINGFEATURES"."PROD".FEATURESTORE_{grain}
        {limit_statement}
    )
    overwrite = {overwrite}
    '''
    logger.info(f'Pulling {len(features)} {grain} features to the datalake at path {data_lake_path}')
    for feature in features:
        select_query = select_query.replace('FEATURES_HERE', f', {feature}FEATURES_HERE')
    select_query = select_query.replace('FEATURES_HERE', '')
    response = sf_connection.run_str_query(select_query)
    logger.info(f'Copy response\n{response}')

# Cell


def temporal_and_static_dump_data_to_datalake(sf_connection: object,
                                              stage_name: str,
                                              data_lake_path: str,
                                              feature_dict: list,
                                              base_query: str,
                                              grain='ECID',
                                              limit_statement='',
                                              overwrite=True):
    """
    Populates stages datalake via a snowflake query.

    Args:
        sf_connection ([type]): SnowFlake Connection
        yaml_file (str, optional): Yaml file name . Defaults to 'dataload.yaml'.
        yaml_section (str, optional): Yaml section to read. Defaults to 'inputdata'.
        data_set (str, optional): Training or Test Set. Defaults to 'train_set'.
        overwrite (bool, optional): Overwrite exisiting file or not. Defaults to True.

    Returns:
        str: query used to created the dump
    """
    full_stage_path = os.path.join(stage_name, data_lake_path)
    # copy snowflake data into the stage
    select_query = f'''
    copy into @{full_stage_path} from
    (
    SELECT
        FEATURES_HERE
        TEMPORAL_HERE
    FROM
    (
     {base_query}
    ) base
    LEFT JOIN "MACHINELEARNINGFEATURES"."PROD"."FEATURESTORE_{grain}" mlf ON base."{grain}" = mlf.{grain}
    {limit_statement}
    )
    OVERWRITE={overwrite}
    ;
    '''
    for ind, feature in enumerate(feature_dict['static_features'].keys()):
        if ind == 0:
            if feature == 'ECID':
                select_query = select_query.replace('FEATURES_HERE', f'base.{feature}FEATURES_HERE')
            else:
                select_query = select_query.replace('FEATURES_HERE', f'mlf.{feature}FEATURES_HERE')
        else:
            if feature == 'ECID':
                select_query = select_query.replace('FEATURES_HERE', f', base.{feature}FEATURES_HERE')
            else:
                select_query = select_query.replace('FEATURES_HERE', f', mlf.{feature}FEATURES_HERE')
    for feature, values in feature_dict['temporal_features'].items():
        select_query = select_query.replace('TEMPORAL_HERE', f', machinelearningfeatures.{os.environ.get("prod_or_dev", "dev")}.{feature}({" , ".join(values["args"])})TEMPORAL_HERE')
    select_query = select_query.replace('FEATURES_HERE', '')
    select_query = select_query.replace('TEMPORAL_HERE', '')
    logging.info(f'query {select_query}')
    response = sf_connection.run_str_query(select_query)
    logging.info(f'Copy response\n{response}')
    return select_query

# Cell


def query_feature_set_from_data_lake(sf_connection: object,
                                     stage_name: str,
                                     data_lake_path: str,
                                     features: list,
                                     limit_statement='',
                                     ):
    """once data resides in the data lake, this will allow you to query the data
    into python RAM

    Args:
    * sf_connection (object): snowflake connection
    * stage_name (str): data lake stage in snowflake
    * data_lake_path (str): extention path in the data lake
    * features (list): list of features
    * limit_statement (str, optional): limit statement to insert to SQL ie "limit 1000". Defaults to ''. Used for debugging.

    Returns:
    * [DataFrame]: feature set
    """

    # create query string
    query = f'''
        select
            FEATURES_HERE
        from @{os.path.join(stage_name, data_lake_path)}
        {limit_statement}
    '''
    for ind, feature in enumerate(features):
        if ind == 0:
            query = query.replace('FEATURES_HERE', f'$1:"_COL_{ind}" as {feature}FEATURES_HERE')
        else:
            query = query.replace('FEATURES_HERE', f', $1:"_COL_{ind}" as {feature}FEATURES_HERE')
    query = query.replace('FEATURES_HERE', '')

    # query data
    logger.info(f'Querying Feature Set From Data Lake {query}')

    df = sf_connection.run_str_query(query)
    df.columns = [i.upper() for i in df.columns]
    logger.info(f'Final Dataset Shape - {df.shape}')
    return df

# Cell


def query_feature_set_from_data_lake_dt(sf_connection: object,
                                        stage_name: str,
                                        data_lake_path: str,
                                        features: list,
                                        dtypes_list: list,
                                        limit_statement='',
                                        ):
    """once data resides in the data lake, this will allow you to query the data
    into python RAM

    Args:
    * sf_connection (object): snowflake connection
    * stage_name (str): data lake stage in snowflake
    * data_lake_path (str): extention path in the data lake
    * features (list): list of features
    * limit_statement (str, optional): limit statement to insert to SQL ie "limit 1000". Defaults to ''. Used for debugging.

    Returns:
    * [DataFrame]: feature set
    """
    query = f'''
        select
            FEATURES_HERE
        from @{os.path.join(stage_name, data_lake_path)}
        {limit_statement}
    '''
    for ind, feature in enumerate(zip(features, dtypes_list)):
        if ind == 0:
            query = query.replace('FEATURES_HERE', f'$1:"_COL_{ind}"::{feature[1].upper()} as {feature[0]}FEATURES_HERE')
        else:
            query = query.replace('FEATURES_HERE', f', $1:"_COL_{ind}"::{feature[1].upper()} as {feature[0]}FEATURES_HERE')
    query = query.replace('FEATURES_HERE', '')

    logging.info(f'Querying Feature Set From Data Lake {query}')
    df = sf_connection.run_str_query(query)
    df.columns = [i.upper() for i in df.columns]
    logging.info(f'Final Dataset Shape - {df.shape}')
    return df

# Cell


def query_pushed_parquet_table_data_lake(sf_connection: object,
                                         stage_name: str,
                                         data_lake_path: str,
                                         feature_dict: dict,
                                         limit_statement='',
                                         pattern='.*parquet'):
    """
    Function is used when there is a parquet table in azure datalake
    that need to be brought into memory for exploration

    Args:
    * sf_connection (object): Snowflake Engine
    * stage_name (str): Azure stage name
    * data_lake_path (str): Data Lake path
    * feature_dict (dict): feature dictionary
    * limit_statement (str, optional): limit statment. Defaults to ''.
    * pattern (str, optional): pattern to read partitions. Defaults to '.*parquet'.

    Returns:
    * pd.DataFrame: Data Frame
    """
    if not stage_name.endswith('/'):
        stage_name += '/'
    if data_lake_path.startswith('/'):
        logging.error('data_lake_path should not start with / please remove and re-run')
        sys.exit()
    query = f'''
        select
            FEATURES_HERE
        from @{os.path.join(stage_name, data_lake_path)} (pattern=>'{pattern}')
        {limit_statement}
    '''
    features = feature_dict.keys()
    for ind, feature in enumerate(features):
        if ind == 0:
            query = query.replace('FEATURES_HERE', f'$1:"{feature.lower()}" as {feature}FEATURES_HERE')
        else:
            query = query.replace('FEATURES_HERE', f', $1:"{feature.lower()}" as {feature}FEATURES_HERE')
    query = query.replace('FEATURES_HERE', '')
    logger.info(f'Querying Feature Set From Data Lake {query}')
    df = sf_connection.run_str_query(query)
    df.columns = [x.upper() for x in df.columns]
    logger.info('fixing dtypes')
    for k, v in feature_dict.items():
        if v['variable_type'] == 'cont':
            df[k] = df[k].astype('float')
    logger.info(f'Final Dataset Shape - {df.shape}')
    return df