# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_azure_wrappers.ipynb (unless otherwise specified).

__all__ = ['logger', 'blob_pusher', 'blob_puller', 'unlink_files', 'save_and_push_data']

# Cell
from ..azure.filehandling import FileHandling

import os
import numpy as np
import logging


logging.basicConfig(level=logging.INFO)
logging.getLogger("azure.core").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.CRITICAL)
logging.getLogger("snowflake.connector").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# Cell


def blob_pusher(container_name: str,
                connection_str: str,
                file_path: list = None,
                blob_dest: list = None,
                **kwargs):
    """
    function that will push file(s) to azure blob

    Args:
    * container_name (str): container name
    * connection_str (str): connection str
    * file_path (list, optional): file location of file(s). Defaults to None.
    * blob_dest (list, optional): where to drop in azure blob. Defaults to container_name.

    Returns:
        str: file_path
    """
    fh = FileHandling(connection_str)
    blob_dest = [container_name] if blob_dest is None else blob_dest
    if len(blob_dest) != len(file_path):
        for f in file_path:
            fh.upload(container_name=container_name,
                      file_path=f,
                      dest=os.path.join(blob_dest[0], f.split('/')[-1]),
                      **kwargs)
    else:
        for f, p in zip(file_path, blob_dest):
            fh.upload(container_name=container_name,
                      file_path=f,
                      dest=str(os.path.join(p, f.split('/')[-1])),
                      **kwargs)
    return file_path

# Cell


def blob_puller(files: list,
                connection_str: str,
                container_name: str,
                drop_location: str = '.',
                **kwargs):
    """
    Can pull a list or one file from azure

    Args:
    * files (list): list of files or a file wrapped in []
    * connection_str (str): connection string to azure blob storage
    * container_name (str): container name
    * drop_location (str, optional): where to drop file(s) locally. Defaults to ''.
    """
    fh = FileHandling(connection_str)
    drop_location = drop_location if drop_location.endswith('/') else drop_location + '/'
    for f in files:
        fh.download_file(container_name=container_name,
                         file=f,
                         file_path=drop_location,
                         **kwargs)

# Cell


def unlink_files(files: list, file_path: str = './'):
    """
    File Clean Up After Model Prediction

    Args:
    * files (list): file(s) name(s)
    * file_path (str, optional): file(s) path(s). Defaults to './'.
    """
    file_list = files
    for x in file_list:
        os.unlink(os.path.join(file_path, x))

# Cell


def save_and_push_data(container_name: str,
                       connection_str: str,
                       df_names: list = None,
                       dfs: list = None,
                       np_names: list = None,
                       nps: list = None,
                       blob_dest: str = None,
                       parquet: bool = True,
                       **kwargs):
    """
    Takes panda dataframes and wirtes them to parquet files
    Takes numpy arrays, list, and dictionaries and writes them out
    as numpy files.

    Note: to get the dictionary out upon load you need to add a .item()
    this will return the dict as a not np array.

    Args:
    * container_name (str): location in blob storage
    * connection_str (str): connect_str for azure
    * df_names (list, optional): list of names for the files. Defaults to [].
    * dfs (list, optional): list of panda dataframes. Defaults to [].
    * np_names (list, optional): list of names for the files. Defaults to [].
    * nps (list, optional): list of numpy arrays to write out. Defaults to [].
    * blob_folder (str, optional): folder you would like. Defaults to None.
    * parquet (bool): true means save df as parquet files. Defaults to True.
    """
#     Once again Snowflake Parquet upload isn't easy will figure out
    if parquet is True:
        _ = [d.to_parquet(f"{n}") for d, n in zip(dfs, df_names)]
    else:
        _ = [d.to_csv(f"{n}") for d, n in zip(dfs, df_names)]
    _ = [np.save(f'{n}', d) for d, n in zip(nps, np_names)]
    files_list = np.concatenate([df_names, np_names]).tolist()
    _ = blob_pusher(container_name=container_name,
                    connection_str=connection_str,
                    file_path=files_list,
                    blob_dest=blob_dest,
                    **kwargs)
    return files_list