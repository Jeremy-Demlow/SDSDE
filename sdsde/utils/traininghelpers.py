# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_utils_traininghelpers.ipynb (unless otherwise specified).

__all__ = ['logger', 'plot_fi', 'binary_classification_reporter_sklearn']

# Cell
import logging
import scikitplot as skplt
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from rfpimp import *
from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve, balanced_accuracy_score

logging.basicConfig(level=logging.INFO)
logging.getLogger("azure.core").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.CRITICAL)
logging.getLogger("snowflake.connector").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# Cell


def plot_fi(fi, std=True, feature_importance_type=''):
    """plots feature importance"""
    if std:
        ax = fi.plot('cols', 'imp', 'barh', figsize=(12, 7), legend=False, xerr='std')
    else:
        ax = fi.plot('cols', 'imp', 'barh', figsize=(12, 7), legend=False)

    ax.set_xlabel(f"{feature_importance_type} Feature Importance")
    return ax


def binary_classification_reporter_sklearn(m, x: pd.DataFrame, y=None, threshold: float = 0.5,
                                           plot: bool = False, feature_importance: bool = False):
    """
    Returns basic information on a binary classification problem
    TODO: Make More flexibile

    Args:
    * m ([type]): model
    * x (pd.DataFrame): Training data
    * y ([type], optional): Labels. Defaults to None.
    * threshold (float, optional): Threshold for logit cut off. Defaults to 0.5.
    * plot (bool, optional): show plots. Defaults to False.
    * feature_importance (bool, optional): calculate fi. Defaults to False.

    Returns:
    * list: probailities and predictions
    """
    logger.info('predicting set')
    probs = m.predict_proba(x)
    returns = probs[:, 1]
    preds = np.where(returns > threshold, 1, 0)
    logger.info(f"Percent Return Predicted Set {'{:.3}%'.format(preds.sum()/len(probs))}")
    if y is not None:
        logger.info(f"Percent Return Actual Set {'{:.3}%'.format(y.sum()/len(probs))}")
        cm = confusion_matrix(y, preds)
        logger.info("Accuracy For Each Class")
        logger.info(cm.diagonal()/cm.sum(axis=1))
        logger.info(cm)
        logger.info(classification_report(y, preds))
        accuracy_value = balanced_accuracy_score(y_true=y, y_pred=preds)
        fpr, tpr, thresholds = roc_curve(y, returns)
        val_auc = auc(fpr, tpr)
        logger.info(f'AUC {val_auc}')
    if plot is True:
        skplt.metrics.plot_roc(y, probs)
        plt.show()
        skplt.metrics.plot_precision_recall(y, probs)
        plt.show()
    if feature_importance is True:
        fi_permutation = importances(m, x, y)
        fi_permutation = (fi_permutation
                          .reset_index()
                          .rename({'Feature': 'cols', 'Importance': 'imp'}, axis=1))
        plot_fi(fi_permutation[:10], False, 'Permutation')
        plot_fi(fi_permutation[-10:], False, 'Permutation')
    if y is not None and feature_importance is True:
        return probs, preds, val_auc, accuracy_value, fi_permutation
    elif y is not None and feature_importance is False:
        return probs, preds, val_auc, accuracy_value
    else:
        return probs, preds