# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_azure.ipynb (unless otherwise specified).

__all__ = ['logger', 'FileHandling', 'unlink_files', 'rename_adls_directory']

# Cell
from azure.storage.blob import BlobServiceClient
from azure.storage.blob import ContainerClient
from azure.storage.filedatalake import DataLakeServiceClient

import logging
import os
import re
import uuid

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cell


class FileHandling:
    def __init__(self, connection_string, logger=None):
        """
        file handling for all sdsde Azure blob storage. Both upload and download
        have there own clients with in this class so that they can call different
        containers with in the storage account within on init. This is designed,
        but can be removed if we still this to be a waste.
        Args:
        * connection_string (str): azure connection string to blob storage
        * self._logger: logging choices can be overwritten if you want the defaults
        """
        self.connection_string = connection_string
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self._logger = logger if logger is not None else logging.getLogger(__name__)

    def upload(self,
               container_name: str,
               file_path: str,
               dest: str = None,
               overwrite: bool = False):
        """
        upload a folder a file to azure blob storage
        Args:
        * container_name (str): azure blob container name
        * file_path (str): file or directory to upload to azure
        * dest (str, optional): destination in azure/ file name in azure. Defaults to ''.
        * overwrite (bool, optional): write over same file names. Defaults to False.
        """
        self.container_name = container_name
        self.container_client = self.blob_service_client.get_container_client(container_name)
        self.create_blob_container(container_name=container_name)
        if (os.path.isdir(file_path)) is not False:
            self.load_dir = True
            self.upload_dir(file_path, dest, overwrite)
        else:
            if dest is not None:
                pass
            else:
                dest = file_path.split('/')[-1]
                self._logger.warning(f'destination to blob is now {dest}')
                self._logger.warning('to delcare new path or file name add dest to upload')
            self.load_dir = False
            self.upload_file(file_path, dest, overwrite)

    def upload_file(self,
                    file_path: str,
                    blob_name: str,
                    overwrite=False):
        """
        simply uploads a file to blob storage
        Args:
        * file_path (str): file to blob
        * blob_name (str): destination in blob
        * overwrite (bool, optional): write over same file names. Defaults to False.
        """
        self._logger.info(f'Uploading {file_path}, to to Azure Storage {blob_name}')
        with open(file_path, "rb") as file:
            try:
                self.container_client.upload_blob(data=file, name=blob_name, overwrite=overwrite)
            except Exception as e:
                self._logger.error(f'"Error Message: {e.error_code.value}"')
        self._logger.info('Azure Upload Complete')

    def upload_dir(self,
                   directory: str,
                   dest: str = None,
                   overwrite: bool = False):
        """
        simply uploads a directory to azure blob note with python
        we can have arguement be used from upload will figure
        that out hopefully
        Args:
        * directory (str): directory that is being moved to blob
        * overwrite (bool, optional): write over same file names. Defaults to False.
        """
        prefix = os.path.basename(directory) + '/'
        # Walk Through Directory
        for root, dirs, files in os.walk(directory):
            for name in files:
                if dest is None:
                    dir_part = os.path.relpath(root, directory)
                    dir_part = '' if dir_part == '.' else dir_part + '/'
                else:
                    dir_part = dest
                file_path = os.path.join(root, name)
                blob_path = prefix + dir_part + name
                self.upload_file(file_path, blob_path, overwrite)

    def create_blob_container(self,
                              container_name: str = str(uuid.uuid4()),
                              unique: bool = False):
        """
        creates/check for container when ``upload`` is called, but
        this function can be used seperately to create
        a new container in isolation within the specific storage account
        Args:
        * container_name (str, optional): creates azure blob container name. Defaults to str(uuid.uuid4()).
        * unique (bool, optional): add a unique tail to container name. Defaults to False.
        """
        regex = re.compile(r'[@_!#$%^&*()<>?/\|}{~:]')
        # Check if container_name is valid
        if (regex.search(container_name) is None):
            self._logger.info(f'{container_name} is a valid')
        else:
            container_name = re.sub(r'[\W_]+', '', container_name)
            self._logger.info(f'container_name changed to {container_name}')
        if unique:
            container_name = container_name + str(uuid.uuid4())
        else:
            container_name = container_name
        # Current Python SDK doesn't support exist
        try:
            # Create the container
            _ = self.blob_service_client.create_container(container_name)
        except Exception as e:
            self._logger.info(f'{e.error_code.value}')

    def download(self,
                 blob_location: str,
                 dest: str,
                 container_name: str,
                 blob_path: str = '',
                 recursive: bool = True,
                 overwrite: bool = False):
        """
        Args:
        * blob_location (str): location in container
        * dest (str): local destination of file
        * container_name (str): azure blob container name.
        * blob_path (str, optional): path in blob helps search. Defaults to ''.
        * recursive (bool, optional): helps with search for file can be false if blob_path is known. Defaults to True.
        * overwrite (bool, optional): write over same file names. Defaults to False.
        """
        if not dest.endswith('/'):
            dest += '/'
        if blob_location.endswith('/'):
            if blob_path == '':
                blobs = self.ls_blob(container_name=container_name, path=blob_path, recursive=recursive)
                blobs = [blobs for blobs in blobs if os.path.basename(os.path.normpath(blob_location)) in blobs]
            else:
                blobs = self.ls_blob(container_name=container_name, path=blob_path, recursive=recursive)
                blobs = [blob_location + blob for blob in blobs]
            for blob in blobs:
                self._logger.info(f'Downloading {blob}')
                self.download_file(container_name, blob, dest, overwrite)
        else:
            self.download_file(container_name, os.path.basename(os.path.normpath(blob_location)), dest, overwrite)
        self._logger.info('Download complete')

    def download_file(self,
                      container_name: str,
                      file: str,
                      file_path: str,
                      overwrite: bool = False):
        """
        Args:
        * container_name (str): azure blob container name.
        * file (str): file to download from blob
        * file_path (str): location to put `file`
        * overwrite (bool, optional): write over same file names. Defaults to False.
        """
        if file_path.endswith('.'):
            file_path += '/'
        blob_dest = file_path + os.path.basename(file) if file_path.endswith('/') else file_path
        self._logger.info(f'{file} to {blob_dest}')
        if not overwrite:
            if os.path.exists(blob_dest):
                self._logger.warning('file path already exist change overwrite to ``True`` if you want to overwrite file')
                return
        os.makedirs(os.path.dirname(blob_dest), exist_ok=True)
        download_client = self.blob_service_client.get_container_client(container_name)
        downloader = download_client.get_blob_client(blob=file)
        with open(blob_dest, 'wb') as file:
            data = downloader.download_blob()
            file.write(data.readall())

    def ls_blob(self,
                container_name: str,
                path: str,
                recursive: bool = False):
        """
        Args:
        * container_name (str): azure blob container name.
        * path (str): blob path to look at
        * recursive (bool, optional): recurisve look. Defaults to False.

        Returns:
        * list: file list
        """
        if not path == '' and not path.endswith('/'):
            path += '/'
        download_client = self.blob_service_client.get_container_client(container_name)
        blob_looker = download_client.list_blobs(name_starts_with=path)
        files = []
        for blob in blob_looker:
            relative_path = os.path.relpath(blob.name, path)
            if recursive or not '/' in relative_path:  # NOQA:
                files.append(relative_path)
        return files

    def rm_files(self,
                 container_name: str,
                 delete_path: str = '',
                 recursive: bool = False):
        """
        removes files from storage account
        Args:
        * container_name (str): azure blob container name.
        * delete_path (str, optional): what to delete file or directory. Defaults to ''.
        * recursive (bool, optional): recursive delete. Defaults to False.
        """
        if not delete_path == '' and not delete_path.endswith('/'):
            delete_path, delete_file = delete_path.rsplit('/', 1)
            delete_path += '/'
            blobs = self.ls_blob(container_name, delete_path, recursive)
            if delete_file in blobs:
                blobs = [delete_file]
        else:
            blobs = self.ls_blob(container_name, delete_path, recursive)
        if not blobs:
            self._logger.warning('location in blob is empty')
            return
        blobs = [delete_path + blob for blob in blobs]
        self._logger.info(f'files to be removed {blobs}')
        delete_client = self.blob_service_client.get_container_client(container_name)
        if len(blobs) > 1:
            for x in blobs:
                delete_client.delete_blobs(*x)
        else:
            delete_client.delete_blob(*blobs)

    def rm_folder(self, container_name: str, folder_path: str):
        """Doesn't work need to build"""
        container_client = ContainerClient.from_connection_string(conn_str=self.connection_string,
                                                                  container_name=container_name)
        container_client.delete_blob(blob=folder_path)

    def rm_container(self,
                     container_name: str):
        "remove container from storage account"
        self.blob_service_client.delete_container(container_name)

    def ls_containers(self,
                      name_starts_with: str = None):
        "show containers in storage account"
        container_names = self.blob_service_client.list_containers(name_starts_with=name_starts_with)
        for names in container_names:
            print(names['name'])

# Cell


def unlink_files(files: list, file_path: str = './'):
    """
    clean up tool for files that shouldn't be there

    Args:
    * files (list): can be a list of just one file to remove
    * file_path (str, optional): location of the file. Defaults to './'.
    """
    file_list = files
    for x in file_list:
        os.unlink(os.path.join(file_path, x))

# Cell
def rename_adls_directory(old_dir, new_dir, storage_account_name, storage_account_key, container):
    """renames a directory in azure data lake

    Args:
    * old_dir (str): name of old directory
    * new_dir (str): new directory name
    * storage_account_name (str): adls storage account name
    * storage_account_key (str): adls storage account secret key (not connection string)
    * container (str): adls container
    """

    # first connect
    try:
        service_client = DataLakeServiceClient(account_url="{}://{}.dfs.core.windows.net".format(
            "https", storage_account_name), credential=storage_account_key)
    except Exception as e:
        print(e)
    logger.info('Connected to data lake')

    # now rename
    try:
        file_system_client = service_client.get_file_system_client(file_system=container)
        directory_client = file_system_client.get_directory_client(old_dir)
        logger.info('new_dir - ' + directory_client.file_system_name + '/' + new_dir)
        logger.info('old_dir - ' + directory_client.file_system_name + '/' + old_dir)
        directory_client.rename_directory(new_name=directory_client.file_system_name + '/' + new_dir)
    except Exception as e:
        print(e)